{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16aa602",
   "metadata": {},
   "source": [
    "# Day 3: Time Series Analysis & Feature Engineering\n",
    "# Smart City IoT Analytics Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 LEARNING OBJECTIVES:\n",
    "- Perform time series analysis on sensor data\n",
    "- Calculate correlations between different sensor types\n",
    "- Engineer features for predictive modeling\n",
    "- Implement window functions for trend analysis\n",
    "\n",
    "## 📅 SCHEDULE:\n",
    "**Morning (4 hours):**\n",
    "1. Temporal Pattern Analysis (2 hours)\n",
    "2. Cross-Sensor Correlation Analysis (2 hours)\n",
    "\n",
    "**Afternoon (4 hours):**\n",
    "3. Feature Engineering (3 hours)\n",
    "4. Trend Analysis (1 hour)\n",
    "\n",
    "## ✅ DELIVERABLES:\n",
    "- Time series analysis dashboard\n",
    "- Correlation study findings\n",
    "- Feature engineering pipeline\n",
    "- Trend analysis reports\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b471c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/05 17:06:52 WARN Utils: Your hostname, Zipcoders-MacBook-Pro-29.local, resolves to a loopback address: 127.0.0.1; using 192.168.87.88 instead (on interface en0)\n",
      "25/09/05 17:06:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/05 17:06:52 WARN Utils: Your hostname, Zipcoders-MacBook-Pro-29.local, resolves to a loopback address: 127.0.0.1; using 192.168.87.88 instead (on interface en0)\n",
      "25/09/05 17:06:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/05 17:06:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/05 17:06:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created new Spark session\n",
      "📈 Day 3: Time Series Analysis & Feature Engineering\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Machine learning imports\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Initialize Spark Session\n",
    "try:\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(\"✅ Using existing Spark session\")\n",
    "except:\n",
    "    spark = (SparkSession.builder\n",
    "             .appName(\"SmartCityIoTPipeline-Day3\")\n",
    "             .master(\"local[*]\")\n",
    "             .config(\"spark.driver.memory\", \"4g\")\n",
    "             .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "             .getOrCreate())\n",
    "    print(\"✅ Created new Spark session\")\n",
    "\n",
    "print(\"📈 Day 3: Time Series Analysis & Feature Engineering\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5134036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Loading data for Day 3 analysis...\n",
      "🕐 Generating data for 169 time points...\n",
      "✅ Basic sample data generated successfully\n",
      "\n",
      "📊 Dataset Overview:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📋 zones: 5 records\n",
      "      Columns: zone_id, zone_name, zone_type, latitude, longitude\n",
      "   📋 traffic: 845 records\n",
      "      Columns: sensor_id, zone_id, timestamp, vehicle_count, avg_speed, location_lat, location_lon\n",
      "   📋 traffic: 845 records\n",
      "      Columns: sensor_id, zone_id, timestamp, vehicle_count, avg_speed, location_lat, location_lon\n",
      "   📋 air_quality: 845 records\n",
      "      Columns: sensor_id, zone_id, timestamp, pm25, no2, humidity\n",
      "   📋 air_quality: 845 records\n",
      "      Columns: sensor_id, zone_id, timestamp, pm25, no2, humidity\n",
      "   📋 weather: 169 records\n",
      "      Columns: timestamp, temperature, humidity, wind_speed, precipitation\n",
      "   📋 energy: 845 records\n",
      "      Columns: meter_id, zone_id, timestamp, energy_consumption_kwh\n",
      "\n",
      "🎯 Ready for Day 3 Time Series Analysis!\n",
      "   📋 weather: 169 records\n",
      "      Columns: timestamp, temperature, humidity, wind_speed, precipitation\n",
      "   📋 energy: 845 records\n",
      "      Columns: meter_id, zone_id, timestamp, energy_consumption_kwh\n",
      "\n",
      "🎯 Ready for Day 3 Time Series Analysis!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATA (Generate if needed, or load from Day 1/2)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n📂 Loading data for Day 3 analysis...\")\n",
    "\n",
    "def load_or_generate_datasets():\n",
    "    \"\"\"Load datasets from previous days or generate sample data\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    try:\n",
    "        # Generate basic sample data directly\n",
    "        from datetime import datetime, timedelta\n",
    "        import random\n",
    "        import builtins  # For using Python's built-in max function\n",
    "        \n",
    "        # Generate time range (last 7 days)\n",
    "        end_time = datetime.now()\n",
    "        start_time = end_time - timedelta(days=7)\n",
    "        \n",
    "        # Generate timestamps (hourly intervals)\n",
    "        timestamps = []\n",
    "        current_time = start_time\n",
    "        while current_time <= end_time:\n",
    "            timestamps.append(current_time)\n",
    "            current_time += timedelta(hours=1)\n",
    "        \n",
    "        print(f\"🕐 Generating data for {len(timestamps)} time points...\")\n",
    "        \n",
    "        # Generate sample zones\n",
    "        zones_data = [\n",
    "            (1, \"Downtown\", \"Commercial\", 40.7589, -73.9851),\n",
    "            (2, \"Residential_North\", \"Residential\", 40.7805, -73.9565), \n",
    "            (3, \"Industrial_South\", \"Industrial\", 40.7282, -74.0776),\n",
    "            (4, \"Tech_District\", \"Commercial\", 40.7484, -73.9857),\n",
    "            (5, \"Green_Park\", \"Recreational\", 40.7812, -73.9665)\n",
    "        ]\n",
    "        \n",
    "        datasets['zones'] = spark.createDataFrame(zones_data, \n",
    "            ['zone_id', 'zone_name', 'zone_type', 'latitude', 'longitude'])\n",
    "        \n",
    "        # Generate traffic data\n",
    "        traffic_data = []\n",
    "        for i, ts in enumerate(timestamps):\n",
    "            for zone_id in range(1, 6):\n",
    "                # Add some realistic patterns (higher traffic during business hours)\n",
    "                hour = ts.hour\n",
    "                base_traffic = 50\n",
    "                if 7 <= hour <= 9 or 17 <= hour <= 19:  # Rush hours\n",
    "                    base_traffic = 150\n",
    "                elif 10 <= hour <= 16:  # Business hours\n",
    "                    base_traffic = 100\n",
    "                \n",
    "                vehicle_count = builtins.max(0, base_traffic + random.randint(-30, 50))\n",
    "                avg_speed = builtins.max(10, int(60 - (vehicle_count / 5) + random.randint(-10, 10)))\n",
    "                \n",
    "                traffic_data.append((\n",
    "                    f\"TRAFFIC_{zone_id}_{i}\",\n",
    "                    zone_id,\n",
    "                    ts,\n",
    "                    vehicle_count,\n",
    "                    avg_speed,\n",
    "                    40.7589 + (zone_id * 0.01),  # location_lat\n",
    "                    -73.9851 - (zone_id * 0.01)  # location_lon\n",
    "                ))\n",
    "        \n",
    "        datasets['traffic'] = spark.createDataFrame(traffic_data,\n",
    "            ['sensor_id', 'zone_id', 'timestamp', 'vehicle_count', 'avg_speed', 'location_lat', 'location_lon'])\n",
    "        \n",
    "        # Generate air quality data\n",
    "        air_data = []\n",
    "        for i, ts in enumerate(timestamps):\n",
    "            for zone_id in range(1, 6):\n",
    "                # Correlate with traffic (more traffic = worse air quality)\n",
    "                base_pm25 = 15 + (zone_id * 5)  # Different baseline by zone\n",
    "                pm25 = builtins.max(5, base_pm25 + random.randint(-5, 15))\n",
    "                no2 = builtins.max(10, 25 + random.randint(-8, 12))\n",
    "                \n",
    "                air_data.append((\n",
    "                    f\"AIR_{zone_id}_{i}\",\n",
    "                    zone_id,\n",
    "                    ts,\n",
    "                    pm25,\n",
    "                    no2,\n",
    "                    random.randint(40, 80)  # humidity\n",
    "                ))\n",
    "        \n",
    "        datasets['air_quality'] = spark.createDataFrame(air_data,\n",
    "            ['sensor_id', 'zone_id', 'timestamp', 'pm25', 'no2', 'humidity'])\n",
    "        \n",
    "        # Generate weather data\n",
    "        weather_data = []\n",
    "        for i, ts in enumerate(timestamps):\n",
    "            temp = 20 + random.randint(-5, 15)  # Temperature around 20°C\n",
    "            humidity = random.randint(40, 80)\n",
    "            wind_speed = random.randint(5, 25)\n",
    "            precipitation = random.choice([0.0, 0.0, 0.0, 0.1, 0.5, 1.0])  # Mostly dry\n",
    "            \n",
    "            weather_data.append((\n",
    "                ts, temp, humidity, wind_speed, precipitation\n",
    "            ))\n",
    "        \n",
    "        datasets['weather'] = spark.createDataFrame(weather_data,\n",
    "            ['timestamp', 'temperature', 'humidity', 'wind_speed', 'precipitation'])\n",
    "        \n",
    "        # Generate energy data\n",
    "        energy_data = []\n",
    "        for i, ts in enumerate(timestamps):\n",
    "            for zone_id in range(1, 6):\n",
    "                # Energy consumption correlated with time of day\n",
    "                hour = ts.hour\n",
    "                base_consumption = 100\n",
    "                if 18 <= hour <= 22:  # Evening peak\n",
    "                    base_consumption = 200\n",
    "                elif 6 <= hour <= 8:   # Morning peak\n",
    "                    base_consumption = 150\n",
    "                \n",
    "                consumption = builtins.max(50, base_consumption + random.randint(-30, 50))\n",
    "                \n",
    "                energy_data.append((\n",
    "                    f\"ENERGY_{zone_id}_{i}\",\n",
    "                    zone_id,\n",
    "                    ts,\n",
    "                    consumption\n",
    "                ))\n",
    "        \n",
    "        datasets['energy'] = spark.createDataFrame(energy_data,\n",
    "            ['meter_id', 'zone_id', 'timestamp', 'energy_consumption_kwh'])\n",
    "        \n",
    "        print(\"✅ Basic sample data generated successfully\")\n",
    "        return datasets\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating datasets: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "# Load or generate the datasets\n",
    "datasets = load_or_generate_datasets()\n",
    "\n",
    "# Quick data overview\n",
    "print(\"\\n📊 Dataset Overview:\")\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        count = df.count()\n",
    "        print(f\"   📋 {name}: {count:,} records\")\n",
    "        if count > 0:\n",
    "            print(f\"      Columns: {', '.join(df.columns)}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {name}: Failed to load\")\n",
    "\n",
    "print(\"\\n🎯 Ready for Day 3 Time Series Analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2700767a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 1: TEMPORAL PATTERN ANALYSIS (Morning - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"⏰ SECTION 1: TEMPORAL PATTERN ANALYSIS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a4c70",
   "metadata": {},
   "source": [
    "## TODO 1.1: Seasonal Decomposition Analysis (60 minutes)\n",
    "\n",
    "🎯 **TASK:** Decompose time series into trend, seasonal, and residual components  \n",
    "💡 **HINT:** Look for daily, weekly, and monthly patterns  \n",
    "📚 **CONCEPTS:** Seasonality, trends, cyclical patterns, decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79fe467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(df, value_col, time_col=\"timestamp\", sensor_col=None):\n",
    "    \"\"\"\n",
    "    Analyze temporal patterns in sensor data\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with time series data\n",
    "        value_col: Column containing values to analyze\n",
    "        time_col: Timestamp column\n",
    "        sensor_col: Sensor ID column (optional)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with temporal pattern analysis\n",
    "    \"\"\"\n",
    "    print(f\"\\n📈 Temporal Pattern Analysis: {value_col}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TODO: Add time-based features for pattern analysis\n",
    "    df_with_time = df.withColumn(\"year\", F.year(time_col)) \\\n",
    "                     .withColumn(\"month\", F.month(time_col)) \\\n",
    "                     .withColumn(\"day\", F.dayofmonth(time_col)) \\\n",
    "                     .withColumn(\"hour\", F.hour(time_col)) \\\n",
    "                     .withColumn(\"day_of_week\", F.dayofweek(time_col)) \\\n",
    "                     .withColumn(\"week_of_year\", F.weekofyear(time_col)) \\\n",
    "                     .withColumn(\"is_weekend\", F.when(F.dayofweek(time_col).isin([1, 7]), True).otherwise(False))\n",
    "    \n",
    "    # TODO: Hourly patterns\n",
    "    print(\"🕐 Hourly Patterns:\")\n",
    "    hourly_patterns = df_with_time.groupBy(\"hour\").agg(\n",
    "        F.avg(value_col).alias(\"avg_value\"),\n",
    "        F.stddev(value_col).alias(\"stddev_value\"),\n",
    "        F.min(value_col).alias(\"min_value\"),\n",
    "        F.max(value_col).alias(\"max_value\"),\n",
    "        F.count(value_col).alias(\"count_readings\")\n",
    "    ).orderBy(\"hour\")\n",
    "    \n",
    "    hourly_patterns.show(24)\n",
    "    \n",
    "    # TODO: Find peak and off-peak hours\n",
    "    peak_hours = hourly_patterns.orderBy(F.desc(\"avg_value\")).limit(3)\n",
    "    print(\"   🔝 Peak hours:\")\n",
    "    peak_hours.show()\n",
    "    \n",
    "    # TODO: Day of week patterns\n",
    "    print(\"\\n📅 Day of Week Patterns:\")\n",
    "    daily_patterns = df_with_time.groupBy(\"day_of_week\").agg(\n",
    "        F.avg(value_col).alias(\"avg_value\"),\n",
    "        F.count(value_col).alias(\"count_readings\")\n",
    "    ).orderBy(\"day_of_week\")\n",
    "    \n",
    "    # Add day names for better readability\n",
    "    daily_patterns = daily_patterns.withColumn(\n",
    "        \"day_name\",\n",
    "        F.when(F.col(\"day_of_week\") == 1, \"Sunday\")\n",
    "         .when(F.col(\"day_of_week\") == 2, \"Monday\")\n",
    "         .when(F.col(\"day_of_week\") == 3, \"Tuesday\")\n",
    "         .when(F.col(\"day_of_week\") == 4, \"Wednesday\")\n",
    "         .when(F.col(\"day_of_week\") == 5, \"Thursday\")\n",
    "         .when(F.col(\"day_of_week\") == 6, \"Friday\")\n",
    "         .when(F.col(\"day_of_week\") == 7, \"Saturday\")\n",
    "    )\n",
    "    \n",
    "    daily_patterns.select(\"day_name\", \"avg_value\", \"count_readings\").show()\n",
    "    \n",
    "    # TODO: Weekend vs Weekday comparison\n",
    "    weekend_vs_weekday = df_with_time.groupBy(\"is_weekend\").agg(\n",
    "        F.avg(value_col).alias(\"avg_value\"),\n",
    "        F.count(value_col).alias(\"count_readings\")\n",
    "    )\n",
    "    \n",
    "    print(\"\\n🏖️ Weekend vs Weekday:\")\n",
    "    weekend_vs_weekday.show()\n",
    "    \n",
    "    # TODO: Monthly patterns (seasonal trends)\n",
    "    print(\"\\n📊 Monthly Patterns:\")\n",
    "    monthly_patterns = df_with_time.groupBy(\"month\").agg(\n",
    "        F.avg(value_col).alias(\"avg_value\"),\n",
    "        F.count(value_col).alias(\"count_readings\")\n",
    "    ).orderBy(\"month\")\n",
    "    \n",
    "    monthly_patterns.show(12)\n",
    "    \n",
    "    return df_with_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63e8f453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚗 Testing temporal pattern analysis with traffic data...\n",
      "\n",
      "📈 Temporal Pattern Analysis: vehicle_count\n",
      "----------------------------------------\n",
      "🕐 Hourly Patterns:\n",
      "+----+------------------+------------------+---------+---------+--------------+\n",
      "|hour|         avg_value|      stddev_value|min_value|max_value|count_readings|\n",
      "+----+------------------+------------------+---------+---------+--------------+\n",
      "|   0| 57.91428571428571|23.417403569054503|       21|       97|            35|\n",
      "|   1| 60.77142857142857|26.761567827857956|       20|       99|            35|\n",
      "|   2| 58.17142857142857|19.297189241053786|       22|       92|            35|\n",
      "|   3| 54.65714285714286|25.330686873414855|       20|       99|            35|\n",
      "|   4| 61.57142857142857| 22.77290148436968|       21|       97|            35|\n",
      "|   5| 66.02857142857142|21.605399262937134|       21|       99|            35|\n",
      "|   6| 60.08571428571429|21.556180263743972|       22|       99|            35|\n",
      "|   7| 165.9142857142857|21.823316495371248|      120|      198|            35|\n",
      "|   8|             161.2|24.129308514994054|      120|      200|            35|\n",
      "|   9| 161.0857142857143|22.007179882632727|      120|      197|            35|\n",
      "|  10|114.57142857142857| 22.87984691727598|       74|      150|            35|\n",
      "|  11|111.37142857142857|25.320400649982425|       72|      148|            35|\n",
      "|  12|101.17142857142858| 20.70448333699976|       71|      137|            35|\n",
      "|  13|110.22857142857143|21.039298962663562|       71|      149|            35|\n",
      "|  14|113.57142857142857|22.469961274195946|       70|      148|            35|\n",
      "|  15|115.97142857142858|21.932363562292398|       70|      150|            35|\n",
      "|  16|106.28571428571429|22.533075595628738|       72|      150|            35|\n",
      "|  17|           159.275|22.061670555338907|      120|      197|            40|\n",
      "|  18|156.97142857142856| 24.55544232749961|      120|      197|            35|\n",
      "|  19|162.77142857142857|25.876500484240992|      121|      199|            35|\n",
      "|  20|59.142857142857146|23.359269299035745|       21|      100|            35|\n",
      "|  21| 59.22857142857143|24.504604266384323|       21|       98|            35|\n",
      "|  22| 57.05714285714286|25.355058501148022|       21|      100|            35|\n",
      "|  23| 59.25714285714286| 24.22364287116163|       20|      100|            35|\n",
      "+----+------------------+------------------+---------+---------+--------------+\n",
      "\n",
      "   🔝 Peak hours:\n",
      "+----+------------------+------------------+---------+---------+--------------+\n",
      "|hour|         avg_value|      stddev_value|min_value|max_value|count_readings|\n",
      "+----+------------------+------------------+---------+---------+--------------+\n",
      "|   7| 165.9142857142857|21.823316495371248|      120|      198|            35|\n",
      "|  19|162.77142857142857|25.876500484240992|      121|      199|            35|\n",
      "|   8|             161.2|24.129308514994054|      120|      200|            35|\n",
      "+----+------------------+------------------+---------+---------+--------------+\n",
      "\n",
      "\n",
      "📅 Day of Week Patterns:\n",
      "+---------+------------------+--------------+\n",
      "| day_name|         avg_value|count_readings|\n",
      "+---------+------------------+--------------+\n",
      "|   Sunday| 99.75833333333334|           120|\n",
      "|   Monday|             97.55|           120|\n",
      "|  Tuesday|101.03333333333333|           120|\n",
      "|Wednesday|             98.75|           120|\n",
      "| Thursday|           101.025|           120|\n",
      "|   Friday|           102.128|           125|\n",
      "| Saturday|100.46666666666667|           120|\n",
      "+---------+------------------+--------------+\n",
      "\n",
      "\n",
      "🏖️ Weekend vs Weekday:\n",
      "+----------+------------------+--------------+\n",
      "|is_weekend|         avg_value|count_readings|\n",
      "+----------+------------------+--------------+\n",
      "|      true|          100.1125|           240|\n",
      "|     false|100.11404958677686|           605|\n",
      "+----------+------------------+--------------+\n",
      "\n",
      "\n",
      "📊 Monthly Patterns:\n",
      "+-----+------------------+--------------+\n",
      "|month|         avg_value|count_readings|\n",
      "+-----+------------------+--------------+\n",
      "|    8| 99.79272727272728|           275|\n",
      "|    9|100.26842105263158|           570|\n",
      "+-----+------------------+--------------+\n",
      "\n",
      "\n",
      "✅ Temporal pattern analysis completed!\n",
      "📊 Sample data with temporal features:\n",
      "+--------------------+-------------+----+-----------+----------+\n",
      "|           timestamp|vehicle_count|hour|day_of_week|is_weekend|\n",
      "+--------------------+-------------+----+-----------+----------+\n",
      "|2025-08-29 17:12:...|          185|  17|          6|     false|\n",
      "|2025-08-29 17:12:...|          120|  17|          6|     false|\n",
      "|2025-08-29 17:12:...|          176|  17|          6|     false|\n",
      "|2025-08-29 17:12:...|          137|  17|          6|     false|\n",
      "|2025-08-29 17:12:...|          164|  17|          6|     false|\n",
      "|2025-08-29 18:12:...|          154|  18|          6|     false|\n",
      "|2025-08-29 18:12:...|          174|  18|          6|     false|\n",
      "|2025-08-29 18:12:...|          131|  18|          6|     false|\n",
      "|2025-08-29 18:12:...|          197|  18|          6|     false|\n",
      "|2025-08-29 18:12:...|          121|  18|          6|     false|\n",
      "+--------------------+-------------+----+-----------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Test the temporal pattern analysis with traffic data\n",
    "print(\"🚗 Testing temporal pattern analysis with traffic data...\")\n",
    "\n",
    "if 'traffic' in datasets and datasets['traffic'] is not None:\n",
    "    # Analyze temporal patterns for traffic vehicle count\n",
    "    traffic_with_patterns = analyze_temporal_patterns(\n",
    "        datasets['traffic'], \n",
    "        'vehicle_count'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ Temporal pattern analysis completed!\")\n",
    "    print(\"📊 Sample data with temporal features:\")\n",
    "    traffic_with_patterns.select('timestamp', 'vehicle_count', 'hour', 'day_of_week', 'is_weekend').show(10)\n",
    "else:\n",
    "    print(\"❌ Traffic data not available for analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
