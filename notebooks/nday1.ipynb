{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Environment Setup & Data Exploration\n",
    "# Smart City IoT Analytics Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 LEARNING OBJECTIVES:\n",
    "- Configure Spark cluster and development environment\n",
    "- Understand IoT data characteristics and challenges  \n",
    "- Implement basic data ingestion patterns\n",
    "- Explore PySpark DataFrame operations\n",
    "\n",
    "## 📅 SCHEDULE:\n",
    "**Morning (4 hours):**\n",
    "1. Environment Setup (2 hours)\n",
    "2. Data Exploration (2 hours)\n",
    "\n",
    "**Afternoon (4 hours):**  \n",
    "3. Basic Data Ingestion (2 hours)\n",
    "4. Initial Data Transformations (2 hours)\n",
    "\n",
    "## ✅ DELIVERABLES:\n",
    "- Working Spark cluster with all services running\n",
    "- Data ingestion notebook with basic EDA\n",
    "- Documentation of data quality findings  \n",
    "- Initial data loading pipeline functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Welcome to the Smart City IoT Analytics Pipeline!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 Welcome to the Smart City IoT Analytics Pipeline!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 1: ENVIRONMENT SETUP (Morning - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.1: Initialize Spark Session (15 minutes)\n",
    "\n",
    "🎯 **TASK:** Create a Spark session configured for local development  \n",
    "💡 **HINT:** Use SparkSession.builder with appropriate configurations  \n",
    "📚 **DOCS:** https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "\n",
    "**TODO:** Create Spark session with the following configurations:\n",
    "- App name: \"SmartCityIoTPipeline-Day1\"\n",
    "- Master: \"local[*]\" (use all available cores)\n",
    "- Memory: \"4g\" for driver\n",
    "- Additional configs for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "25/09/04 19:13:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session Details:\n",
      "   App Name: SmartCityIoTPipeline-Day1\n",
      "   Spark Version: 4.0.0\n",
      "   Master: local[*]\n",
      "   Default Parallelism: 8\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create Spark session with the following configurations:\n",
    "jdbc_jar_path = \"/Users/sai/Documents/Projects/ninth-week/SparkCity/postgresql-42.7.3.jar\"\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"SmartCityIoTPipeline-Day1\")  # TODO: Add your app name\n",
    "         .master(\"local[*]\")   # TODO: Add master configuration\n",
    "         .config(\"spark.driver.memory\", \"4g\")  # TODO: Set memory\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "         .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "         .config(\"spark.jars\", jdbc_jar_path)  # <-- Add this line for JDBC driver\n",
    "         .getOrCreate())\n",
    "\n",
    "# TODO: Verify Spark session is working\n",
    "print(\"✅ Spark Session Details:\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   Default Parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.2: Verify Infrastructure (15 minutes)\n",
    "\n",
    "🎯 **TASK:** Check that all infrastructure services are running  \n",
    "💡 **HINT:** Test database connectivity and file system access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database connection successful!\n",
      "\n",
      "🌐 Spark UI should be accessible at: http://localhost:4040\n",
      "   (Open this in your browser to monitor Spark jobs)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test PostgreSQL connection\n",
    "def test_database_connection():\n",
    "    \"\"\"Test connection to PostgreSQL database\"\"\"\n",
    "    try:\n",
    "        # Database connection parameters\n",
    "        db_properties = {\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"password\", \n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "        \n",
    "        # TODO: Replace with actual connection test\n",
    "        # Test query - should create a simple DataFrame from database\n",
    "        test_df = spark.read.jdbc(\n",
    "            url=\"jdbc:postgresql://localhost:5432/smartcity\",\n",
    "            table=\"(SELECT 1 as test_column) as test_table\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "        \n",
    "        # TODO: Collect and display result\n",
    "        result = test_df.collect()\n",
    "        print(\"✅ Database connection successful!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Database connection failed: {str(e)}\")\n",
    "        print(\"💡 Make sure PostgreSQL container is running: docker-compose up -d\")\n",
    "        return False\n",
    "\n",
    "# TODO: Run the database connection test\n",
    "db_connected = test_database_connection()\n",
    "\n",
    "# TODO: Check Spark UI accessibility\n",
    "print(\"\\n🌐 Spark UI should be accessible at: http://localhost:4040\")\n",
    "print(\"   (Open this in your browser to monitor Spark jobs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.3: Generate Sample Data (30 minutes)\n",
    "\n",
    "🎯 **TASK:** Run the data generation script to create sample IoT data  \n",
    "💡 **HINT:** Use the provided data generation script or run it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Generating data... (0/5 files exist)\n",
      "   Project root: /Users/sai/Documents/Projects/ninth-week/SparkCity\n",
      "   Running script from: /Users/sai/Documents/Projects/ninth-week/SparkCity\n",
      "✅ Data generation successful!\n"
     ]
    }
   ],
   "source": [
    "# Generate Sample IoT Data\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def check_and_generate_data():\n",
    "    \"\"\"Check if data exists, generate if missing\"\"\"\n",
    "    data_files = [\"traffic_sensors.csv\", \"air_quality.json\", \"weather_data.parquet\", \n",
    "                  \"energy_meters.csv\", \"city_zones.csv\"]\n",
    "    data_path = \"data/raw\"\n",
    "    \n",
    "    # Check existing files\n",
    "    existing = [f for f in data_files if os.path.exists(f\"{data_path}/{f}\")]\n",
    "    \n",
    "    if len(existing) == len(data_files):\n",
    "        print(f\"✅ All {len(data_files)} data files found!\")\n",
    "        return True\n",
    "    \n",
    "    # Generate missing data\n",
    "    print(f\"🔄 Generating data... ({len(existing)}/{len(data_files)} files exist)\")\n",
    "    \n",
    "    try:\n",
    "        # Get the project root (go up one level from notebooks folder)\n",
    "        notebook_dir = os.getcwd()  # Current directory (notebooks/)\n",
    "        project_root = os.path.dirname(notebook_dir)  # Go up one level to SparkCity/\n",
    "        \n",
    "        print(f\"   Project root: {project_root}\")\n",
    "        print(f\"   Running script from: {project_root}\")\n",
    "        \n",
    "        # Run from project root directory\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"scripts/generate_data.py\"], \n",
    "            cwd=project_root,  # Run from SparkCity/ directory\n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            timeout=300\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ Data generation successful!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Generation failed!\")\n",
    "            if result.stderr:\n",
    "                print(f\"   Error: {result.stderr.strip()}\")\n",
    "            if result.stdout:\n",
    "                print(f\"   Output: {result.stdout.strip()}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run data check/generation\n",
    "data_ready = check_and_generate_data()\n",
    "\n",
    "# If failed, provide clear manual instructions\n",
    "if not data_ready:\n",
    "    print(\"\\n🔧 MANUAL FIX:\")\n",
    "    print(\"1. Open terminal\")\n",
    "    print(\"2. Run: cd /Users/sai/Documents/Projects/ninth-week/SparkCity\")\n",
    "    print(\"3. Run: python scripts/generate_data.py\")\n",
    "    print(\"4. Re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 2: DATA EXPLORATION (Morning - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📊 SECTION 2: EXPLORATORY DATA ANALYSIS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📊 SECTION 2: EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.1: Load and Examine Data Sources (45 minutes)\n",
    "\n",
    "🎯 **TASK:** Load each data source and examine its structure  \n",
    "💡 **HINT:** Use appropriate Spark readers for different file formats  \n",
    "📚 **CONCEPTS:** Schema inference, file formats, data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = \"../data/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📍 Loading City Zones Reference Data...\n",
      "   📊 Records: 8\n",
      "   📋 Schema:\n",
      "root\n",
      " |-- zone_id: string (nullable = true)\n",
      " |-- zone_name: string (nullable = true)\n",
      " |-- zone_type: string (nullable = true)\n",
      " |-- lat_min: double (nullable = true)\n",
      " |-- lat_max: double (nullable = true)\n",
      " |-- lon_min: double (nullable = true)\n",
      " |-- lon_max: double (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "   🔍 Sample Data:\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|zone_id |zone_name         |zone_type  |lat_min|lat_max|lon_min|lon_max|population|\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|ZONE_001|Downtown          |commercial |40.72  |40.74  |-74.01 |-73.99 |25000     |\n",
      "|ZONE_002|Financial District|commercial |40.7   |40.72  |-74.02 |-74.0  |15000     |\n",
      "|ZONE_003|Residential North |residential|40.76  |40.8   |-74.0  |-73.98 |45000     |\n",
      "|ZONE_004|Residential South |residential|40.7   |40.72  |-73.98 |-73.96 |38000     |\n",
      "|ZONE_005|Industrial Park   |industrial |40.74  |40.76  |-74.02 |-74.0  |5000      |\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load city zones reference data\n",
    "print(\"📍 Loading City Zones Reference Data...\")\n",
    "try:\n",
    "    zones_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/city_zones.csv\")\n",
    "    \n",
    "    # TODO: Display basic information about zones\n",
    "    print(f\"   📊 Records: {zones_df.count()}\")\n",
    "    print(f\"   📋 Schema:\")\n",
    "    zones_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   🔍 Sample Data:\")\n",
    "    zones_df.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading zones data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚗 Loading Traffic Sensors Data...\n",
      "   📊 Records: 100850\n",
      "   📋 Schema:\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- vehicle_count: integer (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      " |-- congestion_level: string (nullable = true)\n",
      " |-- road_type: string (nullable = true)\n",
      "\n",
      "   🔍 Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+-------------+------------------+----------------+-----------+\n",
      "|  sensor_id|           timestamp|      location_lat|      location_lon|vehicle_count|         avg_speed|congestion_level|  road_type|\n",
      "+-----------+--------------------+------------------+------------------+-------------+------------------+----------------+-----------+\n",
      "|TRAFFIC_001|2025-08-28 19:13:...| 40.78941849302994| -74.0041930409108|           32|16.417025239365007|            high|residential|\n",
      "|TRAFFIC_002|2025-08-28 19:13:...| 40.77304212963235|-73.91903939743946|           25|27.763808497333677|            high| commercial|\n",
      "|TRAFFIC_003|2025-08-28 19:13:...|40.797412843124846|-73.91180691102876|           31|13.221915930299499|            high| commercial|\n",
      "|TRAFFIC_004|2025-08-28 19:13:...| 40.76447004554071|-73.91102045449436|           28|30.487846213776308|            high|residential|\n",
      "|TRAFFIC_005|2025-08-28 19:13:...|  40.7580006756147|-73.94232435909976|           32| 25.34875715626183|            high|   arterial|\n",
      "+-----------+--------------------+------------------+------------------+-------------+------------------+----------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load traffic sensors data  \n",
    "print(\"\\n🚗 Loading Traffic Sensors Data...\")\n",
    "try:\n",
    "    # TODO: Load CSV file with proper options\n",
    "    traffic_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/traffic_sensors.csv\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   📊 Records: {traffic_df.count()}\")\n",
    "    print(f\"   📋 Schema:\")\n",
    "    traffic_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   🔍 Sample Data:\")\n",
    "    traffic_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading traffic data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌫️ Loading Air Quality Data...\n",
      "   🔍 Loading JSON file...\n",
      "   📊 Total records loaded: 13460\n",
      "   📋 Schema:\n",
      "root\n",
      " |-- co: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- no2: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "   🔍 Sample Data:\n",
      "+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+---------+------------------+--------------------+\n",
      "|                co|         humidity|      location_lat|      location_lon|               no2|              pm10|              pm25|sensor_id|       temperature|           timestamp|\n",
      "+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+---------+------------------+--------------------+\n",
      "|1.4434738146791752|75.66604410020656| 40.75653235816122|-73.98428454574555|49.075760905779475|55.173371978887076| 33.11903566634626|   AQ_001|15.002217833482884|2025-08-28T19:13:...|\n",
      "|1.8888512905643138|34.32560667689453|  40.7486045225851|-73.99148399062048|30.643727239950987|  64.4310622730802|  29.6807138707888|   AQ_002|23.770313608455425|2025-08-28T19:13:...|\n",
      "|  1.43215898631674| 47.0006824853133|40.772756782058615|-73.96499574521356|  37.0399054846869| 45.09185001586686|37.953755592616915|   AQ_003|10.431211104543662|2025-08-28T19:13:...|\n",
      "|1.1252224066881198|71.08432748774143|40.765579571560615|-73.98589196898428|41.238118236245604| 52.34620504429582|  34.8311657620839|   AQ_004|24.745054535911542|2025-08-28T19:13:...|\n",
      "|1.5892469346851323|63.44601770365849| 40.72138884096869|-74.01138268107208|43.955634845940494| 59.06955665729193| 38.50592070150905|   AQ_005| 9.226279029910831|2025-08-28T19:13:...|\n",
      "+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+---------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "   ✅ All records have sensor_id values\n",
      "   ✅ Air quality data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load air quality data (JSON format) - CORRECTED VERSION\n",
    "print(\"\\n🌫️ Loading Air Quality Data...\")\n",
    "try:\n",
    "    # First, try loading without corrupt record handling since the JSON seems valid\n",
    "    print(\"   🔍 Loading JSON file...\")\n",
    "    \n",
    "    air_quality_df = (spark.read\n",
    "                     .option(\"multiline\", \"true\")\n",
    "                     .json(f\"{data_dir}/air_quality.json\"))\n",
    "    \n",
    "    # Check if we loaded successfully\n",
    "    total_records = air_quality_df.count()\n",
    "    print(f\"   📊 Total records loaded: {total_records}\")\n",
    "    \n",
    "    # Check the schema\n",
    "    print(f\"   📋 Schema:\")\n",
    "    air_quality_df.printSchema()\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"   🔍 Sample Data:\")\n",
    "    air_quality_df.show(5)\n",
    "    \n",
    "    # Check for any missing sensor_id values (basic quality check)\n",
    "    missing_sensor_ids = air_quality_df.filter(F.col(\"sensor_id\").isNull()).count()\n",
    "    if missing_sensor_ids > 0:\n",
    "        print(f\"   ⚠️  Found {missing_sensor_ids} records with missing sensor_id\")\n",
    "    else:\n",
    "        print(\"   ✅ All records have sensor_id values\")\n",
    "    \n",
    "    print(\"   ✅ Air quality data loaded successfully!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading air quality data: {str(e)}\")\n",
    "    \n",
    "    # If the above fails, try alternative approach\n",
    "    print(\"   🔄 Trying alternative loading method...\")\n",
    "    try:\n",
    "        # Try without multiline option\n",
    "        air_quality_df = spark.read.json(f\"{data_dir}/air_quality.json\")\n",
    "        \n",
    "        total_records = air_quality_df.count()\n",
    "        print(f\"   📊 Alternative method - Records loaded: {total_records}\")\n",
    "        \n",
    "        if total_records > 0:\n",
    "            print(\"   ✅ Alternative method successful!\")\n",
    "            air_quality_df.printSchema()\n",
    "            air_quality_df.show(5)\n",
    "        else:\n",
    "            print(\"   ❌ No records loaded with alternative method\")\n",
    "            air_quality_df = None\n",
    "            \n",
    "    except Exception as e2:\n",
    "        print(f\"   ❌ Alternative method also failed: {str(e2)}\")\n",
    "        air_quality_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌤️ Loading Weather Data...\n",
      "   📊 Records: 3370\n",
      "   📋 Schema:\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- wind_direction: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      "\n",
      "   🔍 Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "| station_id|           timestamp|      location_lat|      location_lon|       temperature|          humidity|        wind_speed|    wind_direction|     precipitation|          pressure|\n",
      "+-----------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|WEATHER_001|2025-08-28T19:13:...| 40.74068294081521|-73.98371193888273|16.255509823025246| 37.86867259214939|2.1651984754617093| 62.36050397741386|               0.0|1007.1824215872215|\n",
      "|WEATHER_002|2025-08-28T19:13:...|40.719379533137506|-74.00707270379347|15.898078105189569| 78.51243906370601| 8.189820861766247|162.55291300680636|0.5918512643540632|1007.9079806718611|\n",
      "|WEATHER_003|2025-08-28T19:13:...| 40.79644724145399|-74.00151258838463|15.380372085028634|29.008335613495916| 7.509542336206049| 299.4774663063252|               0.0|1017.4831384373119|\n",
      "|WEATHER_004|2025-08-28T19:13:...|40.702238574622974| -74.0123368585314|  14.5388077716329|   76.816802058598|13.130756163212046|191.80531192092675|               0.0|1013.3914065647743|\n",
      "|WEATHER_005|2025-08-28T19:13:...| 40.77064816596618|-73.92501520149634| 17.40982302390736| 59.89475706373974| 3.450212451845583|357.54695871707236|               0.0|1014.9464999324806|\n",
      "+-----------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load weather data (Parquet format)\n",
    "print(\"\\n🌤️ Loading Weather Data...\")\n",
    "try:\n",
    "    # TODO: Load Parquet file - another different format!\n",
    "    weather_df = spark.read.parquet(f\"{data_dir}/weather_data.parquet\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   📊 Records: {weather_df.count()}\")\n",
    "    print(f\"   📋 Schema:\")\n",
    "    weather_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   🔍 Sample Data:\")\n",
    "    weather_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading weather data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ Loading Energy Meters Data...\n",
      "   📊 Records: 201800\n",
      "   📋 Schema:\n",
      "root\n",
      " |-- meter_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- power_consumption: double (nullable = true)\n",
      " |-- voltage: double (nullable = true)\n",
      " |-- current: double (nullable = true)\n",
      " |-- power_factor: double (nullable = true)\n",
      "\n",
      "   🔍 Sample Data:\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|   meter_id|           timestamp|building_type|      location_lat|      location_lon| power_consumption|           voltage|           current|      power_factor|\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|ENERGY_0001|2025-08-28 19:13:...|  residential| 40.73857038064395| -73.9429162967311| 3.030232301815058|228.38184955614182|13.268271133210844| 0.937310772293651|\n",
      "|ENERGY_0002|2025-08-28 19:13:...|  residential| 40.70003316266749|-73.99318519662994|4.6611421007106735|236.39125276089501|19.717912766532546| 0.893182446401671|\n",
      "|ENERGY_0003|2025-08-28 19:13:...|  residential|40.716194936463395|-74.00827710835044| 3.174200327030417|245.69717999415337| 12.91915652880489|0.8785293495846239|\n",
      "|ENERGY_0004|2025-08-28 19:13:...|   industrial| 40.79664622350928|-73.93846873735934|42.019627823493764| 242.6898433146506|173.14127055994987|0.8854675498569888|\n",
      "|ENERGY_0005|2025-08-28 19:13:...|       retail| 40.74988159533403| -74.0175316560653| 6.432280064761357| 237.1072369028049| 27.12814736818041|0.8778945649339505|\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load energy meters data\n",
    "print(\"\\n⚡ Loading Energy Meters Data...\")\n",
    "try:\n",
    "    # TODO: Load CSV file\n",
    "    energy_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/energy_meters.csv\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   📊 Records: {energy_df.count()}\")\n",
    "    print(f\"   📋 Schema:\")\n",
    "    energy_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   🔍 Sample Data:\")\n",
    "    energy_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading energy data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.2: Basic Data Quality Assessment (45 minutes)\n",
    "\n",
    "🎯 **TASK:** Assess data quality across all datasets  \n",
    "💡 **HINT:** Check for missing values, duplicates, data ranges  \n",
    "📚 **CONCEPTS:** Data profiling, quality metrics, anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Perform basic data quality assessment on a DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to assess\n",
    "        dataset_name: Name of the dataset for reporting\n",
    "    \"\"\"\n",
    "    print(f\"\\n📋 Data Quality Assessment: {dataset_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # TODO: Basic statistics\n",
    "    total_rows = df.count()\n",
    "    total_cols = len(df.columns)\n",
    "    print(f\"   📊 Dimensions: {total_rows:,} rows × {total_cols} columns\")\n",
    "    \n",
    "    # TODO: Check for missing values\n",
    "    print(f\"   🔍 Missing Values:\")\n",
    "    for col in df.columns:\n",
    "        missing_count = df.filter(F.col(col).isNull()).count()\n",
    "        missing_pct = (missing_count / total_rows) * 100\n",
    "        if missing_count > 0:\n",
    "            print(f\"      {col}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    # TODO: Check for duplicate records\n",
    "    duplicate_count = total_rows - df.dropDuplicates().count()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"   🔄 Duplicate Records: {duplicate_count:,}\")\n",
    "    else:\n",
    "        print(f\"   ✅ No duplicate records found\")\n",
    "    \n",
    "    # TODO: Numeric column statistics\n",
    "    numeric_cols = [field.name for field in df.schema.fields \n",
    "                   if field.dataType in [IntegerType(), DoubleType(), FloatType(), LongType()]]\n",
    "    \n",
    "    if numeric_cols:\n",
    "        print(f\"   📈 Numeric Columns Summary:\")\n",
    "        # Show basic statistics for numeric columns\n",
    "        df.select(numeric_cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Data Quality Assessment: City Zones\n",
      "--------------------------------------------------\n",
      "   📊 Dimensions: 8 rows × 8 columns\n",
      "   🔍 Missing Values:\n",
      "   ✅ No duplicate records found\n",
      "   📈 Numeric Columns Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/04 19:14:34 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|summary|             lat_min|             lat_max|            lon_min|            lon_max|        population|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|  count|                   8|                   8|                  8|                  8|                 8|\n",
      "|   mean|  40.730000000000004|             40.7525| -73.99125000000001|          -73.97125|           21250.0|\n",
      "| stddev|0.023904572186687328|0.028157719063465373|0.02474873734153055|0.02474873734153458|14260.334598358582|\n",
      "|    min|                40.7|               40.72|             -74.02|              -74.0|              5000|\n",
      "|    max|               40.76|                40.8|             -73.96|             -73.94|             45000|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "\n",
      "\n",
      "📋 Data Quality Assessment: Traffic Sensors\n",
      "--------------------------------------------------\n",
      "   📊 Dimensions: 100,850 rows × 8 columns\n",
      "   🔍 Missing Values:\n",
      "   ✅ No duplicate records found\n",
      "   📈 Numeric Columns Summary:\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|summary|        location_lat|       location_lon|     vehicle_count|         avg_speed|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|  count|              100850|             100850|            100850|            100850|\n",
      "|   mean|   40.75011790265806| -73.95663481997272|22.197045116509667|45.601101580647274|\n",
      "| stddev|0.027328503495008007|0.03287764405886609|13.561857125784131| 17.04167685328942|\n",
      "|    min|  40.700157401736284| -74.01290766686316|                 0|               5.0|\n",
      "|    max|  40.798402010459405| -73.90082117344016|                91| 116.4255545170856|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "\n",
      "\n",
      "📋 Data Quality Assessment: Air Quality\n",
      "--------------------------------------------------\n",
      "   📊 Dimensions: 13,460 rows × 10 columns\n",
      "   🔍 Missing Values:\n",
      "   ✅ No duplicate records found\n",
      "   📈 Numeric Columns Summary:\n",
      "+-------+------------------+------------------+--------------------+--------------------+------------------+------------------+------------------+-------------------+\n",
      "|summary|                co|          humidity|        location_lat|        location_lon|               no2|              pm10|              pm25|        temperature|\n",
      "+-------+------------------+------------------+--------------------+--------------------+------------------+------------------+------------------+-------------------+\n",
      "|  count|             13460|             13460|               13460|               13460|             13460|             13460|             13460|              13460|\n",
      "|   mean|1.2913183781712168| 55.00618055689926|   40.75175500006564|  -73.96542996525274| 32.29988252907353| 42.85989685612711|26.850596272313943|  20.08826317695994|\n",
      "| stddev|0.4304849697096813|14.397152064423723|0.029045683202332126|0.035916430554797936|10.748077709729799|12.985904734331784|   8.6164760598784|   7.99320079509865|\n",
      "|    min|               0.0|30.006622447061215|   40.70026463979771|   -74.0155812980716|               0.0|               0.0|               0.0|-10.607862591296826|\n",
      "|    max|2.8023737091886805| 79.98680573628073|  40.797411339968505|   -73.9051894484372| 73.19184731854942| 89.65226155858564|62.588210258646455| 50.551147083323585|\n",
      "+-------+------------------+------------------+--------------------+--------------------+------------------+------------------+------------------+-------------------+\n",
      "\n",
      "\n",
      "📋 Data Quality Assessment: Weather Stations\n",
      "--------------------------------------------------\n",
      "   📊 Dimensions: 3,370 rows × 10 columns\n",
      "   🔍 Missing Values:\n",
      "   ✅ No duplicate records found\n",
      "   📈 Numeric Columns Summary:\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+-------------------+--------------------+------------------+\n",
      "|summary|        location_lat|       location_lon|       temperature|          humidity|          wind_speed|     wind_direction|       precipitation|          pressure|\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+-------------------+--------------------+------------------+\n",
      "|  count|                3370|               3370|              3370|              3370|                3370|               3370|                3370|              3370|\n",
      "|   mean|  40.744659328860365| -73.97135064318793|19.810921196145923| 60.00735104978684|   8.115559392808636| 179.07501014213207|0.059748613682613014| 1012.938419331309|\n",
      "| stddev|0.035050829601692896|0.04174693811588695|3.2053776129492166|14.637398030894772|    8.07924377014336| 103.66153873620159|  0.2475344933641174|10.035478736930868|\n",
      "|    min|  40.700431377211714| -74.01364897626706| 7.465013216576269|              20.0|0.002807241069954...|0.19449606013004495|                 0.0| 976.6862164596702|\n",
      "|    max|   40.79644724145399| -73.90420441177432|30.076456657829173|             100.0|   62.42099178181472|  359.9192660727401|   3.268515396201009|1046.5364453753104|\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+-------------------+--------------------+------------------+\n",
      "\n",
      "\n",
      "📋 Data Quality Assessment: Energy Meters\n",
      "--------------------------------------------------\n",
      "   📊 Dimensions: 201,800 rows × 9 columns\n",
      "   🔍 Missing Values:\n",
      "   ✅ No duplicate records found\n",
      "   📈 Numeric Columns Summary:\n",
      "+-------+--------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|summary|        location_lat|       location_lon| power_consumption|           voltage|           current|        power_factor|\n",
      "+-------+--------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|              201800|             201800|            201800|            201800|            201800|              201800|\n",
      "|   mean|   40.75114696746992| -73.95570226085518|17.661501426137587|239.99110505236544| 73.62718756676854|  0.9000784069428297|\n",
      "| stddev|0.029149474776993135|0.03696918385133131| 18.75357389149128| 5.010000846318629| 78.21712082409343|0.028898400050630813|\n",
      "|    min|   40.70003316266749| -74.01817156010618|1.6800237743084105| 218.3401346513567| 6.660334472337805|   0.850000401033654|\n",
      "|    max|   40.79986752880212| -73.90007563085238| 64.99993909041741|  264.016592785443|288.69565536070843|  0.9499999715633012|\n",
      "+-------+--------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Assess quality for each dataset\n",
    "datasets = [\n",
    "    (zones_df, \"City Zones\"),\n",
    "    (traffic_df, \"Traffic Sensors\"), \n",
    "    (air_quality_df, \"Air Quality\"),\n",
    "    (weather_df, \"Weather Stations\"),\n",
    "    (energy_df, \"Energy Meters\")\n",
    "]\n",
    "\n",
    "for df, name in datasets:\n",
    "    try:\n",
    "        assess_data_quality(df, name)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error assessing {name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.3: Temporal Analysis (30 minutes)\n",
    "\n",
    "🎯 **TASK:** Analyze temporal patterns in the IoT data  \n",
    "💡 **HINT:** Look at data distribution over time, identify patterns  \n",
    "📚 **CONCEPTS:** Time series analysis, temporal patterns, data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "⏰ TEMPORAL PATTERN ANALYSIS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60) \n",
    "print(\"⏰ TEMPORAL PATTERN ANALYSIS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚗 Traffic Patterns by Hour:\n",
      "+----+------------------+--------+\n",
      "|hour|      avg_vehicles|readings|\n",
      "+----+------------------+--------+\n",
      "|   0|18.821190476190477|    4200|\n",
      "|   1|18.846190476190475|    4200|\n",
      "|   2|18.988095238095237|    4200|\n",
      "|   3|18.810238095238095|    4200|\n",
      "|   4|18.785476190476192|    4200|\n",
      "|   5|18.865238095238094|    4200|\n",
      "|   6|19.023809523809526|    4200|\n",
      "|   7| 32.33428571428571|    4200|\n",
      "|   8|32.309761904761906|    4200|\n",
      "|   9| 32.45738095238095|    4200|\n",
      "|  10|18.930238095238096|    4200|\n",
      "|  11| 18.48857142857143|    4200|\n",
      "|  12|18.714523809523808|    4200|\n",
      "|  13|18.919285714285714|    4200|\n",
      "|  14|18.798333333333332|    4200|\n",
      "|  15|18.811666666666667|    4200|\n",
      "|  16|            18.595|    4200|\n",
      "|  17| 32.43785714285714|    4200|\n",
      "|  18| 32.24738095238095|    4200|\n",
      "|  19|32.567764705882354|    4250|\n",
      "|  20|             18.66|    4200|\n",
      "|  21|18.568571428571428|    4200|\n",
      "|  22| 18.61452380952381|    4200|\n",
      "|  23|19.010238095238094|    4200|\n",
      "+----+------------------+--------+\n",
      "\n",
      "📝 OBSERVATIONS:\n",
      "   - Rush hour patterns: [YOUR ANALYSIS HERE]\n",
      "   - Off-peak periods: [YOUR ANALYSIS HERE]\n",
      "   - Peak traffic hours: [YOUR ANALYSIS HERE]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Analyze traffic patterns by hour\n",
    "print(\"\\n🚗 Traffic Patterns by Hour:\")\n",
    "try:\n",
    "    # TODO: Extract hour from timestamp and analyze vehicle counts\n",
    "    traffic_hourly = (traffic_df\n",
    "                     .withColumn(\"hour\", F.hour(\"timestamp\"))\n",
    "                     .groupBy(\"hour\")\n",
    "                     .agg(F.avg(\"vehicle_count\").alias(\"avg_vehicles\"),\n",
    "                          F.count(\"*\").alias(\"readings\"))\n",
    "                     .orderBy(\"hour\"))\n",
    "    \n",
    "    # TODO: Show the results\n",
    "    traffic_hourly.show(24)\n",
    "    \n",
    "    # TODO: What patterns do you notice? Add your observations here:\n",
    "    print(\"📝 OBSERVATIONS:\")\n",
    "    print(\"   - Rush hour patterns: [YOUR ANALYSIS HERE]\")\n",
    "    print(\"   - Off-peak periods: [YOUR ANALYSIS HERE]\")\n",
    "    print(\"   - Peak traffic hours: [YOUR ANALYSIS HERE]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error analyzing traffic patterns: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌫️ Air Quality Patterns by Day of Week:\n",
      "+-----------+------------------+------------------+\n",
      "|day_of_week|          avg_pm25|           avg_no2|\n",
      "+-----------+------------------+------------------+\n",
      "|          1|26.913545169521978| 32.50367568259643|\n",
      "|          2|26.748436329345452| 32.07425375361587|\n",
      "|          3|26.622715112231916| 32.06719516571267|\n",
      "|          4|26.569904062775556|32.443047171509015|\n",
      "|          5|26.922534687700107| 32.17436968878897|\n",
      "|          6|27.208702754080225|32.356298105760736|\n",
      "|          7|26.967586432048975|32.481645560951094|\n",
      "+-----------+------------------+------------------+\n",
      "\n",
      "📝 OBSERVATIONS:\n",
      "   - Weekday vs weekend patterns: [YOUR ANALYSIS HERE]\n",
      "   - Pollution trends: [YOUR ANALYSIS HERE]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Analyze air quality patterns by day of week\n",
    "print(\"\\n🌫️ Air Quality Patterns by Day of Week:\")\n",
    "try:\n",
    "    # TODO: Extract day of week and analyze PM2.5 levels\n",
    "    air_quality_daily = (air_quality_df\n",
    "                        .withColumn(\"day_of_week\", F.dayofweek(\"timestamp\"))\n",
    "                        .groupBy(\"day_of_week\")\n",
    "                        .agg(F.avg(\"pm25\").alias(\"avg_pm25\"),\n",
    "                             F.avg(\"no2\").alias(\"avg_no2\"))\n",
    "                        .orderBy(\"day_of_week\"))\n",
    "    \n",
    "    # TODO: Show results\n",
    "    air_quality_daily.show()\n",
    "    \n",
    "    # TODO: Add your observations\n",
    "    print(\"📝 OBSERVATIONS:\")\n",
    "    print(\"   - Weekday vs weekend patterns: [YOUR ANALYSIS HERE]\")\n",
    "    print(\"   - Pollution trends: [YOUR ANALYSIS HERE]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error analyzing air quality patterns: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 3: BASIC DATA INGESTION (Afternoon - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📥 SECTION 3: DATA INGESTION PIPELINE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📥 SECTION 3: DATA INGESTION PIPELINE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3.1: Create Reusable Data Loading Functions (60 minutes)\n",
    "\n",
    "🎯 **TASK:** Create reusable functions for loading different data formats  \n",
    "💡 **HINT:** Handle schema validation and error handling  \n",
    "📚 **CONCEPTS:** Function design, error handling, schema enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path, expected_schema=None):\n",
    "    \"\"\"\n",
    "    Load CSV data with proper error handling and schema validation\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to CSV file\n",
    "        expected_schema: Optional StructType for schema enforcement\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement CSV loading with options\n",
    "        df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "        # TODO: Add schema validation if provided\n",
    "        if expected_schema:\n",
    "            # Validate schema matches expected\n",
    "            pass\n",
    "            \n",
    "        print(f\"✅ Successfully loaded CSV: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading CSV {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    Load JSON data with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement JSON loading\n",
    "        df = spark.read.json(file_path)\n",
    "        \n",
    "        print(f\"✅ Successfully loaded JSON: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading JSON {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_parquet_data(file_path):\n",
    "    \"\"\"\n",
    "    Load Parquet data with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to Parquet file\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement Parquet loading\n",
    "        df = spark.read.parquet(file_path)\n",
    "        \n",
    "        print(f\"✅ Successfully loaded Parquet: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading Parquet {file_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Data Loading Functions:\n",
      "\n",
      "   Testing CSV loader...\n",
      "✅ Successfully loaded CSV: ../data/raw/city_zones.csv\n",
      "      Records loaded: 8\n",
      "\n",
      "   Testing JSON loader...\n",
      "✅ Successfully loaded JSON: ../data/raw/air_quality.json\n",
      "      Records loaded: 161,522\n",
      "\n",
      "   Testing Parquet loader...\n",
      "✅ Successfully loaded Parquet: ../data/raw/weather_data.parquet\n",
      "      Records loaded: 3,370\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test your loading functions\n",
    "print(\"🧪 Testing Data Loading Functions:\")\n",
    "\n",
    "test_files = [\n",
    "    (f\"{data_dir}/city_zones.csv\", \"CSV\", load_csv_data),\n",
    "    (f\"{data_dir}/air_quality.json\", \"JSON\", load_json_data), \n",
    "    (f\"{data_dir}/weather_data.parquet\", \"Parquet\", load_parquet_data)\n",
    "]\n",
    "\n",
    "for file_path, file_type, load_func in test_files:\n",
    "    print(f\"\\n   Testing {file_type} loader...\")\n",
    "    test_df = load_func(file_path)\n",
    "    if test_df:\n",
    "        print(f\"      Records loaded: {test_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3.2: Schema Definition and Enforcement (60 minutes)\n",
    "\n",
    "🎯 **TASK:** Define explicit schemas for data consistency  \n",
    "💡 **HINT:** Use StructType and StructField for schema definition  \n",
    "📚 **CONCEPTS:** Schema design, data types, schema enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# TODO: Define schema for traffic sensors\n",
    "traffic_schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    # TODO: Add remaining fields\n",
    "    StructField(\"vehicle_count\", IntegerType(), False),\n",
    "    StructField(\"avg_speed\", DoubleType(), False),\n",
    "    StructField(\"congestion_level\", StringType(), False),\n",
    "    StructField(\"road_type\", StringType(), False),\n",
    "])\n",
    "\n",
    "# TODO: Define schema for air quality data\n",
    "air_quality_schema = StructType([\n",
    "    # TODO: Define all fields for air quality data\n",
    "    # Hint: Look at the JSON structure and define appropriate types\n",
    "    StructField(\"sensor_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"location_lat\", DoubleType(), True),\n",
    "    StructField(\"location_lon\", DoubleType(), True),\n",
    "    StructField(\"pm25\", DoubleType(), True),\n",
    "    StructField(\"pm10\", DoubleType(), True),\n",
    "    StructField(\"no2\", DoubleType(), True),\n",
    "    StructField(\"co\", DoubleType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"humidity\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# TODO: Define schema for weather data\n",
    "weather_schema = StructType([\n",
    "    # TODO: Define all fields for weather data\n",
    "    StructField(\"station_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"location_lat\", DoubleType(), True),\n",
    "    StructField(\"location_lon\", DoubleType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"humidity\", DoubleType(), True),    \n",
    "    StructField(\"wind_speed\", DoubleType(), True),\n",
    "    StructField(\"wind_direction\", StringType(), True),\n",
    "    StructField(\"precipitation\", DoubleType(), True),\n",
    "    StructField(\"pressure\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# TODO: Define schema for energy data\n",
    "energy_schema = StructType([\n",
    "    # TODO: Define all fields for energy data\n",
    "    StructField(\"meter_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"location_lat\", DoubleType(), True),\n",
    "    StructField(\"location_lon\", DoubleType(), True),\n",
    "    StructField(\"energy_consumption_kwh\", DoubleType(), True),\n",
    "    StructField(\"peak_demand_kw\", DoubleType(), True),\n",
    "    StructField(\"voltage\", DoubleType(), True),\n",
    "    StructField(\"current\", DoubleType(), True),\n",
    "    StructField(\"power_factor\", DoubleType(), True),\n",
    "    StructField(\"frequency\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Testing Schema Enforcement:\n",
      "✅ Schema enforcement successful for ../data/raw/traffic_sensors.csv\n",
      "   Schema enforcement test passed!\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- no2: double (nullable = true)\n",
      " |-- co: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test schema enforcement\n",
    "print(\"\\n🔍 Testing Schema Enforcement:\")\n",
    "\n",
    "def load_with_schema(file_path, schema, file_format=\"csv\"):\n",
    "    \"\"\"Load data with explicit schema enforcement\"\"\"\n",
    "    try:\n",
    "        if file_format == \"csv\":\n",
    "            df = spark.read.schema(schema).option(\"header\", \"true\").csv(file_path)\n",
    "        elif file_format == \"json\":\n",
    "            df = spark.read.schema(schema).json(file_path)\n",
    "        elif file_format == \"parquet\":\n",
    "            df = spark.read.schema(schema).parquet(file_path)\n",
    "        \n",
    "        print(f\"✅ Schema enforcement successful for {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Schema enforcement failed for {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# TODO: Test with one of your schemas\n",
    "test_schema_df = load_with_schema(f\"{data_dir}/traffic_sensors.csv\", air_quality_schema, \"csv\")\n",
    "if test_schema_df:\n",
    "    print(\"   Schema enforcement test passed!\")\n",
    "    test_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 4: INITIAL DATA TRANSFORMATIONS (Afternoon - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔄 SECTION 4: DATA TRANSFORMATIONS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🔄 SECTION 4: DATA TRANSFORMATIONS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.1: Timestamp Standardization (45 minutes)\n",
    "\n",
    "🎯 **TASK:** Standardize timestamp formats across all datasets  \n",
    "💡 **HINT:** Some datasets may have different timestamp formats  \n",
    "📚 **CONCEPTS:** Date/time handling, format standardization, timezone handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_timestamps(df, timestamp_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Standardize timestamp column across datasets\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        timestamp_col: Name of timestamp column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with standardized timestamps\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Convert timestamps to standard format\n",
    "        standardized_df = (df\n",
    "                          .withColumn(\"timestamp_std\", F.to_timestamp(F.col(timestamp_col)))\n",
    "                          .drop(timestamp_col)\n",
    "                          .withColumnRenamed(\"timestamp_std\", timestamp_col))\n",
    "        \n",
    "        # TODO: Add derived time columns\n",
    "        result_df = (standardized_df\n",
    "                    .withColumn(\"year\", F.year(timestamp_col))\n",
    "                    .withColumn(\"month\", F.month(timestamp_col))\n",
    "                    .withColumn(\"day\", F.dayofmonth(timestamp_col))\n",
    "                    .withColumn(\"hour\", F.hour(timestamp_col))\n",
    "                    .withColumn(\"day_of_week\", F.dayofweek(timestamp_col))\n",
    "                    .withColumn(\"is_weekend\", F.when(F.dayofweek(timestamp_col).isin([1, 7]), True).otherwise(False)))\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error standardizing timestamps: {str(e)}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ Testing Timestamp Standardization:\n",
      "   Traffic data timestamp standardization:\n",
      "+--------------------+----+-----+---+----+-----------+----------+\n",
      "|           timestamp|year|month|day|hour|day_of_week|is_weekend|\n",
      "+--------------------+----+-----+---+----+-----------+----------+\n",
      "|2025-08-28 19:13:...|2025|    8| 28|  19|          5|     false|\n",
      "|2025-08-28 19:13:...|2025|    8| 28|  19|          5|     false|\n",
      "|2025-08-28 19:13:...|2025|    8| 28|  19|          5|     false|\n",
      "|2025-08-28 19:13:...|2025|    8| 28|  19|          5|     false|\n",
      "|2025-08-28 19:13:...|2025|    8| 28|  19|          5|     false|\n",
      "+--------------------+----+-----+---+----+-----------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test timestamp standardization\n",
    "print(\"⏰ Testing Timestamp Standardization:\")\n",
    "\n",
    "# Test with traffic data\n",
    "traffic_std = standardize_timestamps(traffic_df)\n",
    "print(\"   Traffic data timestamp standardization:\")\n",
    "traffic_std.select(\"timestamp\", \"year\", \"month\", \"day\", \"hour\", \"day_of_week\", \"is_weekend\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.2: Geographic Zone Mapping (45 minutes)\n",
    "\n",
    "🎯 **TASK:** Map sensor locations to city zones  \n",
    "💡 **HINT:** Join sensor coordinates with zone boundaries  \n",
    "📚 **CONCEPTS:** Spatial joins, geographic data, coordinate systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_zones(sensor_df, zones_df):\n",
    "    \"\"\"\n",
    "    Map sensor locations to city zones\n",
    "    \n",
    "    Args:\n",
    "        sensor_df: DataFrame with sensor locations (lat, lon)\n",
    "        zones_df: DataFrame with zone boundaries\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with zone information added\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Create join condition for geographic mapping\n",
    "        # A sensor is in a zone if its coordinates fall within zone boundaries\n",
    "        join_condition = (\n",
    "            (sensor_df.location_lat >= zones_df.lat_min) &\n",
    "            (sensor_df.location_lat <= zones_df.lat_max) &\n",
    "            (sensor_df.location_lon >= zones_df.lon_min) &\n",
    "            (sensor_df.location_lon <= zones_df.lon_max)\n",
    "        )\n",
    "        \n",
    "        # TODO: Perform the join\n",
    "        result_df = (sensor_df\n",
    "                    .join(zones_df, join_condition, \"left\")\n",
    "                    .select(sensor_df[\"*\"], \n",
    "                           zones_df.zone_id, \n",
    "                           zones_df.zone_name, \n",
    "                           zones_df.zone_type))\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error mapping to zones: {str(e)}\")\n",
    "        return sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗺️ Testing Geographic Zone Mapping:\n",
      "   Traffic sensors with zone mapping:\n",
      "+-----------+------------------+------------------+--------+----------+\n",
      "|  sensor_id|      location_lat|      location_lon| zone_id| zone_type|\n",
      "+-----------+------------------+------------------+--------+----------+\n",
      "|TRAFFIC_001| 40.78941849302994| -74.0041930409108|    NULL|      NULL|\n",
      "|TRAFFIC_002| 40.77304212963235|-73.91903939743946|    NULL|      NULL|\n",
      "|TRAFFIC_003|40.797412843124846|-73.91180691102876|    NULL|      NULL|\n",
      "|TRAFFIC_004| 40.76447004554071|-73.91102045449436|    NULL|      NULL|\n",
      "|TRAFFIC_005|  40.7580006756147|-73.94232435909976|ZONE_008|    retail|\n",
      "|TRAFFIC_006|40.705001678241466|-73.98101112420137|    NULL|      NULL|\n",
      "|TRAFFIC_007| 40.75002147634578|-73.90253544928802|    NULL|      NULL|\n",
      "|TRAFFIC_008| 40.75922357184977|-74.01290766686316|ZONE_005|industrial|\n",
      "|TRAFFIC_009| 40.72301729656365|-73.92172955478621|    NULL|      NULL|\n",
      "|TRAFFIC_010|40.781269783589245|-73.92301264097411|    NULL|      NULL|\n",
      "+-----------+------------------+------------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "   Sensors by zone type:\n",
      "+-----------+-----+\n",
      "|  zone_type|count|\n",
      "+-----------+-----+\n",
      "|       NULL|72612|\n",
      "| commercial|10085|\n",
      "|residential| 6051|\n",
      "|      mixed| 4034|\n",
      "|     retail| 4034|\n",
      "| industrial| 4034|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test zone mapping\n",
    "print(\"\\n🗺️ Testing Geographic Zone Mapping:\")\n",
    "\n",
    "# Test with traffic sensors\n",
    "traffic_with_zones = map_to_zones(traffic_std, zones_df)\n",
    "print(\"   Traffic sensors with zone mapping:\")\n",
    "traffic_with_zones.select(\"sensor_id\", \"location_lat\", \"location_lon\", \"zone_id\", \"zone_type\").show(10)\n",
    "\n",
    "# TODO: Verify mapping worked correctly\n",
    "zone_distribution = traffic_with_zones.groupBy(\"zone_type\").count().orderBy(F.desc(\"count\"))\n",
    "print(\"   Sensors by zone type:\")\n",
    "zone_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.3: Data Type Conversions and Validations (30 minutes)\n",
    "\n",
    "🎯 **TASK:** Ensure proper data types and add validation columns  \n",
    "💡 **HINT:** Cast columns to appropriate types, add data quality flags  \n",
    "📚 **CONCEPTS:** Data type conversion, validation rules, data quality flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_quality_flags(df, sensor_type):\n",
    "    \"\"\"\n",
    "    Add data quality validation flags to DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        sensor_type: Type of sensor for specific validations\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with quality flags added\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result_df = df\n",
    "        \n",
    "        # TODO: Add general quality flags\n",
    "        result_df = result_df.withColumn(\"has_missing_values\", \n",
    "                                        F.when(F.col(\"sensor_id\").isNull(), True).otherwise(False))\n",
    "        \n",
    "        # TODO: Add sensor-specific validations\n",
    "        if sensor_type == \"traffic\":\n",
    "            # Traffic-specific validations\n",
    "            result_df = (result_df\n",
    "                        .withColumn(\"valid_speed\", \n",
    "                                   F.when((F.col(\"avg_speed\") >= 0) & (F.col(\"avg_speed\") <= 100), True).otherwise(False))\n",
    "                        .withColumn(\"valid_vehicle_count\",\n",
    "                                   F.when(F.col(\"vehicle_count\") >= 0, True).otherwise(False)))\n",
    "        \n",
    "        elif sensor_type == \"air_quality\":\n",
    "            # Air quality specific validations\n",
    "            result_df = (result_df\n",
    "                        .withColumn(\"valid_pm25\",\n",
    "                                   F.when((F.col(\"pm25\") >= 0) & (F.col(\"pm25\") <= 500), True).otherwise(False))\n",
    "                        .withColumn(\"valid_temperature\",\n",
    "                                   F.when((F.col(\"temperature\") >= -50) & (F.col(\"temperature\") <= 50), True).otherwise(False)))\n",
    "        \n",
    "        # TODO: Add more sensor-specific validations\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error adding quality flags: {str(e)}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏷️ Testing Data Quality Flags:\n",
      "   Traffic data with quality flags:\n",
      "+-----------+------------------+-------------+-----------+-------------------+\n",
      "|  sensor_id|         avg_speed|vehicle_count|valid_speed|valid_vehicle_count|\n",
      "+-----------+------------------+-------------+-----------+-------------------+\n",
      "|TRAFFIC_001|16.417025239365007|           32|       true|               true|\n",
      "|TRAFFIC_002|27.763808497333677|           25|       true|               true|\n",
      "|TRAFFIC_003|13.221915930299499|           31|       true|               true|\n",
      "|TRAFFIC_004|30.487846213776308|           28|       true|               true|\n",
      "|TRAFFIC_005| 25.34875715626183|           32|       true|               true|\n",
      "|TRAFFIC_006| 39.08658395648244|           31|       true|               true|\n",
      "|TRAFFIC_007|16.090411274953127|           69|       true|               true|\n",
      "|TRAFFIC_008|  23.6876171366908|           23|       true|               true|\n",
      "|TRAFFIC_009|              10.0|           23|       true|               true|\n",
      "|TRAFFIC_010|30.867197348104607|           49|       true|               true|\n",
      "+-----------+------------------+-------------+-----------+-------------------+\n",
      "only showing top 10 rows\n",
      "   Quality statistics:\n",
      "+-----------------+-------------------------+-------------+\n",
      "|valid_speed_count|valid_vehicle_count_count|total_records|\n",
      "+-----------------+-------------------------+-------------+\n",
      "|           100795|                   100850|       100850|\n",
      "+-----------------+-------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test data quality flags\n",
    "print(\"\\n🏷️ Testing Data Quality Flags:\")\n",
    "\n",
    "# Test with traffic data\n",
    "traffic_with_flags = add_data_quality_flags(traffic_with_zones, \"traffic\")\n",
    "print(\"   Traffic data with quality flags:\")\n",
    "traffic_with_flags.select(\"sensor_id\", \"avg_speed\", \"vehicle_count\", \"valid_speed\", \"valid_vehicle_count\").show(10)\n",
    "\n",
    "# TODO: Check quality flag distribution\n",
    "quality_stats = (traffic_with_flags\n",
    "                .agg(F.sum(F.when(F.col(\"valid_speed\"), 1).otherwise(0)).alias(\"valid_speed_count\"),\n",
    "                     F.sum(F.when(F.col(\"valid_vehicle_count\"), 1).otherwise(0)).alias(\"valid_vehicle_count_count\"),\n",
    "                     F.count(\"*\").alias(\"total_records\")))\n",
    "\n",
    "print(\"   Quality statistics:\")\n",
    "quality_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# DAY 1 DELIVERABLES & CHECKPOINTS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📋 DAY 1 COMPLETION CHECKLIST\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📋 DAY 1 COMPLETION CHECKLIST\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ COMPLETION STATUS:\n",
      "   ✅ Spark Session Created\n",
      "   ✅ Database Connection Tested\n",
      "   ✅ Data Loaded Successfully\n",
      "   ✅ Data Quality Assessed\n",
      "   ✅ Loading Functions Created\n",
      "   ✅ Schemas Defined\n",
      "   ✅ Timestamp Standardization Working\n",
      "   ✅ Zone Mapping Implemented\n",
      "   ✅ Quality Flags Added\n",
      "\n",
      "📊 Overall Completion: 100.0%\n",
      "🎉 Great job! You're ready for Day 2!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Complete this checklist by running the validation functions\n",
    "\n",
    "def validate_day1_completion():\n",
    "    \"\"\"Validate that Day 1 objectives have been met\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"spark_session_created\": False,\n",
    "        \"database_connection_tested\": False,\n",
    "        \"data_loaded_successfully\": False,\n",
    "        \"data_quality_assessed\": False,\n",
    "        \"loading_functions_created\": False,\n",
    "        \"schemas_defined\": False,\n",
    "        \"timestamp_standardization_working\": False,\n",
    "        \"zone_mapping_implemented\": False,\n",
    "        \"quality_flags_added\": False\n",
    "    }\n",
    "    \n",
    "    # TODO: Add validation logic for each item\n",
    "    try:\n",
    "        # Check Spark session\n",
    "        if spark and spark.sparkContext._jsc:\n",
    "            checklist[\"spark_session_created\"] = True\n",
    "            \n",
    "        # Check if data exists\n",
    "        if ('traffic_df' in globals() and traffic_df.count() > 0) and ('weather_df' in globals() and weather_df.count() > 0) and ('air_quality_df' in globals() and air_quality_df is not None and air_quality_df.count() > 0) and ('energy_df' in globals() and energy_df.count() > 0) and ('zones_df' in globals() and zones_df.count() > 0):\n",
    "            checklist[\"data_loaded_successfully\"] = True\n",
    "            \n",
    "        # TODO: Add more validation checks\n",
    "        if 'db_connected' in globals() and db_connected:\n",
    "            checklist[\"database_connection_tested\"] = True\n",
    "\n",
    "        if  \"assess_data_quality\" in globals():\n",
    "            checklist[\"data_quality_assessed\"] = True\n",
    "        \n",
    "        loading_functions = ['load_csv_data', 'load_json_data', 'load_parquet_data']\n",
    "        if all(func in globals() for func in loading_functions):\n",
    "            checklist[\"loading_functions_created\"] = True\n",
    "        \n",
    "        schema_vars = ['traffic_schema', 'air_quality_schema', 'weather_schema', 'energy_schema']\n",
    "        if all(schema in globals() for schema in schema_vars):\n",
    "            checklist[\"schemas_defined\"] = True\n",
    "        \n",
    "        if 'standardize_timestamps' in globals() and 'traffic_std' in globals():\n",
    "            checklist[\"timestamp_standardization_working\"] = True\n",
    "        \n",
    "        if 'map_to_zones' in globals() and 'traffic_with_zones' in globals():\n",
    "            checklist[\"zone_mapping_implemented\"] = True\n",
    "         \n",
    "        if 'add_data_quality_flags' in globals() and 'traffic_with_flags' in globals():\n",
    "            checklist[\"quality_flags_added\"] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Validation error: {str(e)}\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"✅ COMPLETION STATUS:\")\n",
    "    for item, status in checklist.items():\n",
    "        status_icon = \"✅\" if status else \"❌\"\n",
    "        print(f\"   {status_icon} {item.replace('_', ' ').title()}\")\n",
    "    \n",
    "    import builtins\n",
    "    completion_rate = builtins.sum(checklist.values()) / len(checklist) * 100\n",
    "    print(f\"\\n📊 Overall Completion: {completion_rate:.1f}%\")\n",
    "    \n",
    "    if completion_rate >= 80:\n",
    "        print(\"🎉 Great job! You're ready for Day 2!\")\n",
    "    else:\n",
    "        print(\"📝 Please review incomplete items before proceeding to Day 2.\")\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# TODO: Run the validation\n",
    "completion_status = validate_day1_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🚀 WHAT'S NEXT?\n",
    "\n",
    "---\n",
    "\n",
    "## 📅 DAY 2 PREVIEW: Data Quality & Cleaning Pipeline\n",
    "\n",
    "Tomorrow you'll work on:\n",
    "1. 🔍 Comprehensive data quality assessment\n",
    "2. 🧹 Advanced cleaning procedures for IoT sensor data  \n",
    "3. 📊 Missing data handling and interpolation strategies\n",
    "4. 🚨 Outlier detection and treatment methods\n",
    "5. 📏 Data standardization and normalization\n",
    "\n",
    "## 📚 RECOMMENDED PREPARATION:\n",
    "- Review PySpark DataFrame operations\n",
    "- Read about time series data quality challenges\n",
    "- Familiarize yourself with statistical outlier detection methods\n",
    "\n",
    "## 💾 SAVE YOUR WORK:\n",
    "- Commit your notebook to Git\n",
    "- Document any issues or questions for tomorrow\n",
    "- Save any custom functions you created\n",
    "\n",
    "## 🤝 QUESTIONS?\n",
    "- Post in the class discussion forum\n",
    "- Review Spark documentation for any unclear concepts\n",
    "- Prepare questions for tomorrow's Q&A session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save your progress\n",
    "print(\"\\n💾 Don't forget to save your notebook and commit your changes!\")\n",
    "\n",
    "# Clean up (optional)\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
