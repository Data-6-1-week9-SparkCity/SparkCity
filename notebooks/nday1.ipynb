{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Environment Setup & Data Exploration\n",
    "# Smart City IoT Analytics Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ LEARNING OBJECTIVES:\n",
    "- Configure Spark cluster and development environment\n",
    "- Understand IoT data characteristics and challenges  \n",
    "- Implement basic data ingestion patterns\n",
    "- Explore PySpark DataFrame operations\n",
    "\n",
    "## üìÖ SCHEDULE:\n",
    "**Morning (4 hours):**\n",
    "1. Environment Setup (2 hours)\n",
    "2. Data Exploration (2 hours)\n",
    "\n",
    "**Afternoon (4 hours):**  \n",
    "3. Basic Data Ingestion (2 hours)\n",
    "4. Initial Data Transformations (2 hours)\n",
    "\n",
    "## ‚úÖ DELIVERABLES:\n",
    "- Working Spark cluster with all services running\n",
    "- Data ingestion notebook with basic EDA\n",
    "- Documentation of data quality findings  \n",
    "- Initial data loading pipeline functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Welcome to the Smart City IoT Analytics Pipeline!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Welcome to the Smart City IoT Analytics Pipeline!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 1: ENVIRONMENT SETUP (Morning - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.1: Initialize Spark Session (15 minutes)\n",
    "\n",
    "üéØ **TASK:** Create a Spark session configured for local development  \n",
    "üí° **HINT:** Use SparkSession.builder with appropriate configurations  \n",
    "üìö **DOCS:** https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "\n",
    "**TODO:** Create Spark session with the following configurations:\n",
    "- App name: \"SmartCityIoTPipeline-Day1\"\n",
    "- Master: \"local[*]\" (use all available cores)\n",
    "- Memory: \"4g\" for driver\n",
    "- Additional configs for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/04 15:31:26 WARN Utils: Your hostname, Zipcoders-MacBook-Pro-3.local, resolves to a loopback address: 127.0.0.1; using 192.168.8.135 instead (on interface en0)\n",
      "25/09/04 15:31:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/04 15:31:26 WARN Utils: Your hostname, Zipcoders-MacBook-Pro-3.local, resolves to a loopback address: 127.0.0.1; using 192.168.8.135 instead (on interface en0)\n",
      "25/09/04 15:31:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/04 15:31:26 WARN Utils: Your hostname, Zipcoders-MacBook-Pro-3.local, resolves to a loopback address: 127.0.0.1; using 192.168.8.135 instead (on interface en0)\n",
      "25/09/04 15:31:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/09/04 15:31:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/04 15:31:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/04 15:31:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Details:\n",
      "   App Name: SmartCityIoTPipeline-Day1\n",
      "   Spark Version: 4.0.0\n",
      "   Master: local[*]\n",
      "   Default Parallelism: 8\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create Spark session with the following configurations:\n",
    "jdbc_jar_path = \"/Users/sai/Documents/Projects/ninth-week/SparkCity/postgresql-42.7.3.jar\"\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"SmartCityIoTPipeline-Day1\")  # TODO: Add your app name\n",
    "         .master(\"local[*]\")   # TODO: Add master configuration\n",
    "         .config(\"spark.driver.memory\", \"4g\")  # TODO: Set memory\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "         .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "         .config(\"spark.jars\", jdbc_jar_path)  # <-- Add this line for JDBC driver\n",
    "         .getOrCreate())\n",
    "\n",
    "# TODO: Verify Spark session is working\n",
    "print(\"‚úÖ Spark Session Details:\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   Default Parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.2: Verify Infrastructure (15 minutes)\n",
    "\n",
    "üéØ **TASK:** Check that all infrastructure services are running  \n",
    "üí° **HINT:** Test database connectivity and file system access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection successful!\n",
      "\n",
      "üåê Spark UI should be accessible at: http://localhost:4040\n",
      "   (Open this in your browser to monitor Spark jobs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TODO: Test PostgreSQL connection\n",
    "def test_database_connection():\n",
    "    \"\"\"Test connection to PostgreSQL database\"\"\"\n",
    "    try:\n",
    "        # Database connection parameters\n",
    "        db_properties = {\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"password\", \n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "        \n",
    "        # TODO: Replace with actual connection test\n",
    "        # Test query - should create a simple DataFrame from database\n",
    "        test_df = spark.read.jdbc(\n",
    "            url=\"jdbc:postgresql://localhost:5432/smartcity\",\n",
    "            table=\"(SELECT 1 as test_column) as test_table\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "        \n",
    "        # TODO: Collect and display result\n",
    "        result = test_df.collect()\n",
    "        print(\"‚úÖ Database connection successful!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database connection failed: {str(e)}\")\n",
    "        print(\"üí° Make sure PostgreSQL container is running: docker-compose up -d\")\n",
    "        return False\n",
    "\n",
    "# TODO: Run the database connection test\n",
    "db_connected = test_database_connection()\n",
    "\n",
    "# TODO: Check Spark UI accessibility\n",
    "print(\"\\nüåê Spark UI should be accessible at: http://localhost:4040\")\n",
    "print(\"   (Open this in your browser to monitor Spark jobs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.3: Generate Sample Data (30 minutes)\n",
    "\n",
    "üéØ **TASK:** Run the data generation script to create sample IoT data  \n",
    "üí° **HINT:** Use the provided data generation script or run it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating data... (0/5 files exist)\n",
      "   Project root: /Users/sai/Documents/Projects/ninth-week/SparkCity\n",
      "   Running script from: /Users/sai/Documents/Projects/ninth-week/SparkCity\n",
      "‚úÖ Data generation successful!\n",
      "‚úÖ Data generation successful!\n",
      "‚úÖ Data generation successful!\n"
     ]
    }
   ],
   "source": [
    "# Generate Sample IoT Data\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def check_and_generate_data():\n",
    "    \"\"\"Check if data exists, generate if missing\"\"\"\n",
    "    data_files = [\"traffic_sensors.csv\", \"air_quality.json\", \"weather_data.parquet\", \n",
    "                  \"energy_meters.csv\", \"city_zones.csv\"]\n",
    "    data_path = \"data/raw\"\n",
    "    \n",
    "    # Check existing files\n",
    "    existing = [f for f in data_files if os.path.exists(f\"{data_path}/{f}\")]\n",
    "    \n",
    "    if len(existing) == len(data_files):\n",
    "        print(f\"‚úÖ All {len(data_files)} data files found!\")\n",
    "        return True\n",
    "    \n",
    "    # Generate missing data\n",
    "    print(f\"üîÑ Generating data... ({len(existing)}/{len(data_files)} files exist)\")\n",
    "    \n",
    "    try:\n",
    "        # Get the project root (go up one level from notebooks folder)\n",
    "        notebook_dir = os.getcwd()  # Current directory (notebooks/)\n",
    "        project_root = os.path.dirname(notebook_dir)  # Go up one level to SparkCity/\n",
    "        \n",
    "        print(f\"   Project root: {project_root}\")\n",
    "        print(f\"   Running script from: {project_root}\")\n",
    "        \n",
    "        # Run from project root directory\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"scripts/generate_data.py\"], \n",
    "            cwd=project_root,  # Run from SparkCity/ directory\n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            timeout=300\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Data generation successful!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Generation failed!\")\n",
    "            if result.stderr:\n",
    "                print(f\"   Error: {result.stderr.strip()}\")\n",
    "            if result.stdout:\n",
    "                print(f\"   Output: {result.stdout.strip()}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run data check/generation\n",
    "data_ready = check_and_generate_data()\n",
    "\n",
    "# If failed, provide clear manual instructions\n",
    "if not data_ready:\n",
    "    print(\"\\nüîß MANUAL FIX:\")\n",
    "    print(\"1. Open terminal\")\n",
    "    print(\"2. Run: cd /Users/sai/Documents/Projects/ninth-week/SparkCity\")\n",
    "    print(\"3. Run: python scripts/generate_data.py\")\n",
    "    print(\"4. Re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 2: DATA EXPLORATION (Morning - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä SECTION 2: EXPLORATORY DATA ANALYSIS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä SECTION 2: EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.1: Load and Examine Data Sources (45 minutes)\n",
    "\n",
    "üéØ **TASK:** Load each data source and examine its structure  \n",
    "üí° **HINT:** Use appropriate Spark readers for different file formats  \n",
    "üìö **CONCEPTS:** Schema inference, file formats, data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = \"../data/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Loading City Zones Reference Data...\n",
      "   üìä Records: 8\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- zone_id: string (nullable = true)\n",
      " |-- zone_name: string (nullable = true)\n",
      " |-- zone_type: string (nullable = true)\n",
      " |-- lat_min: double (nullable = true)\n",
      " |-- lat_max: double (nullable = true)\n",
      " |-- lon_min: double (nullable = true)\n",
      " |-- lon_max: double (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|zone_id |zone_name         |zone_type  |lat_min|lat_max|lon_min|lon_max|population|\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|ZONE_001|Downtown          |commercial |40.72  |40.74  |-74.01 |-73.99 |25000     |\n",
      "|ZONE_002|Financial District|commercial |40.7   |40.72  |-74.02 |-74.0  |15000     |\n",
      "|ZONE_003|Residential North |residential|40.76  |40.8   |-74.0  |-73.98 |45000     |\n",
      "|ZONE_004|Residential South |residential|40.7   |40.72  |-73.98 |-73.96 |38000     |\n",
      "|ZONE_005|Industrial Park   |industrial |40.74  |40.76  |-74.02 |-74.0  |5000      |\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "only showing top 5 rows\n",
      "   üìä Records: 8\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- zone_id: string (nullable = true)\n",
      " |-- zone_name: string (nullable = true)\n",
      " |-- zone_type: string (nullable = true)\n",
      " |-- lat_min: double (nullable = true)\n",
      " |-- lat_max: double (nullable = true)\n",
      " |-- lon_min: double (nullable = true)\n",
      " |-- lon_max: double (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|zone_id |zone_name         |zone_type  |lat_min|lat_max|lon_min|lon_max|population|\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|ZONE_001|Downtown          |commercial |40.72  |40.74  |-74.01 |-73.99 |25000     |\n",
      "|ZONE_002|Financial District|commercial |40.7   |40.72  |-74.02 |-74.0  |15000     |\n",
      "|ZONE_003|Residential North |residential|40.76  |40.8   |-74.0  |-73.98 |45000     |\n",
      "|ZONE_004|Residential South |residential|40.7   |40.72  |-73.98 |-73.96 |38000     |\n",
      "|ZONE_005|Industrial Park   |industrial |40.74  |40.76  |-74.02 |-74.0  |5000      |\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "only showing top 5 rows\n",
      "   üìä Records: 8\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- zone_id: string (nullable = true)\n",
      " |-- zone_name: string (nullable = true)\n",
      " |-- zone_type: string (nullable = true)\n",
      " |-- lat_min: double (nullable = true)\n",
      " |-- lat_max: double (nullable = true)\n",
      " |-- lon_min: double (nullable = true)\n",
      " |-- lon_max: double (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|zone_id |zone_name         |zone_type  |lat_min|lat_max|lon_min|lon_max|population|\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|ZONE_001|Downtown          |commercial |40.72  |40.74  |-74.01 |-73.99 |25000     |\n",
      "|ZONE_002|Financial District|commercial |40.7   |40.72  |-74.02 |-74.0  |15000     |\n",
      "|ZONE_003|Residential North |residential|40.76  |40.8   |-74.0  |-73.98 |45000     |\n",
      "|ZONE_004|Residential South |residential|40.7   |40.72  |-73.98 |-73.96 |38000     |\n",
      "|ZONE_005|Industrial Park   |industrial |40.74  |40.76  |-74.02 |-74.0  |5000      |\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load city zones reference data\n",
    "print(\"üìç Loading City Zones Reference Data...\")\n",
    "try:\n",
    "    zones_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/city_zones.csv\")\n",
    "    \n",
    "    # TODO: Display basic information about zones\n",
    "    print(f\"   üìä Records: {zones_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    zones_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    zones_df.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading zones data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöó Loading Traffic Sensors Data...\n",
      "   üìä Records: 100850\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- vehicle_count: integer (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      " |-- congestion_level: string (nullable = true)\n",
      " |-- road_type: string (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+-------------+-----------------+----------------+----------+\n",
      "|  sensor_id|           timestamp|      location_lat|      location_lon|vehicle_count|        avg_speed|congestion_level| road_type|\n",
      "+-----------+--------------------+------------------+------------------+-------------+-----------------+----------------+----------+\n",
      "|TRAFFIC_001|2025-08-28 15:31:...| 40.74093518187665|-73.98279176603494|           16|61.78683853103945|          medium|  arterial|\n",
      "|TRAFFIC_002|2025-08-28 15:31:...|  40.7461636412382|-73.92950528259067|            9|52.06105507594765|             low|  arterial|\n",
      "|TRAFFIC_003|2025-08-28 15:31:...|40.729430293125645|-74.01479947116061|           17|58.64389727015157|          medium|commercial|\n",
      "|TRAFFIC_004|2025-08-28 15:31:...| 40.78974724014799|-73.90010522496067|           38|57.08453335406747|          medium|   highway|\n",
      "|TRAFFIC_005|2025-08-28 15:31:...|  40.7972361740753|  -73.952672406209|           23|62.62391638831053|          medium|commercial|\n",
      "+-----------+--------------------+------------------+------------------+-------------+-----------------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "   üìä Records: 100850\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- vehicle_count: integer (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      " |-- congestion_level: string (nullable = true)\n",
      " |-- road_type: string (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+-------------+-----------------+----------------+----------+\n",
      "|  sensor_id|           timestamp|      location_lat|      location_lon|vehicle_count|        avg_speed|congestion_level| road_type|\n",
      "+-----------+--------------------+------------------+------------------+-------------+-----------------+----------------+----------+\n",
      "|TRAFFIC_001|2025-08-28 15:31:...| 40.74093518187665|-73.98279176603494|           16|61.78683853103945|          medium|  arterial|\n",
      "|TRAFFIC_002|2025-08-28 15:31:...|  40.7461636412382|-73.92950528259067|            9|52.06105507594765|             low|  arterial|\n",
      "|TRAFFIC_003|2025-08-28 15:31:...|40.729430293125645|-74.01479947116061|           17|58.64389727015157|          medium|commercial|\n",
      "|TRAFFIC_004|2025-08-28 15:31:...| 40.78974724014799|-73.90010522496067|           38|57.08453335406747|          medium|   highway|\n",
      "|TRAFFIC_005|2025-08-28 15:31:...|  40.7972361740753|  -73.952672406209|           23|62.62391638831053|          medium|commercial|\n",
      "+-----------+--------------------+------------------+------------------+-------------+-----------------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "   üìä Records: 100850\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- vehicle_count: integer (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      " |-- congestion_level: string (nullable = true)\n",
      " |-- road_type: string (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+-------------+-----------------+----------------+----------+\n",
      "|  sensor_id|           timestamp|      location_lat|      location_lon|vehicle_count|        avg_speed|congestion_level| road_type|\n",
      "+-----------+--------------------+------------------+------------------+-------------+-----------------+----------------+----------+\n",
      "|TRAFFIC_001|2025-08-28 15:31:...| 40.74093518187665|-73.98279176603494|           16|61.78683853103945|          medium|  arterial|\n",
      "|TRAFFIC_002|2025-08-28 15:31:...|  40.7461636412382|-73.92950528259067|            9|52.06105507594765|             low|  arterial|\n",
      "|TRAFFIC_003|2025-08-28 15:31:...|40.729430293125645|-74.01479947116061|           17|58.64389727015157|          medium|commercial|\n",
      "|TRAFFIC_004|2025-08-28 15:31:...| 40.78974724014799|-73.90010522496067|           38|57.08453335406747|          medium|   highway|\n",
      "|TRAFFIC_005|2025-08-28 15:31:...|  40.7972361740753|  -73.952672406209|           23|62.62391638831053|          medium|commercial|\n",
      "+-----------+--------------------+------------------+------------------+-------------+-----------------+----------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load traffic sensors data  \n",
    "print(\"\\nüöó Loading Traffic Sensors Data...\")\n",
    "try:\n",
    "    # TODO: Load CSV file with proper options\n",
    "    traffic_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/traffic_sensors.csv\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   üìä Records: {traffic_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    traffic_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    traffic_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading traffic data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå´Ô∏è Loading Air Quality Data...\n",
      "   üîÑ Using Spark multiline JSON option...\n",
      "   üìä Records loaded: 13460\n",
      "   üîÑ Converting timestamp from string to TimestampType...\n",
      "   ‚úÖ All timestamps converted successfully\n",
      "   ‚úÖ Air quality data loaded and processed successfully!\n",
      "   üìä Records loaded: 13460\n",
      "   üîÑ Converting timestamp from string to TimestampType...\n",
      "   ‚úÖ All timestamps converted successfully\n",
      "   ‚úÖ Air quality data loaded and processed successfully!\n",
      "   üìä Records loaded: 13460\n",
      "   üîÑ Converting timestamp from string to TimestampType...\n",
      "   ‚úÖ All timestamps converted successfully\n",
      "   ‚úÖ Air quality data loaded and processed successfully!\n",
      "\n",
      "‚úÖ Air Quality Data Loaded Successfully:\n",
      "   üìä Records: 13460\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- co: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- no2: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "\n",
      "‚úÖ Air Quality Data Loaded Successfully:\n",
      "   üìä Records: 13460\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- co: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- no2: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "\n",
      "‚úÖ Air Quality Data Loaded Successfully:\n",
      "   üìä Records: 13460\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- co: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- no2: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+---------+------------------+--------------------+\n",
      "|                 co|          humidity|      location_lat|      location_lon|               no2|              pm10|             pm25|sensor_id|       temperature|           timestamp|\n",
      "+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+---------+------------------+--------------------+\n",
      "| 0.8102206331683751|57.795174785669445| 40.71975952527044|-73.97899952059666| 35.97232087129134|21.887538933453506|9.229380234427648|   AQ_001| 7.380564531998665|2025-08-28 15:31:...|\n",
      "| 1.5849971823736337| 42.30079385564934|  40.7822776176851|-73.90676548494977|59.636319712260146| 54.82619253179658|24.31623702295971|   AQ_002|23.993634983085336|2025-08-28 15:31:...|\n",
      "|0.26055017966013405| 52.55050862138038|40.721407985223124|-73.91324712916987| 49.84885446987872|49.314394244281864|22.63554260213728|   AQ_003|14.231343816283161|2025-08-28 15:31:...|\n",
      "| 1.5143204379145476| 59.94090763228316| 40.77361540101013|-73.95745054804551|17.470870606516737| 20.20121883227064|18.85468785296067|   AQ_004|2.1876089309897964|2025-08-28 15:31:...|\n",
      "| 0.4174260962507127| 63.42399484270446|40.700214104383555|-73.99710072631312|31.662580666391726| 48.29316534250206|39.64152595578063|   AQ_005|13.858790948416976|2025-08-28 15:31:...|\n",
      "+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+---------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "   ‚úÖ Timestamp column type: TimestampType()\n",
      "+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+---------+------------------+--------------------+\n",
      "|                 co|          humidity|      location_lat|      location_lon|               no2|              pm10|             pm25|sensor_id|       temperature|           timestamp|\n",
      "+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+---------+------------------+--------------------+\n",
      "| 0.8102206331683751|57.795174785669445| 40.71975952527044|-73.97899952059666| 35.97232087129134|21.887538933453506|9.229380234427648|   AQ_001| 7.380564531998665|2025-08-28 15:31:...|\n",
      "| 1.5849971823736337| 42.30079385564934|  40.7822776176851|-73.90676548494977|59.636319712260146| 54.82619253179658|24.31623702295971|   AQ_002|23.993634983085336|2025-08-28 15:31:...|\n",
      "|0.26055017966013405| 52.55050862138038|40.721407985223124|-73.91324712916987| 49.84885446987872|49.314394244281864|22.63554260213728|   AQ_003|14.231343816283161|2025-08-28 15:31:...|\n",
      "| 1.5143204379145476| 59.94090763228316| 40.77361540101013|-73.95745054804551|17.470870606516737| 20.20121883227064|18.85468785296067|   AQ_004|2.1876089309897964|2025-08-28 15:31:...|\n",
      "| 0.4174260962507127| 63.42399484270446|40.700214104383555|-73.99710072631312|31.662580666391726| 48.29316534250206|39.64152595578063|   AQ_005|13.858790948416976|2025-08-28 15:31:...|\n",
      "+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+---------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "   ‚úÖ Timestamp column type: TimestampType()\n",
      "+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+---------+------------------+--------------------+\n",
      "|                 co|          humidity|      location_lat|      location_lon|               no2|              pm10|             pm25|sensor_id|       temperature|           timestamp|\n",
      "+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+---------+------------------+--------------------+\n",
      "| 0.8102206331683751|57.795174785669445| 40.71975952527044|-73.97899952059666| 35.97232087129134|21.887538933453506|9.229380234427648|   AQ_001| 7.380564531998665|2025-08-28 15:31:...|\n",
      "| 1.5849971823736337| 42.30079385564934|  40.7822776176851|-73.90676548494977|59.636319712260146| 54.82619253179658|24.31623702295971|   AQ_002|23.993634983085336|2025-08-28 15:31:...|\n",
      "|0.26055017966013405| 52.55050862138038|40.721407985223124|-73.91324712916987| 49.84885446987872|49.314394244281864|22.63554260213728|   AQ_003|14.231343816283161|2025-08-28 15:31:...|\n",
      "| 1.5143204379145476| 59.94090763228316| 40.77361540101013|-73.95745054804551|17.470870606516737| 20.20121883227064|18.85468785296067|   AQ_004|2.1876089309897964|2025-08-28 15:31:...|\n",
      "| 0.4174260962507127| 63.42399484270446|40.700214104383555|-73.99710072631312|31.662580666391726| 48.29316534250206|39.64152595578063|   AQ_005|13.858790948416976|2025-08-28 15:31:...|\n",
      "+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+---------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "   ‚úÖ Timestamp column type: TimestampType()\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load air quality data (JSON format) - FIXED VERSION WITH TIMESTAMP CONVERSION\n",
    "print(\"\\nüå´Ô∏è Loading Air Quality Data...\")\n",
    "\n",
    "def load_air_quality_data_fixed(file_path):\n",
    "    \"\"\"Load air quality data with proper JSON array handling and timestamp conversion\"\"\"\n",
    "    try:\n",
    "        # Method 1: Use Spark's multiline JSON option\n",
    "        print(\"   üîÑ Using Spark multiline JSON option...\")\n",
    "        \n",
    "        air_quality_df = (spark.read\n",
    "                         .option(\"multiline\", \"true\")\n",
    "                         .option(\"mode\", \"DROPMALFORMED\")\n",
    "                         .json(file_path))\n",
    "        \n",
    "        record_count = air_quality_df.count()\n",
    "        print(f\"   üìä Records loaded: {record_count}\")\n",
    "        \n",
    "        if record_count > 0:\n",
    "            # Convert string timestamp to proper TimestampType\n",
    "            print(\"   üîÑ Converting timestamp from string to TimestampType...\")\n",
    "            air_quality_df = air_quality_df.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\"))\n",
    "            \n",
    "            # Verify timestamp conversion worked\n",
    "            timestamp_nulls = air_quality_df.filter(F.col(\"timestamp\").isNull()).count()\n",
    "            if timestamp_nulls > 0:\n",
    "                print(f\"   ‚ö†Ô∏è  Warning: {timestamp_nulls} records have null timestamps after conversion\")\n",
    "            else:\n",
    "                print(\"   ‚úÖ All timestamps converted successfully\")\n",
    "            \n",
    "            print(f\"   ‚úÖ Air quality data loaded and processed successfully!\")\n",
    "            return air_quality_df\n",
    "        else:\n",
    "            print(\"   ‚ùå No records found in file\")\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Method failed: {str(e)}\")\n",
    "        \n",
    "    # Fallback: Try with explicit schema\n",
    "    try:\n",
    "        print(\"   üîÑ Fallback: Using explicit schema...\")\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "        \n",
    "        # Define schema with string timestamp (to be converted later)\n",
    "        air_quality_schema = StructType([\n",
    "            StructField(\"sensor_id\", StringType(), True),\n",
    "            StructField(\"timestamp\", TimestampType(), True),  # Load as string first\n",
    "            StructField(\"location_lat\", DoubleType(), True),\n",
    "            StructField(\"location_lon\", DoubleType(), True),\n",
    "            StructField(\"pm25\", DoubleType(), True),\n",
    "            StructField(\"pm10\", DoubleType(), True),\n",
    "            StructField(\"no2\", DoubleType(), True),\n",
    "            StructField(\"co\", DoubleType(), True),\n",
    "            StructField(\"temperature\", DoubleType(), True),\n",
    "            StructField(\"humidity\", DoubleType(), True)\n",
    "        ])\n",
    "        \n",
    "        air_quality_df = (spark.read\n",
    "                         .schema(air_quality_schema)\n",
    "                         .option(\"multiline\", \"true\")\n",
    "                         .option(\"mode\", \"DROPMALFORMED\")\n",
    "                         .json(file_path))\n",
    "        \n",
    "        # Convert timestamp\n",
    "        air_quality_df = air_quality_df.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\"))\n",
    "        \n",
    "        record_count = air_quality_df.count()\n",
    "        print(f\"   ‚úÖ Fallback successful: {record_count} records\")\n",
    "        return air_quality_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Fallback failed: {str(e)}\")\n",
    "    \n",
    "    print(\"   ‚ùå All methods failed!\")\n",
    "    return None\n",
    "\n",
    "# Load air quality data with the fixed method\n",
    "air_quality_df = load_air_quality_data_fixed(f\"{data_dir}/air_quality.json\")\n",
    "\n",
    "if air_quality_df and air_quality_df.count() > 0:\n",
    "    print(f\"\\n‚úÖ Air Quality Data Loaded Successfully:\")\n",
    "    print(f\"   üìä Records: {air_quality_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    air_quality_df.printSchema()\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    air_quality_df.show(5)\n",
    "    \n",
    "    # Verify timestamp type\n",
    "    timestamp_field = [field for field in air_quality_df.schema.fields if field.name == \"timestamp\"][0]\n",
    "    print(f\"   ‚úÖ Timestamp column type: {timestamp_field.dataType}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to load air quality data\")\n",
    "    # Create a dummy DataFrame for now so the rest of the notebook can continue\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "    \n",
    "    dummy_schema = StructType([\n",
    "        StructField(\"sensor_id\", StringType(), True),\n",
    "        StructField(\"timestamp\", TimestampType(), True),  # Proper TimestampType\n",
    "        StructField(\"location_lat\", DoubleType(), True),\n",
    "        StructField(\"location_lon\", DoubleType(), True),\n",
    "        StructField(\"pm25\", DoubleType(), True),\n",
    "        StructField(\"pm10\", DoubleType(), True),\n",
    "        StructField(\"no2\", DoubleType(), True),\n",
    "        StructField(\"co\", DoubleType(), True),\n",
    "        StructField(\"temperature\", DoubleType(), True),\n",
    "        StructField(\"humidity\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    air_quality_df = spark.createDataFrame([], dummy_schema)\n",
    "    print(\"   üîß Created empty DataFrame with proper schema for continuation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå§Ô∏è Loading Weather Data...\n",
      "   üîç Original timestamp type: StringType()\n",
      "   üîÑ Converting timestamp from string to TimestampType...\n",
      "   üîç Original timestamp type: StringType()\n",
      "   üîÑ Converting timestamp from string to TimestampType...\n",
      "   üîç Original timestamp type: StringType()\n",
      "   üîÑ Converting timestamp from string to TimestampType...\n",
      "   ‚úÖ All timestamps converted successfully\n",
      "   üìä Records: 3370\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- wind_direction: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "| station_id|           timestamp|      location_lat|      location_lon|       temperature|         humidity|        wind_speed|    wind_direction|     precipitation|          pressure|\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|WEATHER_001|2025-08-28 15:31:...| 40.73449183658597| -73.9855160623927| 18.64338089409505|79.55064100314962| 7.952360943534741| 247.8348978901257|               0.0|1005.7270963407274|\n",
      "|WEATHER_002|2025-08-28 15:31:...| 40.73305150439225|-73.98127114517911|19.706573011719957|34.83841802221935| 4.455113837589062| 270.8784706801399|0.2051912403744155|1017.1994461821799|\n",
      "|WEATHER_003|2025-08-28 15:31:...|40.744091856227584| -73.9379339972575| 21.03142042347514| 60.2197167902134|   7.9919208737558|351.34689503117926|               0.0|1012.5375760873535|\n",
      "|WEATHER_004|2025-08-28 15:31:...|40.775433450042385|-73.94858571173108|19.687637164861822|79.98572441808922|2.3204700279760693|326.86206879767315|               0.0|1005.9136783162954|\n",
      "|WEATHER_005|2025-08-28 15:31:...| 40.70935411399543|-73.90582963590589|18.080776870682012|62.00223061822684| 8.214018421509806|155.54522790937784|               0.0|1005.6601136377631|\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "   ‚úÖ Final timestamp type: TimestampType()\n",
      "   üîç Sample timestamp values:\n",
      "+--------------------------+\n",
      "|timestamp                 |\n",
      "+--------------------------+\n",
      "|2025-08-28 15:31:36.389483|\n",
      "|2025-08-28 15:31:36.389483|\n",
      "|2025-08-28 15:31:36.389483|\n",
      "+--------------------------+\n",
      "only showing top 3 rows\n",
      "   ‚úÖ All timestamps converted successfully\n",
      "   üìä Records: 3370\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- wind_direction: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "| station_id|           timestamp|      location_lat|      location_lon|       temperature|         humidity|        wind_speed|    wind_direction|     precipitation|          pressure|\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|WEATHER_001|2025-08-28 15:31:...| 40.73449183658597| -73.9855160623927| 18.64338089409505|79.55064100314962| 7.952360943534741| 247.8348978901257|               0.0|1005.7270963407274|\n",
      "|WEATHER_002|2025-08-28 15:31:...| 40.73305150439225|-73.98127114517911|19.706573011719957|34.83841802221935| 4.455113837589062| 270.8784706801399|0.2051912403744155|1017.1994461821799|\n",
      "|WEATHER_003|2025-08-28 15:31:...|40.744091856227584| -73.9379339972575| 21.03142042347514| 60.2197167902134|   7.9919208737558|351.34689503117926|               0.0|1012.5375760873535|\n",
      "|WEATHER_004|2025-08-28 15:31:...|40.775433450042385|-73.94858571173108|19.687637164861822|79.98572441808922|2.3204700279760693|326.86206879767315|               0.0|1005.9136783162954|\n",
      "|WEATHER_005|2025-08-28 15:31:...| 40.70935411399543|-73.90582963590589|18.080776870682012|62.00223061822684| 8.214018421509806|155.54522790937784|               0.0|1005.6601136377631|\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "   ‚úÖ Final timestamp type: TimestampType()\n",
      "   üîç Sample timestamp values:\n",
      "+--------------------------+\n",
      "|timestamp                 |\n",
      "+--------------------------+\n",
      "|2025-08-28 15:31:36.389483|\n",
      "|2025-08-28 15:31:36.389483|\n",
      "|2025-08-28 15:31:36.389483|\n",
      "+--------------------------+\n",
      "only showing top 3 rows\n",
      "   ‚úÖ All timestamps converted successfully\n",
      "   üìä Records: 3370\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- wind_direction: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "| station_id|           timestamp|      location_lat|      location_lon|       temperature|         humidity|        wind_speed|    wind_direction|     precipitation|          pressure|\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|WEATHER_001|2025-08-28 15:31:...| 40.73449183658597| -73.9855160623927| 18.64338089409505|79.55064100314962| 7.952360943534741| 247.8348978901257|               0.0|1005.7270963407274|\n",
      "|WEATHER_002|2025-08-28 15:31:...| 40.73305150439225|-73.98127114517911|19.706573011719957|34.83841802221935| 4.455113837589062| 270.8784706801399|0.2051912403744155|1017.1994461821799|\n",
      "|WEATHER_003|2025-08-28 15:31:...|40.744091856227584| -73.9379339972575| 21.03142042347514| 60.2197167902134|   7.9919208737558|351.34689503117926|               0.0|1012.5375760873535|\n",
      "|WEATHER_004|2025-08-28 15:31:...|40.775433450042385|-73.94858571173108|19.687637164861822|79.98572441808922|2.3204700279760693|326.86206879767315|               0.0|1005.9136783162954|\n",
      "|WEATHER_005|2025-08-28 15:31:...| 40.70935411399543|-73.90582963590589|18.080776870682012|62.00223061822684| 8.214018421509806|155.54522790937784|               0.0|1005.6601136377631|\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "   ‚úÖ Final timestamp type: TimestampType()\n",
      "   üîç Sample timestamp values:\n",
      "+--------------------------+\n",
      "|timestamp                 |\n",
      "+--------------------------+\n",
      "|2025-08-28 15:31:36.389483|\n",
      "|2025-08-28 15:31:36.389483|\n",
      "|2025-08-28 15:31:36.389483|\n",
      "+--------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load weather data (Parquet format) - UPDATED WITH TIMESTAMP CONVERSION\n",
    "print(\"\\nüå§Ô∏è Loading Weather Data...\")\n",
    "try:\n",
    "    # TODO: Load Parquet file - another different format!\n",
    "    weather_df = spark.read.parquet(f\"{data_dir}/weather_data.parquet\")\n",
    "    \n",
    "    # Check if timestamp needs conversion\n",
    "    timestamp_field = [field for field in weather_df.schema.fields if field.name == \"timestamp\"][0]\n",
    "    print(f\"   üîç Original timestamp type: {timestamp_field.dataType}\")\n",
    "    \n",
    "    # Convert timestamp if it's a string OR if it's not already TimestampType\n",
    "    if str(timestamp_field.dataType) == \"StringType\" or str(timestamp_field.dataType) != \"TimestampType\":\n",
    "        print(\"   üîÑ Converting timestamp from string to TimestampType...\")\n",
    "        \n",
    "        # Try different timestamp conversion approaches\n",
    "        try:\n",
    "            # Method 1: Standard to_timestamp\n",
    "            weather_df = weather_df.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\"))\n",
    "            \n",
    "            # Check if conversion worked\n",
    "            timestamp_nulls = weather_df.filter(F.col(\"timestamp\").isNull()).count()\n",
    "            total_records = weather_df.count()\n",
    "            \n",
    "            if timestamp_nulls == total_records:\n",
    "                # All timestamps became null, try with format\n",
    "                print(\"   üîÑ Standard conversion failed, trying with format...\")\n",
    "                weather_df = spark.read.parquet(f\"{data_dir}/weather_data.parquet\")  # Reload\n",
    "                weather_df = weather_df.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                \n",
    "                timestamp_nulls = weather_df.filter(F.col(\"timestamp\").isNull()).count()\n",
    "                \n",
    "                if timestamp_nulls == total_records:\n",
    "                    # Still failing, try ISO format\n",
    "                    print(\"   üîÑ Trying ISO format...\")\n",
    "                    weather_df = spark.read.parquet(f\"{data_dir}/weather_data.parquet\")  # Reload\n",
    "                    weather_df = weather_df.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\"))\n",
    "                    timestamp_nulls = weather_df.filter(F.col(\"timestamp\").isNull()).count()\n",
    "            \n",
    "            if timestamp_nulls > 0 and timestamp_nulls < total_records:\n",
    "                print(f\"   ‚ö†Ô∏è  Warning: {timestamp_nulls} records have null timestamps after conversion\")\n",
    "            elif timestamp_nulls == 0:\n",
    "                print(\"   ‚úÖ All timestamps converted successfully\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå All timestamps failed conversion - keeping as string\")\n",
    "                weather_df = spark.read.parquet(f\"{data_dir}/weather_data.parquet\")  # Reload original\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Timestamp conversion error: {str(e)}\")\n",
    "            weather_df = spark.read.parquet(f\"{data_dir}/weather_data.parquet\")  # Reload original\n",
    "    else:\n",
    "        print(\"   ‚úÖ Timestamp already in correct format\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   üìä Records: {weather_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    weather_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    weather_df.show(5)\n",
    "    \n",
    "    # Verify final timestamp type\n",
    "    final_timestamp_field = [field for field in weather_df.schema.fields if field.name == \"timestamp\"][0]\n",
    "    print(f\"   ‚úÖ Final timestamp type: {final_timestamp_field.dataType}\")\n",
    "    \n",
    "    # Show actual timestamp values to debug format\n",
    "    print(f\"   üîç Sample timestamp values:\")\n",
    "    weather_df.select(\"timestamp\").show(3, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading weather data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° Loading Energy Meters Data...\n",
      "   üìä Records: 201800\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- meter_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- power_consumption: double (nullable = true)\n",
      " |-- voltage: double (nullable = true)\n",
      " |-- current: double (nullable = true)\n",
      " |-- power_factor: double (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|   meter_id|           timestamp|building_type|      location_lat|      location_lon| power_consumption|           voltage|           current|      power_factor|\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|ENERGY_0001|2025-08-28 15:31:...|       retail| 40.76794695953785|-73.96882957086095|29.931100801963826| 236.1102761002086|126.76746347651822|0.9105925945062783|\n",
      "|ENERGY_0002|2025-08-28 15:31:...|       retail|40.740819336038804|-73.94425349009258| 38.80271013395367|246.87574959994956|157.17505748066233|0.8917675523101363|\n",
      "|ENERGY_0003|2025-08-28 15:31:...|   commercial|40.797646499255265|-73.99089974086998|28.260291541229932|243.71557032960942|115.95603638704631|0.9259273069113542|\n",
      "|ENERGY_0004|2025-08-28 15:31:...|   commercial| 40.72321863924699|-74.01674950662142|  28.5371045049364|242.15274195912096| 117.8475381862655|0.8584678853499876|\n",
      "|ENERGY_0005|2025-08-28 15:31:...|  residential| 40.79650609331661|-73.90566687776388|2.8981035380196016|240.97314621981823| 12.02666597287949|0.9101292308760456|\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "   üìä Records: 201800\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- meter_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- power_consumption: double (nullable = true)\n",
      " |-- voltage: double (nullable = true)\n",
      " |-- current: double (nullable = true)\n",
      " |-- power_factor: double (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|   meter_id|           timestamp|building_type|      location_lat|      location_lon| power_consumption|           voltage|           current|      power_factor|\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|ENERGY_0001|2025-08-28 15:31:...|       retail| 40.76794695953785|-73.96882957086095|29.931100801963826| 236.1102761002086|126.76746347651822|0.9105925945062783|\n",
      "|ENERGY_0002|2025-08-28 15:31:...|       retail|40.740819336038804|-73.94425349009258| 38.80271013395367|246.87574959994956|157.17505748066233|0.8917675523101363|\n",
      "|ENERGY_0003|2025-08-28 15:31:...|   commercial|40.797646499255265|-73.99089974086998|28.260291541229932|243.71557032960942|115.95603638704631|0.9259273069113542|\n",
      "|ENERGY_0004|2025-08-28 15:31:...|   commercial| 40.72321863924699|-74.01674950662142|  28.5371045049364|242.15274195912096| 117.8475381862655|0.8584678853499876|\n",
      "|ENERGY_0005|2025-08-28 15:31:...|  residential| 40.79650609331661|-73.90566687776388|2.8981035380196016|240.97314621981823| 12.02666597287949|0.9101292308760456|\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "   üìä Records: 201800\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- meter_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- power_consumption: double (nullable = true)\n",
      " |-- voltage: double (nullable = true)\n",
      " |-- current: double (nullable = true)\n",
      " |-- power_factor: double (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|   meter_id|           timestamp|building_type|      location_lat|      location_lon| power_consumption|           voltage|           current|      power_factor|\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|ENERGY_0001|2025-08-28 15:31:...|       retail| 40.76794695953785|-73.96882957086095|29.931100801963826| 236.1102761002086|126.76746347651822|0.9105925945062783|\n",
      "|ENERGY_0002|2025-08-28 15:31:...|       retail|40.740819336038804|-73.94425349009258| 38.80271013395367|246.87574959994956|157.17505748066233|0.8917675523101363|\n",
      "|ENERGY_0003|2025-08-28 15:31:...|   commercial|40.797646499255265|-73.99089974086998|28.260291541229932|243.71557032960942|115.95603638704631|0.9259273069113542|\n",
      "|ENERGY_0004|2025-08-28 15:31:...|   commercial| 40.72321863924699|-74.01674950662142|  28.5371045049364|242.15274195912096| 117.8475381862655|0.8584678853499876|\n",
      "|ENERGY_0005|2025-08-28 15:31:...|  residential| 40.79650609331661|-73.90566687776388|2.8981035380196016|240.97314621981823| 12.02666597287949|0.9101292308760456|\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load energy meters data\n",
    "print(\"\\n‚ö° Loading Energy Meters Data...\")\n",
    "try:\n",
    "    # TODO: Load CSV file\n",
    "    energy_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/energy_meters.csv\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   üìä Records: {energy_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    energy_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    energy_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading energy data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.2: Basic Data Quality Assessment (45 minutes)\n",
    "\n",
    "üéØ **TASK:** Assess data quality across all datasets  \n",
    "üí° **HINT:** Check for missing values, duplicates, data ranges  \n",
    "üìö **CONCEPTS:** Data profiling, quality metrics, anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Perform basic data quality assessment on a DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to assess\n",
    "        dataset_name: Name of the dataset for reporting\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìã Data Quality Assessment: {dataset_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # TODO: Basic statistics\n",
    "    total_rows = df.count()\n",
    "    total_cols = len(df.columns)\n",
    "    print(f\"   üìä Dimensions: {total_rows:,} rows √ó {total_cols} columns\")\n",
    "    \n",
    "    # TODO: Check for missing values\n",
    "    print(f\"   üîç Missing Values:\")\n",
    "    for col in df.columns:\n",
    "        missing_count = df.filter(F.col(col).isNull()).count()\n",
    "        missing_pct = (missing_count / total_rows) * 100\n",
    "        if missing_count > 0:\n",
    "            print(f\"      {col}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    # TODO: Check for duplicate records\n",
    "    duplicate_count = total_rows - df.dropDuplicates().count()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"   üîÑ Duplicate Records: {duplicate_count:,}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No duplicate records found\")\n",
    "    \n",
    "    # TODO: Numeric column statistics\n",
    "    numeric_cols = [field.name for field in df.schema.fields \n",
    "                   if field.dataType in [IntegerType(), DoubleType(), FloatType(), LongType()]]\n",
    "    \n",
    "    if numeric_cols:\n",
    "        print(f\"   üìà Numeric Columns Summary:\")\n",
    "        # Show basic statistics for numeric columns\n",
    "        df.select(numeric_cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Data Quality Assessment: City Zones\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 8 rows √ó 8 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|summary|             lat_min|             lat_max|            lon_min|            lon_max|        population|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|  count|                   8|                   8|                  8|                  8|                 8|\n",
      "|   mean|  40.730000000000004|             40.7525| -73.99125000000001|          -73.97125|           21250.0|\n",
      "| stddev|0.023904572186687328|0.028157719063465373|0.02474873734153055|0.02474873734153458|14260.334598358582|\n",
      "|    min|                40.7|               40.72|             -74.02|              -74.0|              5000|\n",
      "|    max|               40.76|                40.8|             -73.96|             -73.94|             45000|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "\n",
      "\n",
      "üìã Data Quality Assessment: Traffic Sensors\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 100,850 rows √ó 8 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|summary|        location_lat|       location_lon|     vehicle_count|         avg_speed|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|  count|              100850|             100850|            100850|            100850|\n",
      "|   mean|   40.74829011096325| -73.95290635646758|22.491680713931583|45.693957851786024|\n",
      "| stddev|0.030179257274438317|0.03334585093595407|13.831975215438666|17.059700944781138|\n",
      "|    min|    40.7013508236137| -74.01479947116061|                 0|               5.0|\n",
      "|    max|   40.79997646235858| -73.90010522496067|                96|112.68941665816138|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "\n",
      "\n",
      "üìã Data Quality Assessment: Air Quality\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 13,460 rows √ó 10 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|summary|                co|          humidity|        location_lat|       location_lon|               no2|              pm10|              pm25|        temperature|\n",
      "+-------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  count|             13460|             13460|               13460|              13460|             13460|             13460|             13460|              13460|\n",
      "|   mean|1.2945979185367196| 55.00623047004631|   40.74728614373005|  -73.9562509147414|32.281938112567154|  43.1406247656037|26.998104228696135| 20.073366529508192|\n",
      "| stddev| 0.430391463419035|14.378824896969824|0.034358517385323126|0.03481051500037863|10.808448152463551|13.108839256109663| 8.662058241414352|   8.03386020758025|\n",
      "|    min|               0.0| 30.00080041265963|  40.700214104383555| -74.01508078174811|               0.0|               0.0|               0.0|-10.897683376054975|\n",
      "|    max|2.9186431432565074| 79.99729491529826|   40.79528990391521| -73.90676548494977| 77.51026339077877| 93.80811747450818| 64.32726261378163|  50.19621918765087|\n",
      "+-------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n",
      "\n",
      "üìã Data Quality Assessment: Weather Stations\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 3,370 rows √ó 10 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+--------------------+--------------------+------------------+------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|summary|        location_lat|        location_lon|       temperature|          humidity|          wind_speed|     wind_direction|      precipitation|          pressure|\n",
      "+-------+--------------------+--------------------+------------------+------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|  count|                3370|                3370|              3370|              3370|                3370|               3370|               3370|              3370|\n",
      "|   mean|  40.745904993561716|  -73.95950915195893|20.079789972344916| 59.98479994683771|   7.926440939466046| 180.19212711665938|0.04248214755345664|1013.1942569495562|\n",
      "| stddev|0.027311853394931074|0.030820873566814837|3.2258524724027526|14.666134146405174|   7.740150659457418| 103.88098705868862| 0.1974550230849226|10.100616588088329|\n",
      "|    min|   40.70226229741167|  -73.99884747340266|11.504686165451222|              20.0|0.008379405427216864|0.06737326005626265|                0.0| 967.7961677442195|\n",
      "|    max|   40.78564612302131|  -73.90582963590589| 32.37980102700793|             100.0|   60.18540041599714|  359.9359494995479| 3.4908734488297073|1045.1343783015802|\n",
      "+-------+--------------------+--------------------+------------------+------------------+--------------------+-------------------+-------------------+------------------+\n",
      "\n",
      "\n",
      "üìã Data Quality Assessment: Energy Meters\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 201,800 rows √ó 9 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+--------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|summary|        location_lat|       location_lon| power_consumption|           voltage|           current|       power_factor|\n",
      "+-------+--------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  count|              201800|             201800|            201800|            201800|            201800|             201800|\n",
      "|   mean|   40.74875012822779| -73.96299794551553|20.445381725410407|240.00506322808613| 85.22724265877248|  0.899959904188112|\n",
      "| stddev|0.029293928850028735|0.03467878830890881| 19.88713944467504|  4.99046037060317| 82.93872716545015|0.02884440398210471|\n",
      "|    min|   40.70195748213163| -74.01991044267211|1.6800347911691142|217.86262454071647|6.6196302123204624| 0.8500001226822763|\n",
      "|    max|   40.79955571845894| -73.90138739464425| 64.99902384491243| 266.9768642533308|288.65434731012294| 0.9499996899589729|\n",
      "+-------+--------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Assess quality for each dataset\n",
    "datasets = [\n",
    "    (zones_df, \"City Zones\"),\n",
    "    (traffic_df, \"Traffic Sensors\"), \n",
    "    (air_quality_df, \"Air Quality\"),\n",
    "    (weather_df, \"Weather Stations\"),\n",
    "    (energy_df, \"Energy Meters\")\n",
    "]\n",
    "\n",
    "for df, name in datasets:\n",
    "    try:\n",
    "        assess_data_quality(df, name)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error assessing {name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.3: Temporal Analysis (30 minutes)\n",
    "\n",
    "üéØ **TASK:** Analyze temporal patterns in the IoT data  \n",
    "üí° **HINT:** Look at data distribution over time, identify patterns  \n",
    "üìö **CONCEPTS:** Time series analysis, temporal patterns, data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚è∞ TEMPORAL PATTERN ANALYSIS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60) \n",
    "print(\"‚è∞ TEMPORAL PATTERN ANALYSIS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöó Traffic Patterns by Hour:\n",
      "+----+------------------+--------+\n",
      "|hour|      avg_vehicles|readings|\n",
      "+----+------------------+--------+\n",
      "|   0|19.194285714285716|    4200|\n",
      "|   1|18.920714285714286|    4200|\n",
      "|   2| 19.03047619047619|    4200|\n",
      "|   3| 18.91761904761905|    4200|\n",
      "|   4|18.966190476190476|    4200|\n",
      "|   5|18.939761904761905|    4200|\n",
      "|   6|19.057380952380953|    4200|\n",
      "|   7| 32.97571428571428|    4200|\n",
      "|   8|32.925714285714285|    4200|\n",
      "|   9|33.072857142857146|    4200|\n",
      "|  10|19.072857142857142|    4200|\n",
      "|  11| 18.91761904761905|    4200|\n",
      "|  12|            18.985|    4200|\n",
      "|  13|19.143333333333334|    4200|\n",
      "|  14|19.128333333333334|    4200|\n",
      "|  15| 18.86094117647059|    4250|\n",
      "|  16|18.875714285714285|    4200|\n",
      "|  17| 32.98833333333334|    4200|\n",
      "|  18| 33.00380952380952|    4200|\n",
      "|  19| 33.00952380952381|    4200|\n",
      "|  20|18.944523809523808|    4200|\n",
      "|  21|19.008333333333333|    4200|\n",
      "|  22| 19.02261904761905|    4200|\n",
      "|  23| 18.88190476190476|    4200|\n",
      "+----+------------------+--------+\n",
      "\n",
      "üìù OBSERVATIONS:\n",
      "   - Rush hour patterns: [YOUR ANALYSIS HERE]\n",
      "   - Off-peak periods: [YOUR ANALYSIS HERE]\n",
      "   - Peak traffic hours: [YOUR ANALYSIS HERE]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Analyze traffic patterns by hour\n",
    "print(\"\\nüöó Traffic Patterns by Hour:\")\n",
    "try:\n",
    "    # TODO: Extract hour from timestamp and analyze vehicle counts\n",
    "    traffic_hourly = (traffic_df\n",
    "                     .withColumn(\"hour\", F.hour(\"timestamp\"))\n",
    "                     .groupBy(\"hour\")\n",
    "                     .agg(F.avg(\"vehicle_count\").alias(\"avg_vehicles\"),\n",
    "                          F.count(\"*\").alias(\"readings\"))\n",
    "                     .orderBy(\"hour\"))\n",
    "    \n",
    "    # TODO: Show the results\n",
    "    traffic_hourly.show(24)\n",
    "    \n",
    "    # TODO: What patterns do you notice? Add your observations here:\n",
    "    print(\"üìù OBSERVATIONS:\")\n",
    "    print(\"   - Rush hour patterns: [YOUR ANALYSIS HERE]\")\n",
    "    print(\"   - Off-peak periods: [YOUR ANALYSIS HERE]\")\n",
    "    print(\"   - Peak traffic hours: [YOUR ANALYSIS HERE]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error analyzing traffic patterns: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå´Ô∏è Air Quality Patterns by Day of Week:\n",
      "+-----------+------------------+------------------+\n",
      "|day_of_week|          avg_pm25|           avg_no2|\n",
      "+-----------+------------------+------------------+\n",
      "|          1| 27.16056898397968|32.580398242994725|\n",
      "|          2|26.749224453125226| 32.59885382271132|\n",
      "|          3|27.360439691886832|31.986703806551912|\n",
      "|          4| 26.87289430915001|31.943333053382048|\n",
      "|          5|26.892404539563756|32.264457689070845|\n",
      "|          6|26.739617497123376| 32.14500795851471|\n",
      "|          7|27.212681164472606| 32.45499430248965|\n",
      "+-----------+------------------+------------------+\n",
      "\n",
      "üìù OBSERVATIONS:\n",
      "   - Weekday vs weekend patterns: [YOUR ANALYSIS HERE]\n",
      "   - Pollution trends: [YOUR ANALYSIS HERE]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Analyze air quality patterns by day of week\n",
    "print(\"\\nüå´Ô∏è Air Quality Patterns by Day of Week:\")\n",
    "try:\n",
    "    # TODO: Extract day of week and analyze PM2.5 levels\n",
    "    air_quality_daily = (air_quality_df\n",
    "                        .withColumn(\"day_of_week\", F.dayofweek(\"timestamp\"))\n",
    "                        .groupBy(\"day_of_week\")\n",
    "                        .agg(F.avg(\"pm25\").alias(\"avg_pm25\"),\n",
    "                             F.avg(\"no2\").alias(\"avg_no2\"))\n",
    "                        .orderBy(\"day_of_week\"))\n",
    "    \n",
    "    # TODO: Show results\n",
    "    air_quality_daily.show()\n",
    "    \n",
    "    # TODO: Add your observations\n",
    "    print(\"üìù OBSERVATIONS:\")\n",
    "    print(\"   - Weekday vs weekend patterns: [YOUR ANALYSIS HERE]\")\n",
    "    print(\"   - Pollution trends: [YOUR ANALYSIS HERE]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error analyzing air quality patterns: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 3: BASIC DATA INGESTION (Afternoon - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üì• SECTION 3: DATA INGESTION PIPELINE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3.1: Create Reusable Data Loading Functions (60 minutes)\n",
    "\n",
    "üéØ **TASK:** Create reusable functions for loading different data formats  \n",
    "üí° **HINT:** Handle schema validation and error handling  \n",
    "üìö **CONCEPTS:** Function design, error handling, schema enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path, expected_schema=None):\n",
    "    \"\"\"\n",
    "    Load CSV data with proper error handling and schema validation\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to CSV file\n",
    "        expected_schema: Optional StructType for schema enforcement\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement CSV loading with options\n",
    "        df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "        # TODO: Add schema validation if provided\n",
    "        if expected_schema:\n",
    "            # Validate schema matches expected\n",
    "            pass\n",
    "            \n",
    "        print(f\"‚úÖ Successfully loaded CSV: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CSV {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    Load JSON data with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement JSON loading\n",
    "        df = spark.read.json(file_path)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded JSON: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading JSON {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_parquet_data(file_path):\n",
    "    \"\"\"\n",
    "    Load Parquet data with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to Parquet file\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement Parquet loading\n",
    "        df = spark.read.parquet(file_path)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded Parquet: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading Parquet {file_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Data Loading Functions:\n",
      "\n",
      "   Testing CSV loader...\n",
      "‚úÖ Successfully loaded CSV: ../data/raw/city_zones.csv\n",
      "      Records loaded: 8\n",
      "\n",
      "   Testing JSON loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded JSON: ../data/raw/air_quality.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Records loaded: 161,522\n",
      "\n",
      "   Testing Parquet loader...\n",
      "‚úÖ Successfully loaded Parquet: ../data/raw/weather_data.parquet\n",
      "      Records loaded: 3,370\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test your loading functions\n",
    "print(\"üß™ Testing Data Loading Functions:\")\n",
    "\n",
    "test_files = [\n",
    "    (f\"{data_dir}/city_zones.csv\", \"CSV\", load_csv_data),\n",
    "    (f\"{data_dir}/air_quality.json\", \"JSON\", load_json_data), \n",
    "    (f\"{data_dir}/weather_data.parquet\", \"Parquet\", load_parquet_data)\n",
    "]\n",
    "\n",
    "for file_path, file_type, load_func in test_files:\n",
    "    print(f\"\\n   Testing {file_type} loader...\")\n",
    "    test_df = load_func(file_path)\n",
    "    if test_df:\n",
    "        print(f\"      Records loaded: {test_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3.2: Schema Definition and Enforcement (60 minutes)\n",
    "\n",
    "üéØ **TASK:** Define explicit schemas for data consistency  \n",
    "üí° **HINT:** Use StructType and StructField for schema definition  \n",
    "üìö **CONCEPTS:** Schema design, data types, schema enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# TODO: Define schema for traffic sensors\n",
    "traffic_schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    # TODO: Add remaining fields\n",
    "    StructField(\"vehicle_count\", IntegerType(), False),\n",
    "    StructField(\"avg_speed\", DoubleType(), False),\n",
    "    StructField(\"congestion_level\", StringType(), False),\n",
    "    StructField(\"road_type\", StringType(), False),\n",
    "])\n",
    "\n",
    "# TODO: Define schema for air quality data\n",
    "air_quality_schema = StructType([\n",
    "    # TODO: Define all fields for air quality data\n",
    "    # Hint: Look at the JSON structure and define appropriate types\n",
    "    StructField(\"sensor_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"location_lat\", DoubleType(), True),\n",
    "    StructField(\"location_lon\", DoubleType(), True),\n",
    "    StructField(\"pm25\", DoubleType(), True),\n",
    "    StructField(\"pm10\", DoubleType(), True),\n",
    "    StructField(\"no2\", DoubleType(), True),\n",
    "    StructField(\"co\", DoubleType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"humidity\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# TODO: Define schema for weather data\n",
    "weather_schema = StructType([\n",
    "    # TODO: Define all fields for weather data\n",
    "    StructField(\"station_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"location_lat\", DoubleType(), True),\n",
    "    StructField(\"location_lon\", DoubleType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"humidity\", DoubleType(), True),    \n",
    "    StructField(\"wind_speed\", DoubleType(), True),\n",
    "    StructField(\"wind_direction\", StringType(), True),\n",
    "    StructField(\"precipitation\", DoubleType(), True),\n",
    "    StructField(\"pressure\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# TODO: Define schema for energy data\n",
    "energy_schema = StructType([\n",
    "    # TODO: Define all fields for energy data\n",
    "    StructField(\"meter_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"location_lat\", DoubleType(), True),\n",
    "    StructField(\"location_lon\", DoubleType(), True),\n",
    "    StructField(\"energy_consumption_kwh\", DoubleType(), True),\n",
    "    StructField(\"peak_demand_kw\", DoubleType(), True),\n",
    "    StructField(\"voltage\", DoubleType(), True),\n",
    "    StructField(\"current\", DoubleType(), True),\n",
    "    StructField(\"power_factor\", DoubleType(), True),\n",
    "    StructField(\"frequency\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing Schema Enforcement:\n",
      "‚úÖ Schema enforcement successful for ../data/raw/traffic_sensors.csv\n",
      "   Schema enforcement test passed!\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- no2: double (nullable = true)\n",
      " |-- co: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test schema enforcement\n",
    "print(\"\\nüîç Testing Schema Enforcement:\")\n",
    "\n",
    "def load_with_schema(file_path, schema, file_format=\"csv\"):\n",
    "    \"\"\"Load data with explicit schema enforcement\"\"\"\n",
    "    try:\n",
    "        if file_format == \"csv\":\n",
    "            df = spark.read.schema(schema).option(\"header\", \"true\").csv(file_path)\n",
    "        elif file_format == \"json\":\n",
    "            df = spark.read.schema(schema).json(file_path)\n",
    "        elif file_format == \"parquet\":\n",
    "            df = spark.read.schema(schema).parquet(file_path)\n",
    "        \n",
    "        print(f\"‚úÖ Schema enforcement successful for {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Schema enforcement failed for {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# TODO: Test with one of your schemas\n",
    "test_schema_df = load_with_schema(f\"{data_dir}/traffic_sensors.csv\", air_quality_schema, \"csv\")\n",
    "if test_schema_df:\n",
    "    print(\"   Schema enforcement test passed!\")\n",
    "    test_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 4: INITIAL DATA TRANSFORMATIONS (Afternoon - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîÑ SECTION 4: DATA TRANSFORMATIONS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîÑ SECTION 4: DATA TRANSFORMATIONS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.1: Timestamp Standardization (45 minutes)\n",
    "\n",
    "üéØ **TASK:** Standardize timestamp formats across all datasets  \n",
    "üí° **HINT:** Some datasets may have different timestamp formats  \n",
    "üìö **CONCEPTS:** Date/time handling, format standardization, timezone handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_timestamps(df, timestamp_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Standardize timestamp column across datasets\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        timestamp_col: Name of timestamp column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with standardized timestamps\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Convert timestamps to standard format\n",
    "        standardized_df = (df\n",
    "                          .withColumn(\"timestamp_std\", F.to_timestamp(F.col(timestamp_col)))\n",
    "                          .drop(timestamp_col)\n",
    "                          .withColumnRenamed(\"timestamp_std\", timestamp_col))\n",
    "        \n",
    "        # TODO: Add derived time columns\n",
    "        result_df = (standardized_df\n",
    "                    .withColumn(\"year\", F.year(timestamp_col))\n",
    "                    .withColumn(\"month\", F.month(timestamp_col))\n",
    "                    .withColumn(\"day\", F.dayofmonth(timestamp_col))\n",
    "                    .withColumn(\"hour\", F.hour(timestamp_col))\n",
    "                    .withColumn(\"day_of_week\", F.dayofweek(timestamp_col))\n",
    "                    .withColumn(\"is_weekend\", F.when(F.dayofweek(timestamp_col).isin([1, 7]), True).otherwise(False)))\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error standardizing timestamps: {str(e)}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test timestamp standardization\n",
    "print(\"‚è∞ Testing Timestamp Standardization:\")\n",
    "\n",
    "# Test with traffic data\n",
    "traffic_std = standardize_timestamps(traffic_df)\n",
    "print(\"   Traffic data timestamp standardization:\")\n",
    "traffic_std.select(\"timestamp\", \"year\", \"month\", \"day\", \"hour\", \"day_of_week\", \"is_weekend\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.2: Geographic Zone Mapping (45 minutes)\n",
    "\n",
    "üéØ **TASK:** Map sensor locations to city zones  \n",
    "üí° **HINT:** Join sensor coordinates with zone boundaries  \n",
    "üìö **CONCEPTS:** Spatial joins, geographic data, coordinate systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_zones(sensor_df, zones_df):\n",
    "    \"\"\"\n",
    "    Map sensor locations to city zones\n",
    "    \n",
    "    Args:\n",
    "        sensor_df: DataFrame with sensor locations (lat, lon)\n",
    "        zones_df: DataFrame with zone boundaries\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with zone information added\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Create join condition for geographic mapping\n",
    "        # A sensor is in a zone if its coordinates fall within zone boundaries\n",
    "        join_condition = (\n",
    "            (sensor_df.location_lat >= zones_df.lat_min) &\n",
    "            (sensor_df.location_lat <= zones_df.lat_max) &\n",
    "            (sensor_df.location_lon >= zones_df.lon_min) &\n",
    "            (sensor_df.location_lon <= zones_df.lon_max)\n",
    "        )\n",
    "        \n",
    "        # TODO: Perform the join\n",
    "        result_df = (sensor_df\n",
    "                    .join(zones_df, join_condition, \"left\")\n",
    "                    .select(sensor_df[\"*\"], \n",
    "                           zones_df.zone_id, \n",
    "                           zones_df.zone_name, \n",
    "                           zones_df.zone_type))\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error mapping to zones: {str(e)}\")\n",
    "        return sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test zone mapping\n",
    "print(\"\\nüó∫Ô∏è Testing Geographic Zone Mapping:\")\n",
    "\n",
    "# Test with traffic sensors\n",
    "traffic_with_zones = map_to_zones(traffic_std, zones_df)\n",
    "print(\"   Traffic sensors with zone mapping:\")\n",
    "traffic_with_zones.select(\"sensor_id\", \"location_lat\", \"location_lon\", \"zone_id\", \"zone_type\").show(10)\n",
    "\n",
    "# TODO: Verify mapping worked correctly\n",
    "zone_distribution = traffic_with_zones.groupBy(\"zone_type\").count().orderBy(F.desc(\"count\"))\n",
    "print(\"   Sensors by zone type:\")\n",
    "zone_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.3: Data Type Conversions and Validations (30 minutes)\n",
    "\n",
    "üéØ **TASK:** Ensure proper data types and add validation columns  \n",
    "üí° **HINT:** Cast columns to appropriate types, add data quality flags  \n",
    "üìö **CONCEPTS:** Data type conversion, validation rules, data quality flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_quality_flags(df, sensor_type):\n",
    "    \"\"\"\n",
    "    Add data quality validation flags to DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        sensor_type: Type of sensor for specific validations\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with quality flags added\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result_df = df\n",
    "        \n",
    "        # TODO: Add general quality flags\n",
    "        result_df = result_df.withColumn(\"has_missing_values\", \n",
    "                                        F.when(F.col(\"sensor_id\").isNull(), True).otherwise(False))\n",
    "        \n",
    "        # TODO: Add sensor-specific validations\n",
    "        if sensor_type == \"traffic\":\n",
    "            # Traffic-specific validations\n",
    "            result_df = (result_df\n",
    "                        .withColumn(\"valid_speed\", \n",
    "                                   F.when((F.col(\"avg_speed\") >= 0) & (F.col(\"avg_speed\") <= 100), True).otherwise(False))\n",
    "                        .withColumn(\"valid_vehicle_count\",\n",
    "                                   F.when(F.col(\"vehicle_count\") >= 0, True).otherwise(False)))\n",
    "        \n",
    "        elif sensor_type == \"air_quality\":\n",
    "            # Air quality specific validations\n",
    "            result_df = (result_df\n",
    "                        .withColumn(\"valid_pm25\",\n",
    "                                   F.when((F.col(\"pm25\") >= 0) & (F.col(\"pm25\") <= 500), True).otherwise(False))\n",
    "                        .withColumn(\"valid_temperature\",\n",
    "                                   F.when((F.col(\"temperature\") >= -50) & (F.col(\"temperature\") <= 50), True).otherwise(False)))\n",
    "        \n",
    "        # TODO: Add more sensor-specific validations\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding quality flags: {str(e)}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test data quality flags\n",
    "print(\"\\nüè∑Ô∏è Testing Data Quality Flags:\")\n",
    "\n",
    "# Test with traffic data\n",
    "traffic_with_flags = add_data_quality_flags(traffic_with_zones, \"traffic\")\n",
    "print(\"   Traffic data with quality flags:\")\n",
    "traffic_with_flags.select(\"sensor_id\", \"avg_speed\", \"vehicle_count\", \"valid_speed\", \"valid_vehicle_count\").show(10)\n",
    "\n",
    "# TODO: Check quality flag distribution\n",
    "quality_stats = (traffic_with_flags\n",
    "                .agg(F.sum(F.when(F.col(\"valid_speed\"), 1).otherwise(0)).alias(\"valid_speed_count\"),\n",
    "                     F.sum(F.when(F.col(\"valid_vehicle_count\"), 1).otherwise(0)).alias(\"valid_vehicle_count_count\"),\n",
    "                     F.count(\"*\").alias(\"total_records\")))\n",
    "\n",
    "print(\"   Quality statistics:\")\n",
    "quality_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# DAY 1 DELIVERABLES & CHECKPOINTS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã DAY 1 COMPLETION CHECKLIST\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this checklist by running the validation functions\n",
    "\n",
    "def validate_day1_completion():\n",
    "    \"\"\"Validate that Day 1 objectives have been met\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"spark_session_created\": False,\n",
    "        \"database_connection_tested\": False,\n",
    "        \"data_loaded_successfully\": False,\n",
    "        \"data_quality_assessed\": False,\n",
    "        \"loading_functions_created\": False,\n",
    "        \"schemas_defined\": False,\n",
    "        \"timestamp_standardization_working\": False,\n",
    "        \"zone_mapping_implemented\": False,\n",
    "        \"quality_flags_added\": False\n",
    "    }\n",
    "    \n",
    "    # TODO: Add validation logic for each item\n",
    "    try:\n",
    "        # Check Spark session\n",
    "        if spark and spark.sparkContext._jsc:\n",
    "            checklist[\"spark_session_created\"] = True\n",
    "            \n",
    "        # Check if data exists\n",
    "        if 'traffic_df' in locals() and traffic_df.count() > 0:\n",
    "            checklist[\"data_loaded_successfully\"] = True\n",
    "            \n",
    "        # TODO: Add more validation checks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation error: {str(e)}\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"‚úÖ COMPLETION STATUS:\")\n",
    "    for item, status in checklist.items():\n",
    "        status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "        print(f\"   {status_icon} {item.replace('_', ' ').title()}\")\n",
    "    \n",
    "    completion_rate = sum(checklist.values()) / len(checklist) * 100\n",
    "    print(f\"\\nüìä Overall Completion: {completion_rate:.1f}%\")\n",
    "    \n",
    "    if completion_rate >= 80:\n",
    "        print(\"üéâ Great job! You're ready for Day 2!\")\n",
    "    else:\n",
    "        print(\"üìù Please review incomplete items before proceeding to Day 2.\")\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# TODO: Run the validation\n",
    "completion_status = validate_day1_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ WHAT'S NEXT?\n",
    "\n",
    "---\n",
    "\n",
    "## üìÖ DAY 2 PREVIEW: Data Quality & Cleaning Pipeline\n",
    "\n",
    "Tomorrow you'll work on:\n",
    "1. üîç Comprehensive data quality assessment\n",
    "2. üßπ Advanced cleaning procedures for IoT sensor data  \n",
    "3. üìä Missing data handling and interpolation strategies\n",
    "4. üö® Outlier detection and treatment methods\n",
    "5. üìè Data standardization and normalization\n",
    "\n",
    "## üìö RECOMMENDED PREPARATION:\n",
    "- Review PySpark DataFrame operations\n",
    "- Read about time series data quality challenges\n",
    "- Familiarize yourself with statistical outlier detection methods\n",
    "\n",
    "## üíæ SAVE YOUR WORK:\n",
    "- Commit your notebook to Git\n",
    "- Document any issues or questions for tomorrow\n",
    "- Save any custom functions you created\n",
    "\n",
    "## ü§ù QUESTIONS?\n",
    "- Post in the class discussion forum\n",
    "- Review Spark documentation for any unclear concepts\n",
    "- Prepare questions for tomorrow's Q&A session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save your progress\n",
    "print(\"\\nüíæ Don't forget to save your notebook and commit your changes!\")\n",
    "\n",
    "# Clean up (optional)\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
