{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Environment Setup & Data Exploration\n",
    "# Smart City IoT Analytics Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 LEARNING OBJECTIVES:\n",
    "- Configure Spark cluster and development environment\n",
    "- Understand IoT data characteristics and challenges  \n",
    "- Implement basic data ingestion patterns\n",
    "- Explore PySpark DataFrame operations\n",
    "\n",
    "## 📅 SCHEDULE:\n",
    "**Morning (4 hours):**\n",
    "1. Environment Setup (2 hours)\n",
    "2. Data Exploration (2 hours)\n",
    "\n",
    "**Afternoon (4 hours):**  \n",
    "3. Basic Data Ingestion (2 hours)\n",
    "4. Initial Data Transformations (2 hours)\n",
    "\n",
    "## ✅ DELIVERABLES:\n",
    "- Working Spark cluster with all services running\n",
    "- Data ingestion notebook with basic EDA\n",
    "- Documentation of data quality findings  \n",
    "- Initial data loading pipeline functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Welcome to the Smart City IoT Analytics Pipeline!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 Welcome to the Smart City IoT Analytics Pipeline!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 1: ENVIRONMENT SETUP (Morning - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.1: Initialize Spark Session (15 minutes)\n",
    "\n",
    "🎯 **TASK:** Create a Spark session configured for local development  \n",
    "💡 **HINT:** Use SparkSession.builder with appropriate configurations  \n",
    "📚 **DOCS:** https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "\n",
    "**TODO:** Create Spark session with the following configurations:\n",
    "- App name: \"SmartCityIoTPipeline-Day1\"\n",
    "- Master: \"local[*]\" (use all available cores)\n",
    "- Memory: \"4g\" for driver\n",
    "- Additional configs for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/05 09:08:16 WARN Utils: Your hostname, Zipcoders-MacBook-Pro-3.local, resolves to a loopback address: 127.0.0.1; using 192.168.87.79 instead (on interface en0)\n",
      "25/09/05 09:08:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/09/05 09:08:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/05 09:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/09/05 09:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session Details:\n",
      "   App Name: SmartCityIoTPipeline-Day1\n",
      "   Spark Version: 4.0.0\n",
      "   Master: local[*]\n",
      "   Default Parallelism: 8\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create Spark session with the following configurations:\n",
    "jdbc_jar_path = \"/Users/sai/Documents/Projects/ninth-week/SparkCity/postgresql-42.7.3.jar\"\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"SmartCityIoTPipeline-Day1\")  # TODO: Add your app name\n",
    "         .master(\"local[*]\")   # TODO: Add master configuration\n",
    "         .config(\"spark.driver.memory\", \"4g\")  # TODO: Set memory\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "         .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "         .config(\"spark.jars\", jdbc_jar_path)  # <-- Add this line for JDBC driver\n",
    "         .getOrCreate())\n",
    "\n",
    "# TODO: Verify Spark session is working\n",
    "print(\"✅ Spark Session Details:\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   Default Parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.2: Verify Infrastructure (15 minutes)\n",
    "\n",
    "🎯 **TASK:** Check that all infrastructure services are running  \n",
    "💡 **HINT:** Test database connectivity and file system access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database connection successful!\n",
      "\n",
      "🌐 Spark UI should be accessible at: http://localhost:4040\n",
      "   (Open this in your browser to monitor Spark jobs)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test PostgreSQL connection\n",
    "def test_database_connection():\n",
    "    \"\"\"Test connection to PostgreSQL database\"\"\"\n",
    "    try:\n",
    "        # Database connection parameters\n",
    "        db_properties = {\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"password\", \n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "        \n",
    "        # TODO: Replace with actual connection test\n",
    "        # Test query - should create a simple DataFrame from database\n",
    "        test_df = spark.read.jdbc(\n",
    "            url=\"jdbc:postgresql://localhost:5432/smartcity\",\n",
    "            table=\"(SELECT 1 as test_column) as test_table\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "        \n",
    "        # TODO: Collect and display result\n",
    "        result = test_df.collect()\n",
    "        print(\"✅ Database connection successful!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Database connection failed: {str(e)}\")\n",
    "        print(\"💡 Make sure PostgreSQL container is running: docker-compose up -d\")\n",
    "        return False\n",
    "\n",
    "# TODO: Run the database connection test\n",
    "db_connected = test_database_connection()\n",
    "\n",
    "# TODO: Check Spark UI accessibility\n",
    "print(\"\\n🌐 Spark UI should be accessible at: http://localhost:4040\")\n",
    "print(\"   (Open this in your browser to monitor Spark jobs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.3: Generate Sample Data (30 minutes)\n",
    "\n",
    "🎯 **TASK:** Run the data generation script to create sample IoT data  \n",
    "💡 **HINT:** Use the provided data generation script or run it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Generating data... (0/5 files exist)\n",
      "   Project root: /Users/sai/Documents/Projects/ninth-week/SparkCity\n",
      "   Running script from: /Users/sai/Documents/Projects/ninth-week/SparkCity\n",
      "✅ Data generation successful!\n"
     ]
    }
   ],
   "source": [
    "# Generate Sample IoT Data\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def check_and_generate_data():\n",
    "    \"\"\"Check if data exists, generate if missing\"\"\"\n",
    "    data_files = [\"traffic_sensors.csv\", \"air_quality.json\", \"weather_data.parquet\", \n",
    "                  \"energy_meters.csv\", \"city_zones.csv\"]\n",
    "    data_path = \"data/raw\"\n",
    "    \n",
    "    # Check existing files\n",
    "    existing = [f for f in data_files if os.path.exists(f\"{data_path}/{f}\")]\n",
    "    \n",
    "    if len(existing) == len(data_files):\n",
    "        print(f\"✅ All {len(data_files)} data files found!\")\n",
    "        return True\n",
    "    \n",
    "    # Generate missing data\n",
    "    print(f\"🔄 Generating data... ({len(existing)}/{len(data_files)} files exist)\")\n",
    "    \n",
    "    try:\n",
    "        # Get the project root (go up one level from notebooks folder)\n",
    "        notebook_dir = os.getcwd()  # Current directory (notebooks/)\n",
    "        project_root = os.path.dirname(notebook_dir)  # Go up one level to SparkCity/\n",
    "        \n",
    "        print(f\"   Project root: {project_root}\")\n",
    "        print(f\"   Running script from: {project_root}\")\n",
    "        \n",
    "        # Run from project root directory\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"scripts/generate_data.py\"], \n",
    "            cwd=project_root,  # Run from SparkCity/ directory\n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            timeout=300\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ Data generation successful!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Generation failed!\")\n",
    "            if result.stderr:\n",
    "                print(f\"   Error: {result.stderr.strip()}\")\n",
    "            if result.stdout:\n",
    "                print(f\"   Output: {result.stdout.strip()}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run data check/generation\n",
    "data_ready = check_and_generate_data()\n",
    "\n",
    "# If failed, provide clear manual instructions\n",
    "if not data_ready:\n",
    "    print(\"\\n🔧 MANUAL FIX:\")\n",
    "    print(\"1. Open terminal\")\n",
    "    print(\"2. Run: cd /Users/sai/Documents/Projects/ninth-week/SparkCity\")\n",
    "    print(\"3. Run: python scripts/generate_data.py\")\n",
    "    print(\"4. Re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 2: DATA EXPLORATION (Morning - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📊 SECTION 2: EXPLORATORY DATA ANALYSIS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📊 SECTION 2: EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.1: Load and Examine Data Sources (45 minutes)\n",
    "\n",
    "🎯 **TASK:** Load each data source and examine its structure  \n",
    "💡 **HINT:** Use appropriate Spark readers for different file formats  \n",
    "📚 **CONCEPTS:** Schema inference, file formats, data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = \"../data/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📍 Loading City Zones Reference Data...\n",
      "   📊 Records: 8\n",
      "   📋 Schema:\n",
      "root\n",
      " |-- zone_id: string (nullable = true)\n",
      " |-- zone_name: string (nullable = true)\n",
      " |-- zone_type: string (nullable = true)\n",
      " |-- lat_min: double (nullable = true)\n",
      " |-- lat_max: double (nullable = true)\n",
      " |-- lon_min: double (nullable = true)\n",
      " |-- lon_max: double (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "   🔍 Sample Data:\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|zone_id |zone_name         |zone_type  |lat_min|lat_max|lon_min|lon_max|population|\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|ZONE_001|Downtown          |commercial |40.72  |40.74  |-74.01 |-73.99 |25000     |\n",
      "|ZONE_002|Financial District|commercial |40.7   |40.72  |-74.02 |-74.0  |15000     |\n",
      "|ZONE_003|Residential North |residential|40.76  |40.8   |-74.0  |-73.98 |45000     |\n",
      "|ZONE_004|Residential South |residential|40.7   |40.72  |-73.98 |-73.96 |38000     |\n",
      "|ZONE_005|Industrial Park   |industrial |40.74  |40.76  |-74.02 |-74.0  |5000      |\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load city zones reference data\n",
    "print(\"📍 Loading City Zones Reference Data...\")\n",
    "try:\n",
    "    zones_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/city_zones.csv\")\n",
    "    \n",
    "    # TODO: Display basic information about zones\n",
    "    print(f\"   📊 Records: {zones_df.count()}\")\n",
    "    print(f\"   📋 Schema:\")\n",
    "    zones_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   🔍 Sample Data:\")\n",
    "    zones_df.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading zones data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚗 Loading Traffic Sensors Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📊 Records: 100850\n",
      "   📋 Schema:\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- vehicle_count: integer (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      " |-- congestion_level: string (nullable = true)\n",
      " |-- road_type: string (nullable = true)\n",
      "\n",
      "   🔍 Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+-------------+------------------+----------------+----------+\n",
      "|  sensor_id|           timestamp|      location_lat|      location_lon|vehicle_count|         avg_speed|congestion_level| road_type|\n",
      "+-----------+--------------------+------------------+------------------+-------------+------------------+----------------+----------+\n",
      "|TRAFFIC_001|2025-08-29 09:08:...| 40.72391489670398|-74.01735761585977|           26|24.891450679573545|            high|  arterial|\n",
      "|TRAFFIC_002|2025-08-29 09:08:...| 40.79338021680954|-73.96238395488332|           26| 14.24125954375276|            high|  arterial|\n",
      "|TRAFFIC_003|2025-08-29 09:08:...|  40.7370375639637|-73.96000639125748|           31|22.931162239013865|            high|commercial|\n",
      "|TRAFFIC_004|2025-08-29 09:08:...|40.767856970315535|-73.90977667922886|           26|37.542924636645445|            high|  arterial|\n",
      "|TRAFFIC_005|2025-08-29 09:08:...| 40.70081285057606|-73.93017948792821|           54|36.519036605733355|            high|   highway|\n",
      "+-----------+--------------------+------------------+------------------+-------------+------------------+----------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load traffic sensors data  \n",
    "print(\"\\n🚗 Loading Traffic Sensors Data...\")\n",
    "try:\n",
    "    # TODO: Load CSV file with proper options\n",
    "    traffic_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/traffic_sensors.csv\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   📊 Records: {traffic_df.count()}\")\n",
    "    print(f\"   📋 Schema:\")\n",
    "    traffic_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   🔍 Sample Data:\")\n",
    "    traffic_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading traffic data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌫️ Loading Air Quality Data...\n",
      "   🔍 Loading JSON file...\n",
      "   📊 Total records loaded: 13460\n",
      "   📋 Schema:\n",
      "root\n",
      " |-- co: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- no2: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "   🔍 Sample Data:\n",
      "+------------------+------------------+------------------+------------------+------------------+------------------+------------------+---------+------------------+--------------------+\n",
      "|                co|          humidity|      location_lat|      location_lon|               no2|              pm10|              pm25|sensor_id|       temperature|           timestamp|\n",
      "+------------------+------------------+------------------+------------------+------------------+------------------+------------------+---------+------------------+--------------------+\n",
      "|1.5835740298818408| 76.13753966492168|  40.7205265366197| -73.9672364131999|50.628945242677744| 50.26783957177982|32.831180619068334|   AQ_001|11.161854488469887|2025-08-29T09:08:...|\n",
      "|1.6517362127544775| 34.47079389846476| 40.79520347439802|-73.98924725107524| 37.92652055538313|53.409817013762115|22.776713019356816|   AQ_002| 35.06800323455204|2025-08-29T09:08:...|\n",
      "|1.5288294769156119|  78.5922376405026| 40.70822552734217|-73.99443738954183| 36.63525765432075| 59.00413585552885| 25.52335245317198|   AQ_003| 16.71978050801571|2025-08-29T09:08:...|\n",
      "| 1.786168306432199|51.103515321143476|40.794253615510094|-73.94901048861428| 45.13845019077324|35.638411729293125| 35.25926412646114|   AQ_004|19.300648087074105|2025-08-29T09:08:...|\n",
      "|  2.20487286237627|  35.1388000874764|40.707971600889195|-73.93480725593834|18.123825944256687|28.398585819157397|29.242807095482725|   AQ_005| 29.80334309191472|2025-08-29T09:08:...|\n",
      "+------------------+------------------+------------------+------------------+------------------+------------------+------------------+---------+------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load air quality data (JSON format) - CORRECTED VERSION\n",
    "print(\"\\n🌫️ Loading Air Quality Data...\")\n",
    "    # First, try loading without corrupt record handling since the JSON seems valid\n",
    "print(\"   🔍 Loading JSON file...\")\n",
    "    \n",
    "air_quality_df = (spark.read\n",
    "                  .option(\"multiline\", \"true\")\n",
    "                  .json(f\"{data_dir}/air_quality.json\"))\n",
    "    \n",
    "# Check if we loaded successfully\n",
    "total_records = air_quality_df.count()\n",
    "print(f\"   📊 Total records loaded: {total_records}\")\n",
    "    \n",
    "# Check the schema\n",
    "print(f\"   📋 Schema:\")\n",
    "air_quality_df.printSchema()\n",
    "    \n",
    "# Show sample data\n",
    "print(f\"   🔍 Sample Data:\")\n",
    "air_quality_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌤️ Loading Weather Data...\n",
      "   📊 Records: 3370\n",
      "   📋 Schema:\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- wind_direction: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      "\n",
      "   🔍 Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+------------------+\n",
      "| station_id|           timestamp|      location_lat|      location_lon|       temperature|          humidity|        wind_speed|    wind_direction|precipitation|          pressure|\n",
      "+-----------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+------------------+\n",
      "|WEATHER_001|2025-08-29T09:08:...| 40.78085489201383|-73.90324777011922|16.268988003913726| 64.46412155895797|23.164863840815872|272.85030976755013|          0.0| 1010.436038533347|\n",
      "|WEATHER_002|2025-08-29T09:08:...| 40.71333337560931|-74.00624363114778|16.500899261496055| 72.25965812114755| 2.843551446498201| 96.08068457367165|          0.0|1023.2830601493264|\n",
      "|WEATHER_003|2025-08-29T09:08:...| 40.73787185303777| -73.9593106877258|17.248843158934292| 49.92374366313025| 30.36194481767519| 337.2059655731078|          0.0|1008.2149082141179|\n",
      "|WEATHER_004|2025-08-29T09:08:...| 40.79330117098755|-73.90687183210817|17.728934882492535|48.670865670053786|14.154392450038506| 73.80558190592315|          0.0|1013.8872626687811|\n",
      "|WEATHER_005|2025-08-29T09:08:...|40.751772926540426|-73.95197805959198|15.435466311311297| 65.68043412114604| 7.245016242281008|246.88621630426186|          0.0|1012.7412039226787|\n",
      "+-----------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load weather data (Parquet format)\n",
    "print(\"\\n🌤️ Loading Weather Data...\")\n",
    "try:\n",
    "    # TODO: Load Parquet file - another different format!\n",
    "    weather_df = spark.read.parquet(f\"{data_dir}/weather_data.parquet\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   📊 Records: {weather_df.count()}\")\n",
    "    print(f\"   📋 Schema:\")\n",
    "    weather_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   🔍 Sample Data:\")\n",
    "    weather_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading weather data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ Loading Energy Meters Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📊 Records: 201800\n",
      "   📋 Schema:\n",
      "root\n",
      " |-- meter_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- power_consumption: double (nullable = true)\n",
      " |-- voltage: double (nullable = true)\n",
      " |-- current: double (nullable = true)\n",
      " |-- power_factor: double (nullable = true)\n",
      "\n",
      "   🔍 Sample Data:\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|   meter_id|           timestamp|building_type|      location_lat|      location_lon| power_consumption|           voltage|           current|      power_factor|\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|ENERGY_0001|2025-08-29 09:08:...|   commercial| 40.79257852155552|-73.94017003245935|27.225329178883477|225.03142090328245|120.98456770881258|0.8643915666826584|\n",
      "|ENERGY_0002|2025-08-29 09:08:...|  residential| 40.70764574380803|-73.98284450507454|2.3063379504915456|238.99701537010148| 9.650070093637112|0.9182958310317837|\n",
      "|ENERGY_0003|2025-08-29 09:08:...|       office| 40.74727796697255|-73.96328416244302|24.152275802900043|235.74720287519017|102.44989339570996|0.8925431786872946|\n",
      "|ENERGY_0004|2025-08-29 09:08:...|       office|40.739151863883215|-73.93723351444349| 21.32318595632612|239.47924006985448| 89.03980967246385|0.8882033020298865|\n",
      "|ENERGY_0005|2025-08-29 09:08:...|       retail| 40.70313393088519|-73.91591928103885|36.049128189899804|242.31471142858376|148.76987029540877|0.8759584678626039|\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load energy meters data\n",
    "print(\"\\n⚡ Loading Energy Meters Data...\")\n",
    "try:\n",
    "    # TODO: Load CSV file\n",
    "    energy_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/energy_meters.csv\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   📊 Records: {energy_df.count()}\")\n",
    "    print(f\"   📋 Schema:\")\n",
    "    energy_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   🔍 Sample Data:\")\n",
    "    energy_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading energy data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.2: Basic Data Quality Assessment (45 minutes)\n",
    "\n",
    "🎯 **TASK:** Assess data quality across all datasets  \n",
    "💡 **HINT:** Check for missing values, duplicates, data ranges  \n",
    "📚 **CONCEPTS:** Data profiling, quality metrics, anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Perform basic data quality assessment on a DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to assess\n",
    "        dataset_name: Name of the dataset for reporting\n",
    "    \"\"\"\n",
    "    print(f\"\\n📋 Data Quality Assessment: {dataset_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # TODO: Basic statistics\n",
    "    total_rows = df.count()\n",
    "    total_cols = len(df.columns)\n",
    "    print(f\"   📊 Dimensions: {total_rows:,} rows × {total_cols} columns\")\n",
    "    \n",
    "    # TODO: Check for missing values\n",
    "    print(f\"   🔍 Missing Values:\")\n",
    "    for col in df.columns:\n",
    "        missing_count = df.filter(F.col(col).isNull()).count()\n",
    "        missing_pct = (missing_count / total_rows) * 100\n",
    "        if missing_count > 0:\n",
    "            print(f\"      {col}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    # TODO: Check for duplicate records\n",
    "    duplicate_count = total_rows - df.dropDuplicates().count()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"   🔄 Duplicate Records: {duplicate_count:,}\")\n",
    "    else:\n",
    "        print(f\"   ✅ No duplicate records found\")\n",
    "    \n",
    "    # TODO: Numeric column statistics\n",
    "    numeric_cols = [field.name for field in df.schema.fields \n",
    "                   if field.dataType in [IntegerType(), DoubleType(), FloatType(), LongType()]]\n",
    "    \n",
    "    if numeric_cols:\n",
    "        print(f\"   📈 Numeric Columns Summary:\")\n",
    "        # Show basic statistics for numeric columns\n",
    "        df.select(numeric_cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Data Quality Assessment: City Zones\n",
      "--------------------------------------------------\n",
      "   📊 Dimensions: 8 rows × 8 columns\n",
      "   🔍 Missing Values:\n",
      "   ✅ No duplicate records found\n",
      "   📈 Numeric Columns Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/05 09:08:31 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|summary|             lat_min|             lat_max|            lon_min|            lon_max|        population|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|  count|                   8|                   8|                  8|                  8|                 8|\n",
      "|   mean|  40.730000000000004|             40.7525| -73.99125000000001|          -73.97125|           21250.0|\n",
      "| stddev|0.023904572186687328|0.028157719063465373|0.02474873734153055|0.02474873734153458|14260.334598358582|\n",
      "|    min|                40.7|               40.72|             -74.02|              -74.0|              5000|\n",
      "|    max|               40.76|                40.8|             -73.96|             -73.94|             45000|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "\n",
      "\n",
      "📋 Data Quality Assessment: Traffic Sensors\n",
      "--------------------------------------------------\n",
      "   📊 Dimensions: 100,850 rows × 8 columns\n",
      "   🔍 Missing Values:\n",
      "   ✅ No duplicate records found\n",
      "   📈 Numeric Columns Summary:\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|summary|        location_lat|       location_lon|     vehicle_count|         avg_speed|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|  count|              100850|             100850|            100850|            100850|\n",
      "|   mean|   40.75500412062866| -73.96322662910207|22.542786316311354| 45.64098785926878|\n",
      "| stddev|0.029220190304060856|0.03369872817690664|13.839913187144107|17.011286205956203|\n",
      "|    min|   40.70081285057606|  -74.0179577354858|                 0|               5.0|\n",
      "|    max|   40.79941290676354| -73.90685413438007|                90| 119.2368051238285|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "\n",
      "\n",
      "📋 Data Quality Assessment: Air Quality\n",
      "--------------------------------------------------\n",
      "   📊 Dimensions: 13,460 rows × 10 columns\n",
      "   🔍 Missing Values:\n",
      "   ✅ No duplicate records found\n",
      "   📈 Numeric Columns Summary:\n",
      "+-------+------------------+------------------+-------------------+-------------------+------------------+------------------+-----------------+-------------------+\n",
      "|summary|                co|          humidity|       location_lat|       location_lon|               no2|              pm10|             pm25|        temperature|\n",
      "+-------+------------------+------------------+-------------------+-------------------+------------------+------------------+-----------------+-------------------+\n",
      "|  count|             13460|             13460|              13460|              13460|             13460|             13460|            13460|              13460|\n",
      "|   mean|1.2875130384289266|54.826182260494996| 40.745297971528956| -73.95941349710336|  32.1504618354429| 42.80334884664813|26.97403312381407| 19.920914396221626|\n",
      "| stddev|0.4308440673649061|14.332142580372237|0.03200395142024157|0.03462471085758109|10.853107898470956|13.174128864512817|8.650390737615393|  8.031755472044066|\n",
      "|    min|               0.0| 30.00292899016368|  40.70135724630687| -74.01714036414015|               0.0|               0.0|              0.0|-13.491271650999714|\n",
      "|    max|3.0235870023107405| 79.98913254535276|  40.79520347439802| -73.90111712768562| 79.89717615532693| 90.96201589939444|57.89319558810176|  50.52690941361086|\n",
      "+-------+------------------+------------------+-------------------+-------------------+------------------+------------------+-----------------+-------------------+\n",
      "\n",
      "\n",
      "📋 Data Quality Assessment: Weather Stations\n",
      "--------------------------------------------------\n",
      "   📊 Dimensions: 3,370 rows × 10 columns\n",
      "   🔍 Missing Values:\n",
      "   ✅ No duplicate records found\n",
      "   📈 Numeric Columns Summary:\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+--------------------+-------------------+------------------+\n",
      "|summary|        location_lat|       location_lon|       temperature|          humidity|          wind_speed|      wind_direction|      precipitation|          pressure|\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+--------------------+-------------------+------------------+\n",
      "|  count|                3370|               3370|              3370|              3370|                3370|                3370|               3370|              3370|\n",
      "|   mean|   40.75513648400018| -73.93923096074455|19.911912653214014| 59.87100917239904|   7.929256132440461|  180.97061373578057|0.05306158454425665| 1013.136207608922|\n",
      "| stddev|0.028984609103931636|0.03123274588108529| 3.430067546762053|15.013023289553832|   7.952782342079477|  104.86450288055956|0.22478625520085693|10.103309351985942|\n",
      "|    min|   40.71333337560931| -74.00624363114778| 9.314022303879696|              20.0|6.265628498756273E-5|0.038498855443562796|                0.0| 978.4432151391723|\n",
      "|    max|   40.79330117098755| -73.90324777011922|  33.3500104717534|             100.0|   69.98453785200658|  359.85559901535805|  2.844568829313417| 1051.576041089785|\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+--------------------+-------------------+------------------+\n",
      "\n",
      "\n",
      "📋 Data Quality Assessment: Energy Meters\n",
      "--------------------------------------------------\n",
      "   📊 Dimensions: 201,800 rows × 9 columns\n",
      "   🔍 Missing Values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ No duplicate records found\n",
      "   📈 Numeric Columns Summary:\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|summary|       location_lat|       location_lon| power_consumption|           voltage|           current|        power_factor|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|             201800|             201800|            201800|            201800|            201800|              201800|\n",
      "|   mean|  40.75242724328286|  -73.9627357967259| 18.26316099880441|240.00823206210796| 76.12580243588519|   0.900013156309099|\n",
      "| stddev|0.03053636528449424|0.03380770173714468|18.578138934626217| 4.995942695194844| 77.46911203777647|0.028857755619309342|\n",
      "|    min|  40.70069358761313|  -74.0198187693212| 1.680094147697381|216.46925988462675| 6.648289482591334|  0.8500000747623474|\n",
      "|    max|  40.79948014467343| -73.90067425664958| 64.99856411105199|  263.928189380369|290.46087984351004|   0.949999711719659|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Assess quality for each dataset\n",
    "datasets = [\n",
    "    (zones_df, \"City Zones\"),\n",
    "    (traffic_df, \"Traffic Sensors\"), \n",
    "    (air_quality_df, \"Air Quality\"),\n",
    "    (weather_df, \"Weather Stations\"),\n",
    "    (energy_df, \"Energy Meters\")\n",
    "]\n",
    "\n",
    "for df, name in datasets:\n",
    "    try:\n",
    "        assess_data_quality(df, name)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error assessing {name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.3: Temporal Analysis (30 minutes)\n",
    "\n",
    "🎯 **TASK:** Analyze temporal patterns in the IoT data  \n",
    "💡 **HINT:** Look at data distribution over time, identify patterns  \n",
    "📚 **CONCEPTS:** Time series analysis, temporal patterns, data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "⏰ TEMPORAL PATTERN ANALYSIS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60) \n",
    "print(\"⏰ TEMPORAL PATTERN ANALYSIS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚗 Traffic Patterns by Hour:\n",
      "+----+------------------+--------+\n",
      "|hour|      avg_vehicles|readings|\n",
      "+----+------------------+--------+\n",
      "|   0| 19.00690476190476|    4200|\n",
      "|   1|19.008809523809525|    4200|\n",
      "|   2|             18.78|    4200|\n",
      "|   3|19.083809523809524|    4200|\n",
      "|   4|18.985238095238095|    4200|\n",
      "|   5|19.177857142857142|    4200|\n",
      "|   6| 19.21404761904762|    4200|\n",
      "|   7| 33.20761904761905|    4200|\n",
      "|   8|32.846666666666664|    4200|\n",
      "|   9| 33.12211764705882|    4250|\n",
      "|  10|18.919047619047618|    4200|\n",
      "|  11|19.210238095238097|    4200|\n",
      "|  12|19.019285714285715|    4200|\n",
      "|  13|19.092380952380953|    4200|\n",
      "|  14| 19.11190476190476|    4200|\n",
      "|  15|18.954285714285714|    4200|\n",
      "|  16|18.887619047619047|    4200|\n",
      "|  17| 33.05714285714286|    4200|\n",
      "|  18| 32.94452380952381|    4200|\n",
      "|  19| 32.94309523809524|    4200|\n",
      "|  20|19.244285714285713|    4200|\n",
      "|  21|18.718333333333334|    4200|\n",
      "|  22|19.193571428571428|    4200|\n",
      "|  23|19.172142857142855|    4200|\n",
      "+----+------------------+--------+\n",
      "\n",
      "📝 OBSERVATIONS:\n",
      "   - Rush hour patterns: [YOUR ANALYSIS HERE]\n",
      "   - Off-peak periods: [YOUR ANALYSIS HERE]\n",
      "   - Peak traffic hours: [YOUR ANALYSIS HERE]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Analyze traffic patterns by hour\n",
    "print(\"\\n🚗 Traffic Patterns by Hour:\")\n",
    "try:\n",
    "    # TODO: Extract hour from timestamp and analyze vehicle counts\n",
    "    traffic_hourly = (traffic_df\n",
    "                     .withColumn(\"hour\", F.hour(\"timestamp\"))\n",
    "                     .groupBy(\"hour\")\n",
    "                     .agg(F.avg(\"vehicle_count\").alias(\"avg_vehicles\"),\n",
    "                          F.count(\"*\").alias(\"readings\"))\n",
    "                     .orderBy(\"hour\"))\n",
    "    \n",
    "    # TODO: Show the results\n",
    "    traffic_hourly.show(24)\n",
    "    \n",
    "    # TODO: What patterns do you notice? Add your observations here:\n",
    "    print(\"📝 OBSERVATIONS:\")\n",
    "    print(\"   - Rush hour patterns: [YOUR ANALYSIS HERE]\")\n",
    "    print(\"   - Off-peak periods: [YOUR ANALYSIS HERE]\")\n",
    "    print(\"   - Peak traffic hours: [YOUR ANALYSIS HERE]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error analyzing traffic patterns: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌫️ Air Quality Patterns by Day of Week:\n",
      "+-----------+------------------+------------------+\n",
      "|day_of_week|          avg_pm25|           avg_no2|\n",
      "+-----------+------------------+------------------+\n",
      "|          1|26.942716969329524|32.438052953900865|\n",
      "|          2|27.118999219784158| 32.18164897685506|\n",
      "|          3| 26.96680402364341| 32.06187965238119|\n",
      "|          4|26.680196055708038|  32.3125643996631|\n",
      "|          5| 27.21896423094461| 32.05198953513624|\n",
      "|          6|26.964014775238265|32.160279436246924|\n",
      "|          7| 26.92664094984856| 31.84671562724106|\n",
      "+-----------+------------------+------------------+\n",
      "\n",
      "📝 OBSERVATIONS:\n",
      "   - Weekday vs weekend patterns: [YOUR ANALYSIS HERE]\n",
      "   - Pollution trends: [YOUR ANALYSIS HERE]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Analyze air quality patterns by day of week\n",
    "print(\"\\n🌫️ Air Quality Patterns by Day of Week:\")\n",
    "try:\n",
    "    # TODO: Extract day of week and analyze PM2.5 levels\n",
    "    air_quality_daily = (air_quality_df\n",
    "                        .withColumn(\"day_of_week\", F.dayofweek(\"timestamp\"))\n",
    "                        .groupBy(\"day_of_week\")\n",
    "                        .agg(F.avg(\"pm25\").alias(\"avg_pm25\"),\n",
    "                             F.avg(\"no2\").alias(\"avg_no2\"))\n",
    "                        .orderBy(\"day_of_week\"))\n",
    "    \n",
    "    # TODO: Show results\n",
    "    air_quality_daily.show()\n",
    "    \n",
    "    # TODO: Add your observations\n",
    "    print(\"📝 OBSERVATIONS:\")\n",
    "    print(\"   - Weekday vs weekend patterns: [YOUR ANALYSIS HERE]\")\n",
    "    print(\"   - Pollution trends: [YOUR ANALYSIS HERE]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error analyzing air quality patterns: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 3: BASIC DATA INGESTION (Afternoon - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📥 SECTION 3: DATA INGESTION PIPELINE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📥 SECTION 3: DATA INGESTION PIPELINE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3.1: Create Reusable Data Loading Functions (60 minutes)\n",
    "\n",
    "🎯 **TASK:** Create reusable functions for loading different data formats  \n",
    "💡 **HINT:** Handle schema validation and error handling  \n",
    "📚 **CONCEPTS:** Function design, error handling, schema enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path, expected_schema=None):\n",
    "    \"\"\"\n",
    "    Load CSV data with proper error handling and schema validation\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to CSV file\n",
    "        expected_schema: Optional StructType for schema enforcement\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement CSV loading with options\n",
    "        df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "        # TODO: Add schema validation if provided\n",
    "        if expected_schema:\n",
    "            # Validate schema matches expected\n",
    "            pass\n",
    "            \n",
    "        print(f\"✅ Successfully loaded CSV: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading CSV {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    Load JSON data with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement JSON loading\n",
    "        df = spark.read.json(file_path)\n",
    "        \n",
    "        print(f\"✅ Successfully loaded JSON: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading JSON {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_parquet_data(file_path):\n",
    "    \"\"\"\n",
    "    Load Parquet data with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to Parquet file\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement Parquet loading\n",
    "        df = spark.read.parquet(file_path)\n",
    "        \n",
    "        print(f\"✅ Successfully loaded Parquet: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading Parquet {file_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Data Loading Functions:\n",
      "\n",
      "   Testing CSV loader...\n",
      "✅ Successfully loaded CSV: ../data/raw/city_zones.csv\n",
      "      Records loaded: 8\n",
      "\n",
      "   Testing JSON loader...\n",
      "✅ Successfully loaded JSON: ../data/raw/air_quality.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 235:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Records loaded: 161,522\n",
      "\n",
      "   Testing Parquet loader...\n",
      "✅ Successfully loaded Parquet: ../data/raw/weather_data.parquet\n",
      "      Records loaded: 3,370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TODO: Test your loading functions\n",
    "print(\"🧪 Testing Data Loading Functions:\")\n",
    "\n",
    "test_files = [\n",
    "    (f\"{data_dir}/city_zones.csv\", \"CSV\", load_csv_data),\n",
    "    (f\"{data_dir}/air_quality.json\", \"JSON\", load_json_data), \n",
    "    (f\"{data_dir}/weather_data.parquet\", \"Parquet\", load_parquet_data)\n",
    "]\n",
    "\n",
    "for file_path, file_type, load_func in test_files:\n",
    "    print(f\"\\n   Testing {file_type} loader...\")\n",
    "    test_df = load_func(file_path)\n",
    "    if test_df:\n",
    "        print(f\"      Records loaded: {test_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3.2: Schema Definition and Enforcement (60 minutes)\n",
    "\n",
    "🎯 **TASK:** Define explicit schemas for data consistency  \n",
    "💡 **HINT:** Use StructType and StructField for schema definition  \n",
    "📚 **CONCEPTS:** Schema design, data types, schema enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# TODO: Define schema for traffic sensors\n",
    "traffic_schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    # TODO: Add remaining fields\n",
    "    StructField(\"vehicle_count\", IntegerType(), False),\n",
    "    StructField(\"avg_speed\", DoubleType(), False),\n",
    "    StructField(\"congestion_level\", StringType(), False),\n",
    "    StructField(\"road_type\", StringType(), False),\n",
    "])\n",
    "\n",
    "# TODO: Define schema for air quality data\n",
    "air_quality_schema = StructType([\n",
    "    # TODO: Define all fields for air quality data\n",
    "    # Hint: Look at the JSON structure and define appropriate types\n",
    "    StructField(\"sensor_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    StructField(\"pm25\", DoubleType(), False),\n",
    "    StructField(\"pm10\", DoubleType(), False),\n",
    "    StructField(\"no2\", DoubleType(), False),\n",
    "    StructField(\"co\", DoubleType(), False),\n",
    "    StructField(\"temperature\", DoubleType(), False),\n",
    "    StructField(\"humidity\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# TODO: Define schema for weather data\n",
    "weather_schema = StructType([\n",
    "    # TODO: Define all fields for weather data\n",
    "    StructField(\"station_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    StructField(\"temperature\", DoubleType(), False),\n",
    "    StructField(\"humidity\", DoubleType(), False),\n",
    "    StructField(\"wind_speed\", DoubleType(), False),\n",
    "    StructField(\"wind_direction\", StringType(), False),\n",
    "    StructField(\"precipitation\", DoubleType(), False),\n",
    "    StructField(\"pressure\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# TODO: Define schema for energy data\n",
    "energy_schema = StructType([\n",
    "    # TODO: Define all fields for energy data\n",
    "    StructField(\"meter_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    StructField(\"energy_consumption_kwh\", DoubleType(), False),\n",
    "    StructField(\"peak_demand_kw\", DoubleType(), False),\n",
    "    StructField(\"voltage\", DoubleType(), False),\n",
    "    StructField(\"current\", DoubleType(), False),\n",
    "    StructField(\"power_factor\", DoubleType(), False),\n",
    "    StructField(\"frequency\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Testing Schema Enforcement:\n",
      "✅ Schema enforcement successful for ../data/raw/traffic_sensors.csv\n",
      "   Schema enforcement test passed!\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- no2: double (nullable = true)\n",
      " |-- co: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test schema enforcement\n",
    "print(\"\\n🔍 Testing Schema Enforcement:\")\n",
    "\n",
    "def load_with_schema(file_path, schema, file_format=\"csv\"):\n",
    "    \"\"\"Load data with explicit schema enforcement\"\"\"\n",
    "    try:\n",
    "        if file_format == \"csv\":\n",
    "            df = spark.read.schema(schema).option(\"header\", \"true\").csv(file_path)\n",
    "        elif file_format == \"json\":\n",
    "            df = spark.read.schema(schema).json(file_path)\n",
    "        elif file_format == \"parquet\":\n",
    "            df = spark.read.schema(schema).parquet(file_path)\n",
    "        \n",
    "        print(f\"✅ Schema enforcement successful for {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Schema enforcement failed for {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# TODO: Test with one of your schemas\n",
    "test_schema_df = load_with_schema(f\"{data_dir}/traffic_sensors.csv\", air_quality_schema, \"csv\")\n",
    "if test_schema_df:\n",
    "    print(\"   Schema enforcement test passed!\")\n",
    "    test_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 4: INITIAL DATA TRANSFORMATIONS (Afternoon - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔄 SECTION 4: DATA TRANSFORMATIONS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🔄 SECTION 4: DATA TRANSFORMATIONS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.1: Timestamp Standardization (45 minutes)\n",
    "\n",
    "🎯 **TASK:** Standardize timestamp formats across all datasets  \n",
    "💡 **HINT:** Some datasets may have different timestamp formats  \n",
    "📚 **CONCEPTS:** Date/time handling, format standardization, timezone handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_timestamps(df, timestamp_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Standardize timestamp column across datasets\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        timestamp_col: Name of timestamp column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with standardized timestamps\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Convert timestamps to standard format\n",
    "        standardized_df = (df\n",
    "                          .withColumn(\"timestamp_std\", F.to_timestamp(F.col(timestamp_col)))\n",
    "                          .drop(timestamp_col)\n",
    "                          .withColumnRenamed(\"timestamp_std\", timestamp_col))\n",
    "        \n",
    "        # TODO: Add derived time columns\n",
    "        result_df = (standardized_df\n",
    "                    .withColumn(\"year\", F.year(timestamp_col))\n",
    "                    .withColumn(\"month\", F.month(timestamp_col))\n",
    "                    .withColumn(\"day\", F.dayofmonth(timestamp_col))\n",
    "                    .withColumn(\"hour\", F.hour(timestamp_col))\n",
    "                    .withColumn(\"day_of_week\", F.dayofweek(timestamp_col))\n",
    "                    .withColumn(\"is_weekend\", F.when(F.dayofweek(timestamp_col).isin([1, 7]), True).otherwise(False)))\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error standardizing timestamps: {str(e)}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ Testing Timestamp Standardization:\n",
      "   Traffic data timestamp standardization:\n",
      "+--------------------+----+-----+---+----+-----------+----------+\n",
      "|           timestamp|year|month|day|hour|day_of_week|is_weekend|\n",
      "+--------------------+----+-----+---+----+-----------+----------+\n",
      "|2025-08-29 09:08:...|2025|    8| 29|   9|          6|     false|\n",
      "|2025-08-29 09:08:...|2025|    8| 29|   9|          6|     false|\n",
      "|2025-08-29 09:08:...|2025|    8| 29|   9|          6|     false|\n",
      "|2025-08-29 09:08:...|2025|    8| 29|   9|          6|     false|\n",
      "|2025-08-29 09:08:...|2025|    8| 29|   9|          6|     false|\n",
      "+--------------------+----+-----+---+----+-----------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test timestamp standardization\n",
    "print(\"⏰ Testing Timestamp Standardization:\")\n",
    "\n",
    "# Test with traffic data\n",
    "traffic_std = standardize_timestamps(traffic_df)\n",
    "print(\"   Traffic data timestamp standardization:\")\n",
    "traffic_std.select(\"timestamp\", \"year\", \"month\", \"day\", \"hour\", \"day_of_week\", \"is_weekend\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.2: Geographic Zone Mapping (45 minutes)\n",
    "\n",
    "🎯 **TASK:** Map sensor locations to city zones  \n",
    "💡 **HINT:** Join sensor coordinates with zone boundaries  \n",
    "📚 **CONCEPTS:** Spatial joins, geographic data, coordinate systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_zones(sensor_df, zones_df):\n",
    "    \"\"\"\n",
    "    Map sensor locations to city zones\n",
    "    \n",
    "    Args:\n",
    "        sensor_df: DataFrame with sensor locations (lat, lon)\n",
    "        zones_df: DataFrame with zone boundaries\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with zone information added\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Create join condition for geographic mapping\n",
    "        # A sensor is in a zone if its coordinates fall within zone boundaries\n",
    "        join_condition = (\n",
    "            (sensor_df.location_lat >= zones_df.lat_min) &\n",
    "            (sensor_df.location_lat <= zones_df.lat_max) &\n",
    "            (sensor_df.location_lon >= zones_df.lon_min) &\n",
    "            (sensor_df.location_lon <= zones_df.lon_max)\n",
    "        )\n",
    "        \n",
    "        # TODO: Perform the join\n",
    "        result_df = (sensor_df\n",
    "                    .join(zones_df, join_condition, \"left\")\n",
    "                    .select(sensor_df[\"*\"], \n",
    "                           zones_df.zone_id, \n",
    "                           zones_df.zone_name, \n",
    "                           zones_df.zone_type))\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error mapping to zones: {str(e)}\")\n",
    "        return sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗺️ Testing Geographic Zone Mapping:\n",
      "   Traffic sensors with zone mapping:\n",
      "+-----------+------------------+------------------+-------+---------+\n",
      "|  sensor_id|      location_lat|      location_lon|zone_id|zone_type|\n",
      "+-----------+------------------+------------------+-------+---------+\n",
      "|TRAFFIC_001| 40.72391489670398|-74.01735761585977|   NULL|     NULL|\n",
      "|TRAFFIC_002| 40.79338021680954|-73.96238395488332|   NULL|     NULL|\n",
      "|TRAFFIC_003|  40.7370375639637|-73.96000639125748|   NULL|     NULL|\n",
      "|TRAFFIC_004|40.767856970315535|-73.90977667922886|   NULL|     NULL|\n",
      "|TRAFFIC_005| 40.70081285057606|-73.93017948792821|   NULL|     NULL|\n",
      "|TRAFFIC_006|40.776153933187196|-73.91129408672754|   NULL|     NULL|\n",
      "|TRAFFIC_007| 40.79183698039812|-73.94001750320811|   NULL|     NULL|\n",
      "|TRAFFIC_008|40.753297242047054|-73.98887909088275|   NULL|     NULL|\n",
      "|TRAFFIC_009| 40.78097912391254|-74.01564118001494|   NULL|     NULL|\n",
      "|TRAFFIC_010|40.727582654909995|-73.92145372604821|   NULL|     NULL|\n",
      "+-----------+------------------+------------------+-------+---------+\n",
      "only showing top 10 rows\n",
      "   Sensors by zone type:\n",
      "+-----------+-----+\n",
      "|  zone_type|count|\n",
      "+-----------+-----+\n",
      "|       NULL|78663|\n",
      "|residential|14119|\n",
      "| industrial| 4034|\n",
      "| commercial| 2017|\n",
      "|      mixed| 2017|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test zone mapping\n",
    "print(\"\\n🗺️ Testing Geographic Zone Mapping:\")\n",
    "\n",
    "# Test with traffic sensors\n",
    "traffic_with_zones = map_to_zones(traffic_std, zones_df)\n",
    "print(\"   Traffic sensors with zone mapping:\")\n",
    "traffic_with_zones.select(\"sensor_id\", \"location_lat\", \"location_lon\", \"zone_id\", \"zone_type\").show(10)\n",
    "\n",
    "# TODO: Verify mapping worked correctly\n",
    "zone_distribution = traffic_with_zones.groupBy(\"zone_type\").count().orderBy(F.desc(\"count\"))\n",
    "print(\"   Sensors by zone type:\")\n",
    "zone_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.3: Data Type Conversions and Validations (30 minutes)\n",
    "\n",
    "🎯 **TASK:** Ensure proper data types and add validation columns  \n",
    "💡 **HINT:** Cast columns to appropriate types, add data quality flags  \n",
    "📚 **CONCEPTS:** Data type conversion, validation rules, data quality flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_quality_flags(df, sensor_type):\n",
    "    \"\"\"\n",
    "    Add data quality validation flags to DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        sensor_type: Type of sensor for specific validations\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with quality flags added\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result_df = df\n",
    "        \n",
    "        # TODO: Add general quality flags\n",
    "        result_df = result_df.withColumn(\"has_missing_values\", \n",
    "                                        F.when(F.col(\"sensor_id\").isNull(), True).otherwise(False))\n",
    "        \n",
    "        # TODO: Add sensor-specific validations\n",
    "        if sensor_type == \"traffic\":\n",
    "            # Traffic-specific validations\n",
    "            result_df = (result_df\n",
    "                        .withColumn(\"valid_speed\", \n",
    "                                   F.when((F.col(\"avg_speed\") >= 0) & (F.col(\"avg_speed\") <= 100), True).otherwise(False))\n",
    "                        .withColumn(\"valid_vehicle_count\",\n",
    "                                   F.when(F.col(\"vehicle_count\") >= 0, True).otherwise(False)))\n",
    "        \n",
    "        elif sensor_type == \"air_quality\":\n",
    "            # Air quality specific validations\n",
    "            result_df = (result_df\n",
    "                        .withColumn(\"valid_pm25\",\n",
    "                                   F.when((F.col(\"pm25\") >= 0) & (F.col(\"pm25\") <= 500), True).otherwise(False))\n",
    "                        .withColumn(\"valid_temperature\",\n",
    "                                   F.when((F.col(\"temperature\") >= -50) & (F.col(\"temperature\") <= 50), True).otherwise(False)))\n",
    "        \n",
    "        # TODO: Add more sensor-specific validations\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error adding quality flags: {str(e)}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏷️ Testing Data Quality Flags:\n",
      "   Traffic data with quality flags:\n",
      "+-----------+------------------+-------------+-----------+-------------------+\n",
      "|  sensor_id|         avg_speed|vehicle_count|valid_speed|valid_vehicle_count|\n",
      "+-----------+------------------+-------------+-----------+-------------------+\n",
      "|TRAFFIC_001|24.891450679573545|           26|       true|               true|\n",
      "|TRAFFIC_002| 14.24125954375276|           26|       true|               true|\n",
      "|TRAFFIC_003|22.931162239013865|           31|       true|               true|\n",
      "|TRAFFIC_004|37.542924636645445|           26|       true|               true|\n",
      "|TRAFFIC_005|36.519036605733355|           54|       true|               true|\n",
      "|TRAFFIC_006| 21.86641271210729|           50|       true|               true|\n",
      "|TRAFFIC_007|  26.1833011870564|           33|       true|               true|\n",
      "|TRAFFIC_008|              10.0|           20|       true|               true|\n",
      "|TRAFFIC_009| 36.93425409236505|           25|       true|               true|\n",
      "|TRAFFIC_010|29.831339353455405|           31|       true|               true|\n",
      "+-----------+------------------+-------------+-----------+-------------------+\n",
      "only showing top 10 rows\n",
      "   Quality statistics:\n",
      "+-----------------+-------------------------+-------------+\n",
      "|valid_speed_count|valid_vehicle_count_count|total_records|\n",
      "+-----------------+-------------------------+-------------+\n",
      "|           100808|                   100850|       100850|\n",
      "+-----------------+-------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test data quality flags\n",
    "print(\"\\n🏷️ Testing Data Quality Flags:\")\n",
    "\n",
    "# Test with traffic data\n",
    "traffic_with_flags = add_data_quality_flags(traffic_with_zones, \"traffic\")\n",
    "print(\"   Traffic data with quality flags:\")\n",
    "traffic_with_flags.select(\"sensor_id\", \"avg_speed\", \"vehicle_count\", \"valid_speed\", \"valid_vehicle_count\").show(10)\n",
    "\n",
    "# TODO: Check quality flag distribution\n",
    "quality_stats = (traffic_with_flags\n",
    "                .agg(F.sum(F.when(F.col(\"valid_speed\"), 1).otherwise(0)).alias(\"valid_speed_count\"),\n",
    "                     F.sum(F.when(F.col(\"valid_vehicle_count\"), 1).otherwise(0)).alias(\"valid_vehicle_count_count\"),\n",
    "                     F.count(\"*\").alias(\"total_records\")))\n",
    "\n",
    "print(\"   Quality statistics:\")\n",
    "quality_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# DAY 1 DELIVERABLES & CHECKPOINTS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📋 DAY 1 COMPLETION CHECKLIST\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📋 DAY 1 COMPLETION CHECKLIST\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ COMPLETION STATUS:\n",
      "   ✅ Spark Session Created\n",
      "   ✅ Database Connection Tested\n",
      "   ✅ Data Loaded Successfully\n",
      "   ✅ Data Quality Assessed\n",
      "   ✅ Loading Functions Created\n",
      "   ✅ Schemas Defined\n",
      "   ✅ Timestamp Standardization Working\n",
      "   ✅ Zone Mapping Implemented\n",
      "   ✅ Quality Flags Added\n",
      "\n",
      "📊 Overall Completion: 100.0%\n",
      "🎉 Great job! You're ready for Day 2!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Complete this checklist by running the validation functions\n",
    "\n",
    "def validate_day1_completion():\n",
    "    \"\"\"Validate that Day 1 objectives have been met\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"spark_session_created\": False,\n",
    "        \"database_connection_tested\": False,\n",
    "        \"data_loaded_successfully\": False,\n",
    "        \"data_quality_assessed\": False,\n",
    "        \"loading_functions_created\": False,\n",
    "        \"schemas_defined\": False,\n",
    "        \"timestamp_standardization_working\": False,\n",
    "        \"zone_mapping_implemented\": False,\n",
    "        \"quality_flags_added\": False\n",
    "    }\n",
    "    \n",
    "    # TODO: Add validation logic for each item\n",
    "    try:\n",
    "        # Check Spark session\n",
    "        if spark and spark.sparkContext._jsc:\n",
    "            checklist[\"spark_session_created\"] = True\n",
    "            \n",
    "        # Check if data exists\n",
    "        if ('traffic_df' in globals() and traffic_df.count() > 0) and ('weather_df' in globals() and weather_df.count() > 0) and ('air_quality_df' in globals() and air_quality_df is not None and air_quality_df.count() > 0) and ('energy_df' in globals() and energy_df.count() > 0) and ('zones_df' in globals() and zones_df.count() > 0):\n",
    "            checklist[\"data_loaded_successfully\"] = True\n",
    "            \n",
    "        # TODO: Add more validation checks\n",
    "        if 'db_connected' in globals() and db_connected:\n",
    "            checklist[\"database_connection_tested\"] = True\n",
    "\n",
    "        if  \"assess_data_quality\" in globals():\n",
    "            checklist[\"data_quality_assessed\"] = True\n",
    "        \n",
    "        loading_functions = ['load_csv_data', 'load_json_data', 'load_parquet_data']\n",
    "        if all(func in globals() for func in loading_functions):\n",
    "            checklist[\"loading_functions_created\"] = True\n",
    "        \n",
    "        schema_vars = ['traffic_schema', 'air_quality_schema', 'weather_schema', 'energy_schema']\n",
    "        if all(schema in globals() for schema in schema_vars):\n",
    "            checklist[\"schemas_defined\"] = True\n",
    "        \n",
    "        if 'standardize_timestamps' in globals() and 'traffic_std' in globals():\n",
    "            checklist[\"timestamp_standardization_working\"] = True\n",
    "        \n",
    "        if 'map_to_zones' in globals() and 'traffic_with_zones' in globals():\n",
    "            checklist[\"zone_mapping_implemented\"] = True\n",
    "         \n",
    "        if 'add_data_quality_flags' in globals() and 'traffic_with_flags' in globals():\n",
    "            checklist[\"quality_flags_added\"] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Validation error: {str(e)}\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"✅ COMPLETION STATUS:\")\n",
    "    for item, status in checklist.items():\n",
    "        status_icon = \"✅\" if status else \"❌\"\n",
    "        print(f\"   {status_icon} {item.replace('_', ' ').title()}\")\n",
    "    \n",
    "    import builtins\n",
    "    completion_rate = builtins.sum(checklist.values()) / len(checklist) * 100\n",
    "    print(f\"\\n📊 Overall Completion: {completion_rate:.1f}%\")\n",
    "    \n",
    "    if completion_rate >= 80:\n",
    "        print(\"🎉 Great job! You're ready for Day 2!\")\n",
    "    else:\n",
    "        print(\"📝 Please review incomplete items before proceeding to Day 2.\")\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# TODO: Run the validation\n",
    "completion_status = validate_day1_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🚀 WHAT'S NEXT?\n",
    "\n",
    "---\n",
    "\n",
    "## 📅 DAY 2 PREVIEW: Data Quality & Cleaning Pipeline\n",
    "\n",
    "Tomorrow you'll work on:\n",
    "1. 🔍 Comprehensive data quality assessment\n",
    "2. 🧹 Advanced cleaning procedures for IoT sensor data  \n",
    "3. 📊 Missing data handling and interpolation strategies\n",
    "4. 🚨 Outlier detection and treatment methods\n",
    "5. 📏 Data standardization and normalization\n",
    "\n",
    "## 📚 RECOMMENDED PREPARATION:\n",
    "- Review PySpark DataFrame operations\n",
    "- Read about time series data quality challenges\n",
    "- Familiarize yourself with statistical outlier detection methods\n",
    "\n",
    "## 💾 SAVE YOUR WORK:\n",
    "- Commit your notebook to Git\n",
    "- Document any issues or questions for tomorrow\n",
    "- Save any custom functions you created\n",
    "\n",
    "## 🤝 QUESTIONS?\n",
    "- Post in the class discussion forum\n",
    "- Review Spark documentation for any unclear concepts\n",
    "- Prepare questions for tomorrow's Q&A session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Don't forget to save your notebook and commit your changes!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Save your progress\n",
    "print(\"\\n💾 Don't forget to save your notebook and commit your changes!\")\n",
    "\n",
    "# Clean up (optional)\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
