{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Environment Setup & Data Exploration\n",
    "# Smart City IoT Analytics Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ LEARNING OBJECTIVES:\n",
    "- Configure Spark cluster and development environment\n",
    "- Understand IoT data characteristics and challenges  \n",
    "- Implement basic data ingestion patterns\n",
    "- Explore PySpark DataFrame operations\n",
    "\n",
    "## üìÖ SCHEDULE:\n",
    "**Morning (4 hours):**\n",
    "1. Environment Setup (2 hours)\n",
    "2. Data Exploration (2 hours)\n",
    "\n",
    "**Afternoon (4 hours):**  \n",
    "3. Basic Data Ingestion (2 hours)\n",
    "4. Initial Data Transformations (2 hours)\n",
    "\n",
    "## ‚úÖ DELIVERABLES:\n",
    "- Working Spark cluster with all services running\n",
    "- Data ingestion notebook with basic EDA\n",
    "- Documentation of data quality findings  \n",
    "- Initial data loading pipeline functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Welcome to the Smart City IoT Analytics Pipeline!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Welcome to the Smart City IoT Analytics Pipeline!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 1: ENVIRONMENT SETUP (Morning - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.1: Initialize Spark Session (15 minutes)\n",
    "\n",
    "üéØ **TASK:** Create a Spark session configured for local development  \n",
    "üí° **HINT:** Use SparkSession.builder with appropriate configurations  \n",
    "üìö **DOCS:** https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "\n",
    "**TODO:** Create Spark session with the following configurations:\n",
    "- App name: \"SmartCityIoTPipeline-Day1\"\n",
    "- Master: \"local[*]\" (use all available cores)\n",
    "- Memory: \"4g\" for driver\n",
    "- Additional configs for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/05 11:43:10 WARN Utils: Your hostname, Zipcoders-MacBook-Pro-3.local, resolves to a loopback address: 127.0.0.1; using 192.168.87.79 instead (on interface en0)\n",
      "25/09/05 11:43:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/09/05 11:43:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/05 11:43:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/09/05 11:43:11 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Details:\n",
      "   App Name: SmartCityIoTPipeline-Day1\n",
      "   Spark Version: 4.0.0\n",
      "   Master: local[*]\n",
      "   Default Parallelism: 8\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create Spark session with the following configurations:\n",
    "jdbc_jar_path = \"/Users/sai/Documents/Projects/ninth-week/SparkCity/postgresql-42.7.3.jar\"\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"SmartCityIoTPipeline-Day1\")  # TODO: Add your app name\n",
    "         .master(\"local[*]\")   # TODO: Add master configuration\n",
    "         .config(\"spark.driver.memory\", \"4g\")  # TODO: Set memory\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "         .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "         .config(\"spark.jars\", jdbc_jar_path)  # <-- Add this line for JDBC driver\n",
    "         .getOrCreate())\n",
    "\n",
    "# TODO: Verify Spark session is working\n",
    "print(\"‚úÖ Spark Session Details:\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   Default Parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.2: Verify Infrastructure (15 minutes)\n",
    "\n",
    "üéØ **TASK:** Check that all infrastructure services are running  \n",
    "üí° **HINT:** Test database connectivity and file system access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection successful!\n",
      "\n",
      "üåê Spark UI should be accessible at: http://localhost:4040\n",
      "   (Open this in your browser to monitor Spark jobs)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test PostgreSQL connection\n",
    "def test_database_connection():\n",
    "    \"\"\"Test connection to PostgreSQL database\"\"\"\n",
    "    try:\n",
    "        # Database connection parameters\n",
    "        db_properties = {\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"password\", \n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "        \n",
    "        # TODO: Replace with actual connection test\n",
    "        # Test query - should create a simple DataFrame from database\n",
    "        test_df = spark.read.jdbc(\n",
    "            url=\"jdbc:postgresql://localhost:5432/smartcity\",\n",
    "            table=\"(SELECT 1 as test_column) as test_table\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "        \n",
    "        # TODO: Collect and display result\n",
    "        result = test_df.collect()\n",
    "        print(\"‚úÖ Database connection successful!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database connection failed: {str(e)}\")\n",
    "        print(\"üí° Make sure PostgreSQL container is running: docker-compose up -d\")\n",
    "        return False\n",
    "\n",
    "# TODO: Run the database connection test\n",
    "db_connected = test_database_connection()\n",
    "\n",
    "# TODO: Check Spark UI accessibility\n",
    "print(\"\\nüåê Spark UI should be accessible at: http://localhost:4040\")\n",
    "print(\"   (Open this in your browser to monitor Spark jobs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.3: Generate Sample Data (30 minutes)\n",
    "\n",
    "üéØ **TASK:** Run the data generation script to create sample IoT data  \n",
    "üí° **HINT:** Use the provided data generation script or run it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating data... (0/5 files exist)\n",
      "   Project root: /Users/sai/Documents/Projects/ninth-week/SparkCity\n",
      "   Running script from: /Users/sai/Documents/Projects/ninth-week/SparkCity\n",
      "‚úÖ Data generation successful!\n"
     ]
    }
   ],
   "source": [
    "# Generate Sample IoT Data\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def check_and_generate_data():\n",
    "    \"\"\"Check if data exists, generate if missing\"\"\"\n",
    "    data_files = [\"traffic_sensors.csv\", \"air_quality.json\", \"weather_data.parquet\", \n",
    "                  \"energy_meters.csv\", \"city_zones.csv\"]\n",
    "    data_path = \"data/raw\"\n",
    "    \n",
    "    # Check existing files\n",
    "    existing = [f for f in data_files if os.path.exists(f\"{data_path}/{f}\")]\n",
    "    \n",
    "    if len(existing) == len(data_files):\n",
    "        print(f\"‚úÖ All {len(data_files)} data files found!\")\n",
    "        return True\n",
    "    \n",
    "    # Generate missing data\n",
    "    print(f\"üîÑ Generating data... ({len(existing)}/{len(data_files)} files exist)\")\n",
    "    \n",
    "    try:\n",
    "        # Get the project root (go up one level from notebooks folder)\n",
    "        notebook_dir = os.getcwd()  # Current directory (notebooks/)\n",
    "        project_root = os.path.dirname(notebook_dir)  # Go up one level to SparkCity/\n",
    "        \n",
    "        print(f\"   Project root: {project_root}\")\n",
    "        print(f\"   Running script from: {project_root}\")\n",
    "        \n",
    "        # Run from project root directory\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"scripts/generate_data.py\"], \n",
    "            cwd=project_root,  # Run from SparkCity/ directory\n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            timeout=300\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Data generation successful!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Generation failed!\")\n",
    "            if result.stderr:\n",
    "                print(f\"   Error: {result.stderr.strip()}\")\n",
    "            if result.stdout:\n",
    "                print(f\"   Output: {result.stdout.strip()}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run data check/generation\n",
    "data_ready = check_and_generate_data()\n",
    "\n",
    "# If failed, provide clear manual instructions\n",
    "if not data_ready:\n",
    "    print(\"\\nüîß MANUAL FIX:\")\n",
    "    print(\"1. Open terminal\")\n",
    "    print(\"2. Run: cd /Users/sai/Documents/Projects/ninth-week/SparkCity\")\n",
    "    print(\"3. Run: python scripts/generate_data.py\")\n",
    "    print(\"4. Re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 2: DATA EXPLORATION (Morning - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä SECTION 2: EXPLORATORY DATA ANALYSIS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä SECTION 2: EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.1: Load and Examine Data Sources (45 minutes)\n",
    "\n",
    "üéØ **TASK:** Load each data source and examine its structure  \n",
    "üí° **HINT:** Use appropriate Spark readers for different file formats  \n",
    "üìö **CONCEPTS:** Schema inference, file formats, data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = \"../data/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Loading City Zones Reference Data...\n",
      "   üìä Records: 8\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- zone_id: string (nullable = true)\n",
      " |-- zone_name: string (nullable = true)\n",
      " |-- zone_type: string (nullable = true)\n",
      " |-- lat_min: double (nullable = true)\n",
      " |-- lat_max: double (nullable = true)\n",
      " |-- lon_min: double (nullable = true)\n",
      " |-- lon_max: double (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|zone_id |zone_name         |zone_type  |lat_min|lat_max|lon_min|lon_max|population|\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|ZONE_001|Downtown          |commercial |40.72  |40.74  |-74.01 |-73.99 |25000     |\n",
      "|ZONE_002|Financial District|commercial |40.7   |40.72  |-74.02 |-74.0  |15000     |\n",
      "|ZONE_003|Residential North |residential|40.76  |40.8   |-74.0  |-73.98 |45000     |\n",
      "|ZONE_004|Residential South |residential|40.7   |40.72  |-73.98 |-73.96 |38000     |\n",
      "|ZONE_005|Industrial Park   |industrial |40.74  |40.76  |-74.02 |-74.0  |5000      |\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load city zones reference data\n",
    "print(\"üìç Loading City Zones Reference Data...\")\n",
    "try:\n",
    "    zones_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/city_zones.csv\")\n",
    "    \n",
    "    # TODO: Display basic information about zones\n",
    "    print(f\"   üìä Records: {zones_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    zones_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    zones_df.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading zones data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöó Loading Traffic Sensors Data...\n",
      "   üìä Records: 100850\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- vehicle_count: integer (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      " |-- congestion_level: string (nullable = true)\n",
      " |-- road_type: string (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+-------------+------------------+----------------+-----------+\n",
      "|  sensor_id|           timestamp|      location_lat|      location_lon|vehicle_count|         avg_speed|congestion_level|  road_type|\n",
      "+-----------+--------------------+------------------+------------------+-------------+------------------+----------------+-----------+\n",
      "|TRAFFIC_001|2025-08-29 11:43:...|  40.7901819372493|-73.94717853181487|            3| 65.80749574895847|             low|   arterial|\n",
      "|TRAFFIC_002|2025-08-29 11:43:...|40.763328789233874|-73.97617748104044|           39| 41.02141713733583|          medium|    highway|\n",
      "|TRAFFIC_003|2025-08-29 11:43:...|40.797135706539336|-73.94914250782259|           24| 45.59579718445981|          medium|    highway|\n",
      "|TRAFFIC_004|2025-08-29 11:43:...| 40.74663304704115| -73.9810073793957|           15| 67.94499054227262|          medium|residential|\n",
      "|TRAFFIC_005|2025-08-29 11:43:...|40.792497877302864|-73.98873557827757|           23|62.170026447114694|          medium|residential|\n",
      "+-----------+--------------------+------------------+------------------+-------------+------------------+----------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load traffic sensors data  \n",
    "print(\"\\nüöó Loading Traffic Sensors Data...\")\n",
    "try:\n",
    "    # TODO: Load CSV file with proper options\n",
    "    traffic_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/traffic_sensors.csv\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   üìä Records: {traffic_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    traffic_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    traffic_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading traffic data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå´Ô∏è Loading Air Quality Data...\n",
      "   üîç Loading JSON file...\n",
      "   üìä Total records loaded: 13460\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- co: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- no2: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+---------+------------------+--------------------+\n",
      "|                co|         humidity|     location_lat|      location_lon|               no2|             pm10|              pm25|sensor_id|       temperature|           timestamp|\n",
      "+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+---------+------------------+--------------------+\n",
      "|1.1315789786272916|65.40801427095481|40.71777919420702|-73.97740310486924|23.332950074077015|41.50861228736408|22.379731163161413|   AQ_001|12.139692045179405|2025-08-29T11:43:...|\n",
      "| 1.664750785503347|40.19554838899451|40.71504263689168|-73.92558405687251|46.533958177172345|48.56127607927163|15.231898621255896|   AQ_002|16.922606157382962|2025-08-29T11:43:...|\n",
      "|0.7406702764368348|61.10343676368756|40.75110443557877| -73.9257892625344|28.581838110006565|32.34430289379155|30.358253847448196|   AQ_003|23.620500164796805|2025-08-29T11:43:...|\n",
      "|0.7545167804453979|65.82833518965496|40.78968677543556|-73.99876013615332| 20.20044333718749|57.72582158522364| 27.90487647106579|   AQ_004|20.206061730412582|2025-08-29T11:43:...|\n",
      "|2.1301710362216637|  33.853289335332|40.70430158414048|-73.98173046321281| 31.81697765382183|58.30111440016043|30.967919184213304|   AQ_005| 24.29317170604617|2025-08-29T11:43:...|\n",
      "+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+---------+------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load air quality data (JSON format) - CORRECTED VERSION\n",
    "print(\"\\nüå´Ô∏è Loading Air Quality Data...\")\n",
    "    # First, try loading without corrupt record handling since the JSON seems valid\n",
    "print(\"   üîç Loading JSON file...\")\n",
    "    \n",
    "air_quality_df = (spark.read\n",
    "                  .option(\"multiline\", \"true\")\n",
    "                  .json(f\"{data_dir}/air_quality.json\"))\n",
    "    \n",
    "# Check if we loaded successfully\n",
    "total_records = air_quality_df.count()\n",
    "print(f\"   üìä Total records loaded: {total_records}\")\n",
    "    \n",
    "# Check the schema\n",
    "print(f\"   üìã Schema:\")\n",
    "air_quality_df.printSchema()\n",
    "    \n",
    "# Show sample data\n",
    "print(f\"   üîç Sample Data:\")\n",
    "air_quality_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå§Ô∏è Loading Weather Data...\n",
      "   üìä Records: 3370\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- wind_direction: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+------------------+\n",
      "| station_id|           timestamp|      location_lat|      location_lon|       temperature|          humidity|        wind_speed|    wind_direction|precipitation|          pressure|\n",
      "+-----------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+------------------+\n",
      "|WEATHER_001|2025-08-29T11:43:...|40.751832533111674|-73.91709782788587|20.856074751824877| 86.39168934977558| 7.639710983854159|299.61292658053134|          0.0|1021.1113540837691|\n",
      "|WEATHER_002|2025-08-29T11:43:...| 40.72323067121871|-73.96690776264975| 20.43419062364872|56.884999823391674| 10.35965693496526| 247.2604452337947|          0.0|1018.1475386191756|\n",
      "|WEATHER_003|2025-08-29T11:43:...|40.756641446170555|-73.90517147123623| 20.87262304842677| 80.19975985576102| 0.523457637967726|55.540116757418836|          0.0|1011.5596505337078|\n",
      "|WEATHER_004|2025-08-29T11:43:...|40.766831852490945|-73.91398333913382| 21.11952097213624|59.589642199387754|0.5989564365027281|331.21843459858184|          0.0|1013.7923737741877|\n",
      "|WEATHER_005|2025-08-29T11:43:...|40.737261032312375|-73.93361031136246|   20.923172376472|62.548198116194015| 31.65895108860382|220.15702878979718|          0.0|1000.9360647109107|\n",
      "+-----------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load weather data (Parquet format)\n",
    "print(\"\\nüå§Ô∏è Loading Weather Data...\")\n",
    "try:\n",
    "    # TODO: Load Parquet file - another different format!\n",
    "    weather_df = spark.read.parquet(f\"{data_dir}/weather_data.parquet\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   üìä Records: {weather_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    weather_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    weather_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading weather data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° Loading Energy Meters Data...\n",
      "   üìä Records: 201800\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- meter_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- power_consumption: double (nullable = true)\n",
      " |-- voltage: double (nullable = true)\n",
      " |-- current: double (nullable = true)\n",
      " |-- power_factor: double (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|   meter_id|           timestamp|building_type|      location_lat|      location_lon| power_consumption|           voltage|           current|      power_factor|\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|ENERGY_0001|2025-08-29 11:43:...|       retail| 40.70865350158157|-73.99175505143161|42.268677281313025|233.34826504710304|181.13988236758814|0.8587659092753049|\n",
      "|ENERGY_0002|2025-08-29 11:43:...|  residential|40.771079250065405|-73.97218196345709| 1.862645634678003|235.35768059820208| 7.914106010663294|0.9204311183141889|\n",
      "|ENERGY_0003|2025-08-29 11:43:...|  residential| 40.70703546629731|-73.95708057621566| 2.659785574655394|237.07680455343606|11.219088175519465|0.9116423573681385|\n",
      "|ENERGY_0004|2025-08-29 11:43:...|       retail|40.718048015036295|-74.01223033137819| 38.79643002008927|236.25397695627828|164.21492886559545|0.9299160715780659|\n",
      "|ENERGY_0005|2025-08-29 11:43:...|       retail| 40.72508216631324|-73.96731692369401| 42.02180122023927| 244.6449950247764| 171.7664455632273|0.8882343668339194|\n",
      "+-----------+--------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TODO: Load energy meters data\n",
    "print(\"\\n‚ö° Loading Energy Meters Data...\")\n",
    "try:\n",
    "    # TODO: Load CSV file\n",
    "    energy_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/energy_meters.csv\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   üìä Records: {energy_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    energy_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    energy_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading energy data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.2: Basic Data Quality Assessment (45 minutes)\n",
    "\n",
    "üéØ **TASK:** Assess data quality across all datasets  \n",
    "üí° **HINT:** Check for missing values, duplicates, data ranges  \n",
    "üìö **CONCEPTS:** Data profiling, quality metrics, anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Perform basic data quality assessment on a DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to assess\n",
    "        dataset_name: Name of the dataset for reporting\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìã Data Quality Assessment: {dataset_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # TODO: Basic statistics\n",
    "    total_rows = df.count()\n",
    "    total_cols = len(df.columns)\n",
    "    print(f\"   üìä Dimensions: {total_rows:,} rows √ó {total_cols} columns\")\n",
    "    \n",
    "    # TODO: Check for missing values\n",
    "    print(f\"   üîç Missing Values:\")\n",
    "    for col in df.columns:\n",
    "        missing_count = df.filter(F.col(col).isNull()).count()\n",
    "        missing_pct = (missing_count / total_rows) * 100\n",
    "        if missing_count > 0:\n",
    "            print(f\"      {col}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    # TODO: Check for duplicate records\n",
    "    duplicate_count = total_rows - df.dropDuplicates().count()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"   üîÑ Duplicate Records: {duplicate_count:,}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No duplicate records found\")\n",
    "    \n",
    "    # TODO: Numeric column statistics\n",
    "    numeric_cols = [field.name for field in df.schema.fields \n",
    "                   if field.dataType in [IntegerType(), DoubleType(), FloatType(), LongType()]]\n",
    "    \n",
    "    if numeric_cols:\n",
    "        print(f\"   üìà Numeric Columns Summary:\")\n",
    "        # Show basic statistics for numeric columns\n",
    "        df.select(numeric_cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Data Quality Assessment: City Zones\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 8 rows √ó 8 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/05 11:43:23 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|summary|             lat_min|             lat_max|            lon_min|            lon_max|        population|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|  count|                   8|                   8|                  8|                  8|                 8|\n",
      "|   mean|  40.730000000000004|             40.7525| -73.99125000000001|          -73.97125|           21250.0|\n",
      "| stddev|0.023904572186687328|0.028157719063465373|0.02474873734153055|0.02474873734153458|14260.334598358582|\n",
      "|    min|                40.7|               40.72|             -74.02|              -74.0|              5000|\n",
      "|    max|               40.76|                40.8|             -73.96|             -73.94|             45000|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "\n",
      "\n",
      "üìã Data Quality Assessment: Traffic Sensors\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 100,850 rows √ó 8 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+-------------------+-------------------+------------------+------------------+\n",
      "|summary|       location_lat|       location_lon|     vehicle_count|         avg_speed|\n",
      "+-------+-------------------+-------------------+------------------+------------------+\n",
      "|  count|             100850|             100850|            100850|            100850|\n",
      "|   mean|  40.75278681602612| -73.95589585865889| 22.83709469509172| 45.64269040577325|\n",
      "| stddev|0.03259179926090208|0.03312428257651053|14.047125851047431| 17.03343151606037|\n",
      "|    min| 40.702939697464906| -74.01954049897303|                 0|               5.0|\n",
      "|    max|  40.79862448764542| -73.90342187874433|                93|113.05734668929286|\n",
      "+-------+-------------------+-------------------+------------------+------------------+\n",
      "\n",
      "\n",
      "üìã Data Quality Assessment: Air Quality\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 13,460 rows √ó 10 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+-------------------+------------------+-------------------+-------------------+-----------------+------------------+-----------------+------------------+\n",
      "|summary|                 co|          humidity|       location_lat|       location_lon|              no2|              pm10|             pm25|       temperature|\n",
      "+-------+-------------------+------------------+-------------------+-------------------+-----------------+------------------+-----------------+------------------+\n",
      "|  count|              13460|             13460|              13460|              13460|            13460|             13460|            13460|             13460|\n",
      "|   mean| 1.2937798107778513| 55.03808636842673|  40.74531183459404| -73.95982646431543|32.21108126849809| 42.75136543817467|26.89235186450606|19.976054399554698|\n",
      "| stddev|0.42994777901799097|14.369070435265995|0.03210168776245781|0.03524599672686608| 10.6963465475577|13.098206321346577|8.591317798736757| 7.932318059580668|\n",
      "|    min|                0.0|30.005466472438005|  40.70134867173775| -74.00617233747684|              0.0|               0.0|              0.0|-9.945562055040885|\n",
      "|    max| 3.0226808959137124| 79.99789982072713|  40.79921879529554| -73.90129107491357|74.02385973098154| 92.70675720329365|59.49797769864463|52.229502467154646|\n",
      "+-------+-------------------+------------------+-------------------+-------------------+-----------------+------------------+-----------------+------------------+\n",
      "\n",
      "\n",
      "üìã Data Quality Assessment: Weather Stations\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 3,370 rows √ó 10 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+-------------------+--------------------+------------------+\n",
      "|summary|        location_lat|       location_lon|       temperature|          humidity|          wind_speed|     wind_direction|       precipitation|          pressure|\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+-------------------+--------------------+------------------+\n",
      "|  count|                3370|               3370|              3370|              3370|                3370|               3370|                3370|              3370|\n",
      "|   mean|   40.74059534457311| -73.95173296550635|20.029696344098962|60.359868429484465|  7.9964601308250005| 186.78038043601745|0.052147827659560185|1013.1276255274769|\n",
      "| stddev|0.021762199881357226|0.03397917170177398| 3.243513118468253|14.879517525148627|   7.876205909445852| 103.92829997229444| 0.22011987476949252|10.172046470760808|\n",
      "|    min|   40.70234792629635| -74.01095952092722|10.671743053504096|              20.0|0.004920433697778091|0.06038925511785198|                 0.0| 979.4013691273003|\n",
      "|    max|  40.766831852490945| -73.90517147123623|30.125733663441018|             100.0|   55.40035642546959|  359.8432926282403|   2.509632226055626| 1053.669617197452|\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+-------------------+--------------------+------------------+\n",
      "\n",
      "\n",
      "üìã Data Quality Assessment: Energy Meters\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 201,800 rows √ó 9 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+--------------------+-------------------+------------------+------------------+-----------------+--------------------+\n",
      "|summary|        location_lat|       location_lon| power_consumption|           voltage|          current|        power_factor|\n",
      "+-------+--------------------+-------------------+------------------+------------------+-----------------+--------------------+\n",
      "|  count|              201800|             201800|            201800|            201800|           201800|              201800|\n",
      "|   mean|  40.747023396739486| -73.95443714555799| 15.90777156769843|239.99539808869932|66.31039471159629|  0.9000313232480632|\n",
      "| stddev|0.030433681288384317|0.03446320633271663|17.654887840473002| 4.998907510036675|73.62036936264094|0.028878550069612513|\n",
      "|    min|   40.70008160308784| -74.01987424873846| 1.680013211460792|217.60499025089814|6.631512544634478|  0.8500006517757239|\n",
      "|    max|  40.799902331432996| -73.90050298926252| 64.99991239500325|263.67126861249164|289.0949276653417|  0.9499996620741166|\n",
      "+-------+--------------------+-------------------+------------------+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Assess quality for each dataset\n",
    "datasets = [\n",
    "    (zones_df, \"City Zones\"),\n",
    "    (traffic_df, \"Traffic Sensors\"), \n",
    "    (air_quality_df, \"Air Quality\"),\n",
    "    (weather_df, \"Weather Stations\"),\n",
    "    (energy_df, \"Energy Meters\")\n",
    "]\n",
    "\n",
    "for df, name in datasets:\n",
    "    try:\n",
    "        assess_data_quality(df, name)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error assessing {name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.3: Temporal Analysis (30 minutes)\n",
    "\n",
    "üéØ **TASK:** Analyze temporal patterns in the IoT data  \n",
    "üí° **HINT:** Look at data distribution over time, identify patterns  \n",
    "üìö **CONCEPTS:** Time series analysis, temporal patterns, data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚è∞ TEMPORAL PATTERN ANALYSIS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60) \n",
    "print(\"‚è∞ TEMPORAL PATTERN ANALYSIS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöó Traffic Patterns by Hour:\n",
      "+----+------------------+--------+\n",
      "|hour|      avg_vehicles|readings|\n",
      "+----+------------------+--------+\n",
      "|   0|19.347142857142856|    4200|\n",
      "|   1|19.194761904761904|    4200|\n",
      "|   2|19.359761904761903|    4200|\n",
      "|   3|19.230238095238096|    4200|\n",
      "|   4|19.411666666666665|    4200|\n",
      "|   5|19.496190476190478|    4200|\n",
      "|   6| 19.27357142857143|    4200|\n",
      "|   7| 33.56952380952381|    4200|\n",
      "|   8| 33.46214285714286|    4200|\n",
      "|   9| 33.64738095238095|    4200|\n",
      "|  10|              19.3|    4200|\n",
      "|  11|19.196941176470588|    4250|\n",
      "|  12|             19.44|    4200|\n",
      "|  13|19.405714285714286|    4200|\n",
      "|  14|19.200714285714287|    4200|\n",
      "|  15| 19.08142857142857|    4200|\n",
      "|  16| 19.25642857142857|    4200|\n",
      "|  17| 33.44452380952381|    4200|\n",
      "|  18| 33.48142857142857|    4200|\n",
      "|  19| 33.39738095238095|    4200|\n",
      "|  20|19.241904761904763|    4200|\n",
      "|  21|19.249285714285715|    4200|\n",
      "|  22|19.282857142857143|    4200|\n",
      "|  23|19.162619047619046|    4200|\n",
      "+----+------------------+--------+\n",
      "\n",
      "üìù OBSERVATIONS:\n",
      "   - Rush hour patterns: Vehicle counts are highest between 7-9 AM and 5-7 PM, indicating morning and evening rush hours.\n",
      "   - Off-peak periods: Vehicle counts are lowest between 10 AM - 4 PM, indicating off-peak hours.\n",
      "   - Peak traffic hours: The highest vehicle counts occur between 8-9 AM and 6-7 PM.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Analyze traffic patterns by hour\n",
    "print(\"\\nüöó Traffic Patterns by Hour:\")\n",
    "try:\n",
    "    # TODO: Extract hour from timestamp and analyze vehicle counts\n",
    "    traffic_hourly = (traffic_df\n",
    "                     .withColumn(\"hour\", F.hour(\"timestamp\"))\n",
    "                     .groupBy(\"hour\")\n",
    "                     .agg(F.avg(\"vehicle_count\").alias(\"avg_vehicles\"),\n",
    "                          F.count(\"*\").alias(\"readings\"))\n",
    "                     .orderBy(\"hour\"))\n",
    "    \n",
    "    # TODO: Show the results\n",
    "    traffic_hourly.show(24)\n",
    "    \n",
    "    # TODO: What patterns do you notice? Add your observations here:\n",
    "    print(\"üìù OBSERVATIONS:\")\n",
    "    print(\"   - Rush hour patterns: Vehicle counts are highest between 7-9 AM and 5-7 PM, indicating morning and evening rush hours.\")\n",
    "    print(\"   - Off-peak periods: Vehicle counts are lowest between 10 AM - 4 PM, indicating off-peak hours.\")\n",
    "    print(\"   - Peak traffic hours: The highest vehicle counts occur between 8-9 AM and 6-7 PM.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error analyzing traffic patterns: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå´Ô∏è Air Quality Patterns by Day of Week:\n",
      "+-----------+------------------+------------------+\n",
      "|day_of_week|          avg_pm25|           avg_no2|\n",
      "+-----------+------------------+------------------+\n",
      "|          1|26.869501876537324|31.949400424775533|\n",
      "|          2| 27.03651775052699|32.005987921964525|\n",
      "|          3|26.878296053390265|32.434042696557675|\n",
      "|          4|26.827758674819695| 32.26709352558483|\n",
      "|          5| 26.80864770596835| 32.04311845036208|\n",
      "|          6|26.992614934781333|32.433433201576705|\n",
      "|          7|26.832081648535546|32.342176492695124|\n",
      "+-----------+------------------+------------------+\n",
      "\n",
      "üìù OBSERVATIONS:\n",
      "   - Weekday vs weekend patterns: PM2.5 levels tend to be higher on weekdays compared to weekends, likely due to increased traffic and industrial activity.\n",
      "   - Pollution trends: PM2.5 levels are generally higher in the middle of the week (Tuesday-Thursday) and lower on Mondays and Fridays.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Analyze air quality patterns by day of week\n",
    "print(\"\\nüå´Ô∏è Air Quality Patterns by Day of Week:\")\n",
    "try:\n",
    "    # TODO: Extract day of week and analyze PM2.5 levels\n",
    "    air_quality_daily = (air_quality_df\n",
    "                        .withColumn(\"day_of_week\", F.dayofweek(\"timestamp\"))\n",
    "                        .groupBy(\"day_of_week\")\n",
    "                        .agg(F.avg(\"pm25\").alias(\"avg_pm25\"),\n",
    "                             F.avg(\"no2\").alias(\"avg_no2\"))\n",
    "                        .orderBy(\"day_of_week\"))\n",
    "    \n",
    "    # TODO: Show results\n",
    "    air_quality_daily.show()\n",
    "    \n",
    "    # TODO: Add your observations\n",
    "    print(\"üìù OBSERVATIONS:\")\n",
    "    print(\"   - Weekday vs weekend patterns: PM2.5 levels tend to be higher on weekdays compared to weekends, likely due to increased traffic and industrial activity.\")\n",
    "    print(\"   - Pollution trends: PM2.5 levels are generally higher in the middle of the week (Tuesday-Thursday) and lower on Mondays and Fridays.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error analyzing air quality patterns: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 3: BASIC DATA INGESTION (Afternoon - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üì• SECTION 3: DATA INGESTION PIPELINE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üì• SECTION 3: DATA INGESTION PIPELINE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3.1: Create Reusable Data Loading Functions (60 minutes)\n",
    "\n",
    "üéØ **TASK:** Create reusable functions for loading different data formats  \n",
    "üí° **HINT:** Handle schema validation and error handling  \n",
    "üìö **CONCEPTS:** Function design, error handling, schema enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path, expected_schema=None):\n",
    "    \"\"\"\n",
    "    Load CSV data with proper error handling and schema validation\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to CSV file\n",
    "        expected_schema: Optional StructType for schema enforcement\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement CSV loading with options\n",
    "        df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "        # TODO: Add schema validation if provided\n",
    "        if expected_schema:\n",
    "            # Validate schema matches expected\n",
    "            pass\n",
    "            \n",
    "        print(f\"‚úÖ Successfully loaded CSV: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CSV {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    Load JSON data with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement JSON loading\n",
    "        df = spark.read.json(file_path)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded JSON: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading JSON {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_parquet_data(file_path):\n",
    "    \"\"\"\n",
    "    Load Parquet data with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to Parquet file\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement Parquet loading\n",
    "        df = spark.read.parquet(file_path)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded Parquet: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading Parquet {file_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Data Loading Functions:\n",
      "\n",
      "   Testing CSV loader...\n",
      "‚úÖ Successfully loaded CSV: ../data/raw/city_zones.csv\n",
      "      Records loaded: 8\n",
      "\n",
      "   Testing JSON loader...\n",
      "‚úÖ Successfully loaded JSON: ../data/raw/air_quality.json\n",
      "      Records loaded: 161,522\n",
      "\n",
      "   Testing Parquet loader...\n",
      "‚úÖ Successfully loaded Parquet: ../data/raw/weather_data.parquet\n",
      "      Records loaded: 3,370\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test your loading functions\n",
    "print(\"üß™ Testing Data Loading Functions:\")\n",
    "\n",
    "test_files = [\n",
    "    (f\"{data_dir}/city_zones.csv\", \"CSV\", load_csv_data),\n",
    "    (f\"{data_dir}/air_quality.json\", \"JSON\", load_json_data), \n",
    "    (f\"{data_dir}/weather_data.parquet\", \"Parquet\", load_parquet_data)\n",
    "]\n",
    "\n",
    "for file_path, file_type, load_func in test_files:\n",
    "    print(f\"\\n   Testing {file_type} loader...\")\n",
    "    test_df = load_func(file_path)\n",
    "    if test_df:\n",
    "        print(f\"      Records loaded: {test_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3.2: Schema Definition and Enforcement (60 minutes)\n",
    "\n",
    "üéØ **TASK:** Define explicit schemas for data consistency  \n",
    "üí° **HINT:** Use StructType and StructField for schema definition  \n",
    "üìö **CONCEPTS:** Schema design, data types, schema enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# TODO: Define schema for traffic sensors\n",
    "traffic_schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    # TODO: Add remaining fields\n",
    "    StructField(\"vehicle_count\", IntegerType(), False),\n",
    "    StructField(\"avg_speed\", DoubleType(), False),\n",
    "    StructField(\"congestion_level\", StringType(), False),\n",
    "    StructField(\"road_type\", StringType(), False),\n",
    "])\n",
    "\n",
    "# TODO: Define schema for air quality data\n",
    "air_quality_schema = StructType([\n",
    "    # TODO: Define all fields for air quality data\n",
    "    # Hint: Look at the JSON structure and define appropriate types\n",
    "    StructField(\"sensor_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    StructField(\"pm25\", DoubleType(), False),\n",
    "    StructField(\"pm10\", DoubleType(), False),\n",
    "    StructField(\"no2\", DoubleType(), False),\n",
    "    StructField(\"co\", DoubleType(), False),\n",
    "    StructField(\"temperature\", DoubleType(), False),\n",
    "    StructField(\"humidity\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# TODO: Define schema for weather data\n",
    "weather_schema = StructType([\n",
    "    # TODO: Define all fields for weather data\n",
    "    StructField(\"station_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    StructField(\"temperature\", DoubleType(), False),\n",
    "    StructField(\"humidity\", DoubleType(), False),\n",
    "    StructField(\"wind_speed\", DoubleType(), False),\n",
    "    StructField(\"wind_direction\", StringType(), False),\n",
    "    StructField(\"precipitation\", DoubleType(), False),\n",
    "    StructField(\"pressure\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# TODO: Define schema for energy data\n",
    "energy_schema = StructType([\n",
    "    # TODO: Define all fields for energy data\n",
    "    StructField(\"meter_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    StructField(\"energy_consumption_kwh\", DoubleType(), False),\n",
    "    StructField(\"peak_demand_kw\", DoubleType(), False),\n",
    "    StructField(\"voltage\", DoubleType(), False),\n",
    "    StructField(\"current\", DoubleType(), False),\n",
    "    StructField(\"power_factor\", DoubleType(), False),\n",
    "    StructField(\"frequency\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing Schema Enforcement:\n",
      "‚úÖ Schema enforcement successful for ../data/raw/traffic_sensors.csv\n",
      "   Schema enforcement test passed!\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- vehicle_count: integer (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      " |-- congestion_level: string (nullable = true)\n",
      " |-- road_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test schema enforcement\n",
    "print(\"\\nüîç Testing Schema Enforcement:\")\n",
    "\n",
    "def load_with_schema(file_path, schema, file_format=\"csv\"):\n",
    "    \"\"\"Load data with explicit schema enforcement\"\"\"\n",
    "    try:\n",
    "        if file_format == \"csv\":\n",
    "            df = spark.read.schema(schema).option(\"header\", \"true\").csv(file_path)\n",
    "        elif file_format == \"json\":\n",
    "            df = spark.read.schema(schema).json(file_path)\n",
    "        elif file_format == \"parquet\":\n",
    "            df = spark.read.schema(schema).parquet(file_path)\n",
    "        \n",
    "        print(f\"‚úÖ Schema enforcement successful for {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Schema enforcement failed for {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# TODO: Test with one of your schemas\n",
    "test_schema_df = load_with_schema(f\"{data_dir}/traffic_sensors.csv\", traffic_schema, \"csv\", )\n",
    "if test_schema_df:\n",
    "    print(\"   Schema enforcement test passed!\")\n",
    "    test_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 4: INITIAL DATA TRANSFORMATIONS (Afternoon - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîÑ SECTION 4: DATA TRANSFORMATIONS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîÑ SECTION 4: DATA TRANSFORMATIONS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.1: Timestamp Standardization (45 minutes)\n",
    "\n",
    "üéØ **TASK:** Standardize timestamp formats across all datasets  \n",
    "üí° **HINT:** Some datasets may have different timestamp formats  \n",
    "üìö **CONCEPTS:** Date/time handling, format standardization, timezone handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_timestamps(df, timestamp_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Standardize timestamp column across datasets\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        timestamp_col: Name of timestamp column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with standardized timestamps\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Convert timestamps to standard format\n",
    "        standardized_df = (df\n",
    "                          .withColumn(\"timestamp_std\", F.to_timestamp(F.col(timestamp_col)))\n",
    "                          .drop(timestamp_col)\n",
    "                          .withColumnRenamed(\"timestamp_std\", timestamp_col))\n",
    "        \n",
    "        # TODO: Add derived time columns\n",
    "        result_df = (standardized_df\n",
    "                    .withColumn(\"year\", F.year(timestamp_col))\n",
    "                    .withColumn(\"month\", F.month(timestamp_col))\n",
    "                    .withColumn(\"day\", F.dayofmonth(timestamp_col))\n",
    "                    .withColumn(\"hour\", F.hour(timestamp_col))\n",
    "                    .withColumn(\"day_of_week\", F.dayofweek(timestamp_col))\n",
    "                    .withColumn(\"is_weekend\", F.when(F.dayofweek(timestamp_col).isin([1, 7]), True).otherwise(False)))\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error standardizing timestamps: {str(e)}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ Testing Timestamp Standardization:\n",
      "   Traffic data timestamp standardization:\n",
      "+--------------------+----+-----+---+----+-----------+----------+\n",
      "|           timestamp|year|month|day|hour|day_of_week|is_weekend|\n",
      "+--------------------+----+-----+---+----+-----------+----------+\n",
      "|2025-08-29 11:43:...|2025|    8| 29|  11|          6|     false|\n",
      "|2025-08-29 11:43:...|2025|    8| 29|  11|          6|     false|\n",
      "|2025-08-29 11:43:...|2025|    8| 29|  11|          6|     false|\n",
      "|2025-08-29 11:43:...|2025|    8| 29|  11|          6|     false|\n",
      "|2025-08-29 11:43:...|2025|    8| 29|  11|          6|     false|\n",
      "+--------------------+----+-----+---+----+-----------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test timestamp standardization\n",
    "print(\"‚è∞ Testing Timestamp Standardization:\")\n",
    "\n",
    "# Test with traffic data\n",
    "traffic_std = standardize_timestamps(traffic_df)\n",
    "print(\"   Traffic data timestamp standardization:\")\n",
    "traffic_std.select(\"timestamp\", \"year\", \"month\", \"day\", \"hour\", \"day_of_week\", \"is_weekend\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.2: Geographic Zone Mapping (45 minutes)\n",
    "\n",
    "üéØ **TASK:** Map sensor locations to city zones  \n",
    "üí° **HINT:** Join sensor coordinates with zone boundaries  \n",
    "üìö **CONCEPTS:** Spatial joins, geographic data, coordinate systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_zones(sensor_df, zones_df):\n",
    "    \"\"\"\n",
    "    Map sensor locations to city zones\n",
    "    \n",
    "    Args:\n",
    "        sensor_df: DataFrame with sensor locations (lat, lon)\n",
    "        zones_df: DataFrame with zone boundaries\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with zone information added\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Create join condition for geographic mapping\n",
    "        # A sensor is in a zone if its coordinates fall within zone boundaries\n",
    "        join_condition = (\n",
    "            (sensor_df.location_lat >= zones_df.lat_min) &\n",
    "            (sensor_df.location_lat <= zones_df.lat_max) &\n",
    "            (sensor_df.location_lon >= zones_df.lon_min) &\n",
    "            (sensor_df.location_lon <= zones_df.lon_max)\n",
    "        )\n",
    "        \n",
    "        # TODO: Perform the join\n",
    "        result_df = (sensor_df\n",
    "                    .join(zones_df, join_condition, \"left\")\n",
    "                    .select(sensor_df[\"*\"], \n",
    "                           zones_df.zone_id, \n",
    "                           zones_df.zone_name, \n",
    "                           zones_df.zone_type))\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error mapping to zones: {str(e)}\")\n",
    "        return sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üó∫Ô∏è Testing Geographic Zone Mapping:\n",
      "   Traffic sensors with zone mapping:\n",
      "+-----------+------------------+------------------+--------+-----------+\n",
      "|  sensor_id|      location_lat|      location_lon| zone_id|  zone_type|\n",
      "+-----------+------------------+------------------+--------+-----------+\n",
      "|TRAFFIC_001|  40.7901819372493|-73.94717853181487|    NULL|       NULL|\n",
      "|TRAFFIC_002|40.763328789233874|-73.97617748104044|ZONE_006| commercial|\n",
      "|TRAFFIC_003|40.797135706539336|-73.94914250782259|    NULL|       NULL|\n",
      "|TRAFFIC_004| 40.74663304704115| -73.9810073793957|    NULL|       NULL|\n",
      "|TRAFFIC_005|40.792497877302864|-73.98873557827757|ZONE_003|residential|\n",
      "|TRAFFIC_006| 40.70701165750195|-73.90730001839292|    NULL|       NULL|\n",
      "|TRAFFIC_007|40.730020725863056|-73.99769899358863|ZONE_001| commercial|\n",
      "|TRAFFIC_008| 40.75687583890613|-73.96445848415442|    NULL|       NULL|\n",
      "|TRAFFIC_009| 40.75711379282591|-73.92722284344627|    NULL|       NULL|\n",
      "|TRAFFIC_010| 40.78827437477642|-73.91754906513106|    NULL|       NULL|\n",
      "+-----------+------------------+------------------+--------+-----------+\n",
      "only showing top 10 rows\n",
      "   Sensors by zone type:\n",
      "+-----------+-----+\n",
      "|  zone_type|count|\n",
      "+-----------+-----+\n",
      "|       NULL|72612|\n",
      "| commercial|14119|\n",
      "|residential|14119|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test zone mapping\n",
    "print(\"\\nüó∫Ô∏è Testing Geographic Zone Mapping:\")\n",
    "\n",
    "# Test with traffic sensors\n",
    "traffic_with_zones = map_to_zones(traffic_std, zones_df)\n",
    "print(\"   Traffic sensors with zone mapping:\")\n",
    "traffic_with_zones.select(\"sensor_id\", \"location_lat\", \"location_lon\", \"zone_id\", \"zone_type\").show(10)\n",
    "\n",
    "# TODO: Verify mapping worked correctly\n",
    "zone_distribution = traffic_with_zones.groupBy(\"zone_type\").count().orderBy(F.desc(\"count\"))\n",
    "print(\"   Sensors by zone type:\")\n",
    "zone_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.3: Data Type Conversions and Validations (30 minutes)\n",
    "\n",
    "üéØ **TASK:** Ensure proper data types and add validation columns  \n",
    "üí° **HINT:** Cast columns to appropriate types, add data quality flags  \n",
    "üìö **CONCEPTS:** Data type conversion, validation rules, data quality flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_quality_flags(df, sensor_type):\n",
    "    \"\"\"\n",
    "    Add data quality validation flags to DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        sensor_type: Type of sensor for specific validations\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with quality flags added\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result_df = df\n",
    "        \n",
    "        # TODO: Add general quality flags\n",
    "        result_df = result_df.withColumn(\"has_missing_values\", \n",
    "                                        F.when(F.col(\"sensor_id\").isNull(), True).otherwise(False))\n",
    "        \n",
    "        # TODO: Add sensor-specific validations\n",
    "        if sensor_type == \"traffic\":\n",
    "            # Traffic-specific validations\n",
    "            result_df = (result_df\n",
    "                        .withColumn(\"valid_speed\", \n",
    "                                   F.when((F.col(\"avg_speed\") >= 0) & (F.col(\"avg_speed\") <= 100), True).otherwise(False))\n",
    "                        .withColumn(\"valid_vehicle_count\",\n",
    "                                   F.when(F.col(\"vehicle_count\") >= 0, True).otherwise(False)))\n",
    "        \n",
    "        elif sensor_type == \"air_quality\":\n",
    "            # Air quality specific validations\n",
    "            result_df = (result_df\n",
    "                        .withColumn(\"valid_pm25\",\n",
    "                                   F.when((F.col(\"pm25\") >= 0) & (F.col(\"pm25\") <= 500), True).otherwise(False))\n",
    "                        .withColumn(\"valid_temperature\",\n",
    "                                   F.when((F.col(\"temperature\") >= -50) & (F.col(\"temperature\") <= 50), True).otherwise(False)))\n",
    "        \n",
    "        # TODO: Add more sensor-specific validations\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding quality flags: {str(e)}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè∑Ô∏è Testing Data Quality Flags:\n",
      "   Traffic data with quality flags:\n",
      "+-----------+------------------+-------------+-----------+-------------------+\n",
      "|  sensor_id|         avg_speed|vehicle_count|valid_speed|valid_vehicle_count|\n",
      "+-----------+------------------+-------------+-----------+-------------------+\n",
      "|TRAFFIC_001| 65.80749574895847|            3|       true|               true|\n",
      "|TRAFFIC_002| 41.02141713733583|           39|       true|               true|\n",
      "|TRAFFIC_003| 45.59579718445981|           24|       true|               true|\n",
      "|TRAFFIC_004| 67.94499054227262|           15|       true|               true|\n",
      "|TRAFFIC_005|62.170026447114694|           23|       true|               true|\n",
      "|TRAFFIC_006|36.548574796347616|           19|       true|               true|\n",
      "|TRAFFIC_007| 57.56578133528534|           23|       true|               true|\n",
      "|TRAFFIC_008| 73.06058671785361|           18|       true|               true|\n",
      "|TRAFFIC_009|               5.0|            1|       true|               true|\n",
      "|TRAFFIC_010| 61.75484587039908|            5|       true|               true|\n",
      "+-----------+------------------+-------------+-----------+-------------------+\n",
      "only showing top 10 rows\n",
      "   Quality statistics:\n",
      "+-----------------+-------------------------+-------------+\n",
      "|valid_speed_count|valid_vehicle_count_count|total_records|\n",
      "+-----------------+-------------------------+-------------+\n",
      "|           100817|                   100850|       100850|\n",
      "+-----------------+-------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test data quality flags\n",
    "print(\"\\nüè∑Ô∏è Testing Data Quality Flags:\")\n",
    "\n",
    "# Test with traffic data\n",
    "traffic_with_flags = add_data_quality_flags(traffic_with_zones, \"traffic\")\n",
    "print(\"   Traffic data with quality flags:\")\n",
    "traffic_with_flags.select(\"sensor_id\", \"avg_speed\", \"vehicle_count\", \"valid_speed\", \"valid_vehicle_count\").show(10)\n",
    "\n",
    "# TODO: Check quality flag distribution\n",
    "quality_stats = (traffic_with_flags\n",
    "                .agg(F.sum(F.when(F.col(\"valid_speed\"), 1).otherwise(0)).alias(\"valid_speed_count\"),\n",
    "                     F.sum(F.when(F.col(\"valid_vehicle_count\"), 1).otherwise(0)).alias(\"valid_vehicle_count_count\"),\n",
    "                     F.count(\"*\").alias(\"total_records\")))\n",
    "\n",
    "print(\"   Quality statistics:\")\n",
    "quality_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# DAY 1 DELIVERABLES & CHECKPOINTS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìã DAY 1 COMPLETION CHECKLIST\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã DAY 1 COMPLETION CHECKLIST\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ COMPLETION STATUS:\n",
      "   ‚úÖ Spark Session Created\n",
      "   ‚úÖ Database Connection Tested\n",
      "   ‚úÖ Data Loaded Successfully\n",
      "   ‚úÖ Data Quality Assessed\n",
      "   ‚úÖ Loading Functions Created\n",
      "   ‚úÖ Schemas Defined\n",
      "   ‚úÖ Timestamp Standardization Working\n",
      "   ‚úÖ Zone Mapping Implemented\n",
      "   ‚úÖ Quality Flags Added\n",
      "\n",
      "üìä Overall Completion: 100.0%\n",
      "üéâ Great job! You're ready for Day 2!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Complete this checklist by running the validation functions\n",
    "\n",
    "def validate_day1_completion():\n",
    "    \"\"\"Validate that Day 1 objectives have been met\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"spark_session_created\": False,\n",
    "        \"database_connection_tested\": False,\n",
    "        \"data_loaded_successfully\": False,\n",
    "        \"data_quality_assessed\": False,\n",
    "        \"loading_functions_created\": False,\n",
    "        \"schemas_defined\": False,\n",
    "        \"timestamp_standardization_working\": False,\n",
    "        \"zone_mapping_implemented\": False,\n",
    "        \"quality_flags_added\": False\n",
    "    }\n",
    "    \n",
    "    # TODO: Add validation logic for each item\n",
    "    try:\n",
    "        # Check Spark session\n",
    "        if spark and spark.sparkContext._jsc:\n",
    "            checklist[\"spark_session_created\"] = True\n",
    "            \n",
    "        # Check if data exists\n",
    "        if ('traffic_df' in globals() and traffic_df.count() > 0) and ('weather_df' in globals() and weather_df.count() > 0) and ('air_quality_df' in globals() and air_quality_df is not None and air_quality_df.count() > 0) and ('energy_df' in globals() and energy_df.count() > 0) and ('zones_df' in globals() and zones_df.count() > 0):\n",
    "            checklist[\"data_loaded_successfully\"] = True\n",
    "            \n",
    "        # TODO: Add more validation checks\n",
    "        if 'db_connected' in globals() and db_connected:\n",
    "            checklist[\"database_connection_tested\"] = True\n",
    "\n",
    "        if  \"assess_data_quality\" in globals():\n",
    "            checklist[\"data_quality_assessed\"] = True\n",
    "        \n",
    "        loading_functions = ['load_csv_data', 'load_json_data', 'load_parquet_data']\n",
    "        if all(func in globals() for func in loading_functions):\n",
    "            checklist[\"loading_functions_created\"] = True\n",
    "        \n",
    "        schema_vars = ['traffic_schema', 'air_quality_schema', 'weather_schema', 'energy_schema']\n",
    "        if all(schema in globals() for schema in schema_vars):\n",
    "            checklist[\"schemas_defined\"] = True\n",
    "        \n",
    "        if 'standardize_timestamps' in globals() and 'traffic_std' in globals():\n",
    "            checklist[\"timestamp_standardization_working\"] = True\n",
    "        \n",
    "        if 'map_to_zones' in globals() and 'traffic_with_zones' in globals():\n",
    "            checklist[\"zone_mapping_implemented\"] = True\n",
    "         \n",
    "        if 'add_data_quality_flags' in globals() and 'traffic_with_flags' in globals():\n",
    "            checklist[\"quality_flags_added\"] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation error: {str(e)}\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"‚úÖ COMPLETION STATUS:\")\n",
    "    for item, status in checklist.items():\n",
    "        status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "        print(f\"   {status_icon} {item.replace('_', ' ').title()}\")\n",
    "    \n",
    "    import builtins\n",
    "    completion_rate = builtins.sum(checklist.values()) / len(checklist) * 100\n",
    "    print(f\"\\nüìä Overall Completion: {completion_rate:.1f}%\")\n",
    "    \n",
    "    if completion_rate >= 80:\n",
    "        print(\"üéâ Great job! You're ready for Day 2!\")\n",
    "    else:\n",
    "        print(\"üìù Please review incomplete items before proceeding to Day 2.\")\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# TODO: Run the validation\n",
    "completion_status = validate_day1_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ WHAT'S NEXT?\n",
    "\n",
    "---\n",
    "\n",
    "## üìÖ DAY 2 PREVIEW: Data Quality & Cleaning Pipeline\n",
    "\n",
    "Tomorrow you'll work on:\n",
    "1. üîç Comprehensive data quality assessment\n",
    "2. üßπ Advanced cleaning procedures for IoT sensor data  \n",
    "3. üìä Missing data handling and interpolation strategies\n",
    "4. üö® Outlier detection and treatment methods\n",
    "5. üìè Data standardization and normalization\n",
    "\n",
    "## üìö RECOMMENDED PREPARATION:\n",
    "- Review PySpark DataFrame operations\n",
    "- Read about time series data quality challenges\n",
    "- Familiarize yourself with statistical outlier detection methods\n",
    "\n",
    "## üíæ SAVE YOUR WORK:\n",
    "- Commit your notebook to Git\n",
    "- Document any issues or questions for tomorrow\n",
    "- Save any custom functions you created\n",
    "\n",
    "## ü§ù QUESTIONS?\n",
    "- Post in the class discussion forum\n",
    "- Review Spark documentation for any unclear concepts\n",
    "- Prepare questions for tomorrow's Q&A session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Don't forget to save your notebook and commit your changes!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Save your progress\n",
    "print(\"\\nüíæ Don't forget to save your notebook and commit your changes!\")\n",
    "\n",
    "# Clean up (optional)\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
