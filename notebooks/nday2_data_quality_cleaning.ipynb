{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72b2eb0",
   "metadata": {},
   "source": [
    "# Day 2: Data Quality & Cleaning Pipeline\n",
    "# Smart City IoT Analytics Pipeline\n",
    "\n",
    "\"\"\"\n",
    "üéØ LEARNING OBJECTIVES:\n",
    "- Implement comprehensive data quality assessment\n",
    "- Design cleaning procedures for IoT sensor data\n",
    "- Handle missing values and outliers appropriately\n",
    "- Create reusable data quality functions\n",
    "\n",
    "üìÖ SCHEDULE:\n",
    "Morning (4 hours):\n",
    "1. Data Quality Assessment (2 hours)\n",
    "2. Missing Data Strategy (2 hours)\n",
    "\n",
    "Afternoon (4 hours):\n",
    "3. Outlier Detection & Treatment (2 hours)\n",
    "4. Data Standardization (2 hours)\n",
    "\n",
    "‚úÖ DELIVERABLES:\n",
    "- Data quality assessment report\n",
    "- Comprehensive cleaning pipeline\n",
    "- Outlier detection and treatment functions\n",
    "- Standardized datasets ready for analysis\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337c103",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79adbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Machine learning imports (for outlier detection)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49d73575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using existing Spark session\n",
      "üîß Day 2: Data Quality & Cleaning Pipeline\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session if not already defined\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if 'spark' not in globals():\n",
    "\tspark = SparkSession.builder.appName(\"SmartCityIoT\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"‚úÖ Using existing Spark session\")\n",
    "\n",
    "print(\"üîß Day 2: Data Quality & Cleaning Pipeline\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e4c99",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 1: COMPREHENSIVE DATA PROFILING (Morning - 2 hours)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8d4d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä SECTION 1: COMPREHENSIVE DATA PROFILING\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä SECTION 1: COMPREHENSIVE DATA PROFILING\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0657fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All datasets loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data from Day 1 (or reload if needed)\n",
    "data_dir = \"../data/raw\"\n",
    "\n",
    "# Ensure Spark session is initialized\n",
    "if 'spark' not in globals():\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(\"SmartCityIoT\").getOrCreate()\n",
    "\n",
    "# TODO 1.1: Load all datasets with error handling\n",
    "def load_all_datasets():\n",
    "    \"\"\"Load all sensor datasets with consistent error handling\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    try:\n",
    "        # Load each dataset\n",
    "        datasets['zones'] = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/city_zones.csv\")\n",
    "        datasets['traffic'] = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/traffic_sensors.csv\")\n",
    "        datasets['air_quality'] = spark.read.option(\"multiline\", \"true\").json(f\"{data_dir}/air_quality.json\")\n",
    "        datasets['weather'] = spark.read.parquet(f\"{data_dir}/weather_data.parquet\")\n",
    "        datasets['energy'] = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/energy_meters.csv\")\n",
    "        \n",
    "        # Convert timestamp columns to proper format\n",
    "        for name, df in datasets.items():\n",
    "            if name != 'zones' and 'timestamp' in df.columns:\n",
    "                datasets[name] = df.withColumn(\"timestamp\", F.to_timestamp(F.col(\"timestamp\")))\n",
    "        \n",
    "        print(\"‚úÖ All datasets loaded successfully\")\n",
    "        return datasets\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading datasets: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "datasets = load_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16fff8b",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 1.2: Advanced Data Quality Metrics (45 minutes)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49945abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting comprehensive data profiling...\n",
      "\n",
      "üîç Comprehensive Profile: zones\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "üìà Numeric Column Analysis:\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|summary|             lat_min|             lat_max|            lon_min|            lon_max|        population|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|  count|                   8|                   8|                  8|                  8|                 8|\n",
      "|   mean|  40.730000000000004|             40.7525| -73.99125000000001|          -73.97125|           21250.0|\n",
      "| stddev|0.023904572186687328|0.028157719063465373|0.02474873734153055|0.02474873734153458|14260.334598358582|\n",
      "|    min|                40.7|               40.72|             -74.02|              -74.0|              5000|\n",
      "|    max|               40.76|                40.8|             -73.96|             -73.94|             45000|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "\n",
      "\n",
      "üîç Comprehensive Profile: traffic\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "‚è∞ Temporal Analysis:\n",
      "   Time Range: 2025-08-29 14:28:16.911911 to 2025-09-05 14:28:16.911911\n",
      "   Records with timestamps: 100,850\n",
      "   Expected interval: 5 minutes\n",
      "üìÖ Data Freshness: Latest record is 0.7 hours old\n",
      "üìà Numeric Column Analysis:\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|summary|        location_lat|       location_lon|     vehicle_count|         avg_speed|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|  count|              100850|             100850|            100850|            100850|\n",
      "|   mean|   40.75320834101112|  -73.9619793834017|19.323073872087257| 45.61919472407045|\n",
      "| stddev|0.029220082523713303|0.03726541447320453|11.084167318965944|   16.999185956333|\n",
      "|    min|   40.70434848830766| -74.01912928744918|                 0|               5.0|\n",
      "|    max|   40.79844616789913| -73.90076573245757|                93|120.03978323964058|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "\n",
      "üìÇ Categorical Column Analysis:\n",
      "   sensor_id: 50 distinct values\n",
      "   congestion_level: 3 distinct values\n",
      "      Top values:\n",
      "+----------------+-----+\n",
      "|congestion_level|count|\n",
      "+----------------+-----+\n",
      "|medium          |63264|\n",
      "|low             |20680|\n",
      "|high            |16906|\n",
      "+----------------+-----+\n",
      "\n",
      "   road_type: 4 distinct values\n",
      "      Top values:\n",
      "+-----------+-----+\n",
      "|road_type  |count|\n",
      "+-----------+-----+\n",
      "|residential|34289|\n",
      "|commercial |30255|\n",
      "|arterial   |24204|\n",
      "|highway    |12102|\n",
      "+-----------+-----+\n",
      "\n",
      "\n",
      "üîç Comprehensive Profile: air_quality\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "‚è∞ Temporal Analysis:\n",
      "   Time Range: 2025-08-29 14:28:16.911911 to 2025-09-05 14:28:16.911911\n",
      "   Records with timestamps: 13,460\n",
      "   Expected interval: 15 minutes\n",
      "üìÖ Data Freshness: Latest record is 0.7 hours old\n",
      "üìà Numeric Column Analysis:\n",
      "+-------+-------------------+------------------+--------------------+--------------------+------------------+------------------+------------------+-------------------+\n",
      "|summary|                 co|          humidity|        location_lat|        location_lon|               no2|              pm10|              pm25|        temperature|\n",
      "+-------+-------------------+------------------+--------------------+--------------------+------------------+------------------+------------------+-------------------+\n",
      "|  count|              13460|             13460|               13460|               13460|             13460|             13460|             13460|              13460|\n",
      "|   mean| 1.2900736929757035| 55.12699975827585|   40.75212350523285|  -73.96167447242394|32.183299753390116| 43.05733234826362|26.758635350550588| 19.963971151571318|\n",
      "| stddev|0.42741224626476826|14.457166404487968|0.028832645097996296|0.032623345658152135|10.651162949737675|13.107351206196094| 8.604607204447301|  7.985421215787184|\n",
      "|    min|                0.0|30.004764392258206|   40.70193711260887|  -74.01724969964505|               0.0|               0.0|               0.0|-7.3082572574924605|\n",
      "|    max| 2.8313115389647057| 79.99906759126318|    40.7943421280996|  -73.90616890178737| 70.54844740749834| 98.37707888619828| 61.12616498619434|  48.57435492833947|\n",
      "+-------+-------------------+------------------+--------------------+--------------------+------------------+------------------+------------------+-------------------+\n",
      "\n",
      "üìÇ Categorical Column Analysis:\n",
      "   sensor_id: 20 distinct values\n",
      "\n",
      "üîç Comprehensive Profile: weather\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "‚è∞ Temporal Analysis:\n",
      "   Time Range: 2025-08-29 14:28:16.911911 to 2025-09-05 14:28:16.911911\n",
      "   Records with timestamps: 3,370\n",
      "   Expected interval: 30 minutes\n",
      "üìÖ Data Freshness: Latest record is 0.7 hours old\n",
      "üìà Numeric Column Analysis:\n",
      "+-------+--------------------+--------------------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|summary|        location_lat|        location_lon|       temperature|          humidity|          wind_speed|      wind_direction|       precipitation|          pressure|\n",
      "+-------+--------------------+--------------------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|  count|                3370|                3370|              3370|              3370|                3370|                3370|                3370|              3370|\n",
      "|   mean|   40.75014106645451|  -73.95257877727384|19.953453533395002| 59.99762088423202|   8.195464160056657|   179.8723803318062|0.047221526808522034|1013.5212283021946|\n",
      "| stddev|0.031073801012479887|0.040804861125973056|3.1096480471892236|14.882461981952115|   8.242706989474538|  104.96954725590521| 0.19873646811945375| 9.975738160979306|\n",
      "|    min|   40.70475547575634|  -74.01210188291412| 9.552070959483915|              20.0|0.003174538275455578|0.026942148757020945|                 0.0| 979.4041564357088|\n",
      "|    max|   40.79786969244136|  -73.90292568066448| 30.53243894610191|             100.0|   76.12446365039241|  359.91711122399636|  2.5770781203310897|1042.1290003725026|\n",
      "+-------+--------------------+--------------------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n",
      "\n",
      "üìÇ Categorical Column Analysis:\n",
      "   station_id: 10 distinct values\n",
      "      Top values:\n",
      "+-----------+-----+\n",
      "|station_id |count|\n",
      "+-----------+-----+\n",
      "|WEATHER_002|337  |\n",
      "|WEATHER_004|337  |\n",
      "|WEATHER_006|337  |\n",
      "|WEATHER_005|337  |\n",
      "|WEATHER_001|337  |\n",
      "+-----------+-----+\n",
      "\n",
      "\n",
      "üîç Comprehensive Profile: energy\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "‚è∞ Temporal Analysis:\n",
      "   Time Range: 2025-08-29 14:28:16.911911 to 2025-09-05 14:28:16.911911\n",
      "   Records with timestamps: 201,800\n",
      "   Expected interval: 10 minutes\n",
      "üìÖ Data Freshness: Latest record is 0.7 hours old\n",
      "üìà Numeric Column Analysis:\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|summary|       location_lat|       location_lon| power_consumption|           voltage|           current|       power_factor|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  count|             201800|             201800|            201800|            201800|            201800|             201800|\n",
      "|   mean| 40.753671432932336| -73.95888884925883|17.615469968232883|239.98895420240856| 73.42992249256345|  0.900081788593561|\n",
      "| stddev|0.03048470866948471|0.03460479605863812| 18.48958803528234| 4.996874527970554| 77.10308873337803|0.02893472109691345|\n",
      "|    min| 40.701192552768944|  -74.0199693790321|1.6800314936493617|217.27770649886753| 6.533669650468592|  0.850000466841164|\n",
      "|    max|  40.79993092795841| -73.90033044349764| 64.99893837986431|262.09653510883226|288.31868201812426| 0.9499995735729162|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n",
      "üìÇ Categorical Column Analysis:\n",
      "   meter_id: 200 distinct values\n",
      "   building_type: 5 distinct values\n",
      "      Top values:\n",
      "+-------------+-----+\n",
      "|building_type|count|\n",
      "+-------------+-----+\n",
      "|residential  |45405|\n",
      "|retail       |44396|\n",
      "|office       |41369|\n",
      "|commercial   |36324|\n",
      "|industrial   |34306|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "üéØ TASK: Create comprehensive data quality profiling functions\n",
    "üí° HINT: Look beyond basic missing values - consider temporal patterns, distributions\n",
    "üìö CONCEPTS: Data profiling, quality metrics, statistical validation\n",
    "\"\"\"\n",
    "\n",
    "def comprehensive_data_profile(df, dataset_name, time_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive data quality profile\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to profile\n",
    "        dataset_name: Name for reporting\n",
    "        time_col: Timestamp column name (can be None for non-time series data)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with quality metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Comprehensive Profile: {dataset_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_rows = df.count()\n",
    "    total_cols = len(df.columns)\n",
    "    \n",
    "    profile = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'total_rows': total_rows,\n",
    "        'total_columns': total_cols,\n",
    "        'memory_usage_mb': 0,  # Estimate\n",
    "        'quality_issues': []\n",
    "    }\n",
    "    \n",
    "    # TODO: Calculate missing value patterns\n",
    "    print(\"üìã Missing Value Analysis:\")\n",
    "    missing_analysis = {}\n",
    "    for col in df.columns:\n",
    "        missing_count = df.filter(F.col(col).isNull()).count()\n",
    "        missing_pct = (missing_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "        missing_analysis[col] = {'count': missing_count, 'percentage': missing_pct}\n",
    "        \n",
    "        if missing_pct > 5:  # Flag columns with >5% missing\n",
    "            profile['quality_issues'].append(f\"High missing values in {col}: {missing_pct:.2f}%\")\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            print(f\"   {col}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    profile['missing_analysis'] = missing_analysis\n",
    "    \n",
    "    # TODO: Temporal data gaps (if timestamp column exists and is not None)\n",
    "    if time_col and time_col in df.columns:\n",
    "        print(\"‚è∞ Temporal Analysis:\")\n",
    "        \n",
    "        # Get time range\n",
    "        time_stats = df.agg(\n",
    "            F.min(time_col).alias('min_time'),\n",
    "            F.max(time_col).alias('max_time'),\n",
    "            F.count(time_col).alias('time_count')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"   Time Range: {time_stats['min_time']} to {time_stats['max_time']}\")\n",
    "        print(f\"   Records with timestamps: {time_stats['time_count']:,}\")\n",
    "        \n",
    "        # TODO: Check for temporal gaps\n",
    "        # Calculate expected vs actual record counts\n",
    "        if dataset_name == 'traffic':\n",
    "            expected_interval_minutes = 5\n",
    "        elif dataset_name == 'air_quality':\n",
    "            expected_interval_minutes = 15\n",
    "        elif dataset_name == 'weather':\n",
    "            expected_interval_minutes = 30\n",
    "        elif dataset_name == 'energy':\n",
    "            expected_interval_minutes = 10\n",
    "        else:\n",
    "            expected_interval_minutes = 15\n",
    "        \n",
    "        print(f\"   Expected interval: {expected_interval_minutes} minutes\")\n",
    "        \n",
    "        # TODO: Data freshness (for time series data)\n",
    "        latest_record = df.agg(F.max(time_col).alias('latest')).collect()[0]['latest']\n",
    "        if latest_record:\n",
    "            from datetime import datetime\n",
    "            hours_old = (datetime.now() - latest_record).total_seconds() / 3600\n",
    "            print(f\"üìÖ Data Freshness: Latest record is {hours_old:.1f} hours old\")\n",
    "    \n",
    "    # TODO: Numeric column distributions\n",
    "    numeric_cols = [field.name for field in df.schema.fields \n",
    "                   if field.dataType in [IntegerType(), DoubleType(), FloatType(), LongType()]]\n",
    "    \n",
    "    if numeric_cols:\n",
    "        print(\"üìà Numeric Column Analysis:\")\n",
    "        # Get basic statistics for numeric columns\n",
    "        stats_df = df.select(numeric_cols).describe()\n",
    "        stats_df.show()\n",
    "        \n",
    "        # TODO: Check for suspicious patterns in numeric data\n",
    "        for col in numeric_cols:\n",
    "            if col not in ['location_lat', 'location_lon']:\n",
    "                # Check for columns with very low variance (potentially stuck sensors)\n",
    "                variance_check = df.agg(F.variance(col).alias('variance')).collect()[0]['variance']\n",
    "                if variance_check is not None and variance_check < 0.001:\n",
    "                    profile['quality_issues'].append(f\"Very low variance in {col}: {variance_check}\")\n",
    "    \n",
    "    # TODO: Categorical column analysis\n",
    "    categorical_cols = [field.name for field in df.schema.fields \n",
    "                       if field.dataType == StringType() and field.name not in [time_col] if time_col]\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(\"üìÇ Categorical Column Analysis:\")\n",
    "        for col in categorical_cols:\n",
    "            distinct_count = df.select(col).distinct().count()\n",
    "            print(f\"   {col}: {distinct_count} distinct values\")\n",
    "            \n",
    "            # Show top values\n",
    "            if distinct_count < 20:\n",
    "                top_values = df.groupBy(col).count().orderBy(F.desc(\"count\")).limit(5)\n",
    "                print(f\"      Top values:\")\n",
    "                top_values.show(5, truncate=False)\n",
    "    \n",
    "    # TODO: Check for duplicate records\n",
    "    duplicate_count = total_rows - df.dropDuplicates().count()\n",
    "    if duplicate_count > 0:\n",
    "        profile['quality_issues'].append(f\"Duplicate records found: {duplicate_count}\")\n",
    "        print(f\"üîÑ Duplicate Records: {duplicate_count:,}\")\n",
    "    \n",
    "    return profile\n",
    "\n",
    "      \n",
    "\n",
    "# TODO: Profile all datasets\n",
    "print(\"üîç Starting comprehensive data profiling...\")\n",
    "\n",
    "profiles = {}\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        try:\n",
    "            # Check if the dataset has a timestamp column\n",
    "            if name != 'zones' and 'timestamp' in df.columns:\n",
    "                profiles[name] = comprehensive_data_profile(df, name, time_col=\"timestamp\")\n",
    "            else:\n",
    "                # For datasets without timestamp column (like zones), don't pass time_col\n",
    "                profiles[name] = comprehensive_data_profile(df, name, time_col=None)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error profiling {name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d6f93",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 1.3: Sensor Health Analysis (45 minutes)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "473a8028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè• Analyzing sensor health across all datasets...\n",
      "\n",
      "üè• Sensor Health Analysis\n",
      "------------------------------\n",
      "üöó Traffic Sensor Health Summary:\n",
      "+-------+-----+\n",
      "| status|count|\n",
      "+-------+-----+\n",
      "|healthy|   50|\n",
      "+-------+-----+\n",
      "\n",
      "\n",
      "üè• Sensor Health Analysis\n",
      "------------------------------\n",
      "üå´Ô∏è Air Quality Sensor Health Summary:\n",
      "+-------+-----+\n",
      "| status|count|\n",
      "+-------+-----+\n",
      "|healthy|   20|\n",
      "+-------+-----+\n",
      "\n",
      "\n",
      "üè• Sensor Health Analysis\n",
      "------------------------------\n",
      "üå¶Ô∏è Weather Sensor Health Summary:\n",
      "+-------+-----+\n",
      "| status|count|\n",
      "+-------+-----+\n",
      "|healthy|   10|\n",
      "+-------+-----+\n",
      "\n",
      "\n",
      "üè• Sensor Health Analysis\n",
      "------------------------------\n",
      "‚ö° Energy Sensor Health Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2416:>                                                       (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| status|count|\n",
      "+-------+-----+\n",
      "|healthy|  200|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import necessary PySpark functions\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\"\"\"\n",
    "üéØ TASK: Identify sensors with potential operational issues\n",
    "üí° HINT: Look for patterns that indicate sensor malfunctions\n",
    "üìö CONCEPTS: Sensor diagnostics, operational monitoring, health scoring\n",
    "\"\"\"\n",
    "\n",
    "def analyze_sensor_health(df, sensor_id_col, value_cols, time_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Analyze individual sensor health and identify problematic sensors\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with sensor data\n",
    "        sensor_id_col: Column name for sensor ID\n",
    "        value_cols: List of measurement columns to analyze\n",
    "        time_col: Timestamp column\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with sensor health metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nüè• Sensor Health Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # TODO: Calculate health metrics per sensor\n",
    "    window_spec = Window.partitionBy(sensor_id_col)\n",
    "    \n",
    "    health_metrics = df.groupBy(sensor_id_col).agg(\n",
    "        F.count(\"*\").alias(\"total_readings\"),\n",
    "        F.min(time_col).alias(\"first_reading\"),\n",
    "        F.max(time_col).alias(\"last_reading\")\n",
    "    )\n",
    "    \n",
    "    # TODO: Add missing data percentage per sensor\n",
    "    for col in value_cols:\n",
    "        missing_col_name = f\"{col}_missing_pct\"\n",
    "        \n",
    "        # Calculate missing percentage per sensor using separate aggregation\n",
    "        missing_stats = df.groupBy(sensor_id_col).agg(\n",
    "            F.count(\"*\").alias(\"total_count\"),\n",
    "            F.sum(F.when(F.col(col).isNull(), 1).otherwise(0)).alias(\"missing_count\")\n",
    "        ).withColumn(\n",
    "            missing_col_name,\n",
    "            (F.col(\"missing_count\") / F.col(\"total_count\") * 100).cast(\"double\")\n",
    "        ).select(sensor_id_col, missing_col_name)\n",
    "        \n",
    "        # Join the missing percentage back to health_metrics\n",
    "        health_metrics = health_metrics.join(missing_stats, sensor_id_col, \"left\")\n",
    "\n",
    "    # TODO: Add variance analysis (detect stuck sensors)\n",
    "    for col in value_cols:\n",
    "        if col in df.columns:\n",
    "            variance_col_name = f\"{col}_variance\"\n",
    "            sensor_variance = df.groupBy(sensor_id_col).agg(\n",
    "                F.variance(col).alias(variance_col_name)\n",
    "            )\n",
    "            health_metrics = health_metrics.join(sensor_variance, sensor_id_col, \"left\")\n",
    "    \n",
    "    # TODO: Calculate data gaps (irregular reporting)\n",
    "    # This is more complex - calculate time differences between consecutive readings\n",
    "    if time_col in df.columns:\n",
    "        time_diff_col = \"time_diff_minutes\"\n",
    "        df_with_lag = df.withColumn(\n",
    "            \"prev_time\",\n",
    "            F.lag(time_col).over(Window.partitionBy(sensor_id_col).orderBy(time_col))\n",
    "        ).withColumn(\n",
    "            time_diff_col,\n",
    "            (F.unix_timestamp(time_col) - F.unix_timestamp(\"prev_time\")) / 60\n",
    "        )\n",
    "        \n",
    "        avg_time_diff = df_with_lag.groupBy(sensor_id_col).agg(\n",
    "            F.avg(time_diff_col).alias(\"avg_time_diff_minutes\"),\n",
    "            F.max(time_diff_col).alias(\"max_time_diff_minutes\")\n",
    "        )\n",
    "        \n",
    "        health_metrics = health_metrics.join(avg_time_diff, sensor_id_col, \"left\")\n",
    "    \n",
    "    # TODO: Create overall health score\n",
    "    # Combine multiple factors into a single health score (0-100)\n",
    "    first_value_col = value_cols[0] if value_cols else \"vehicle_count\"\n",
    "    missing_pct_col = f\"{first_value_col}_missing_pct\"\n",
    "    variance_col = f\"{first_value_col}_variance\"\n",
    "    \n",
    "    health_metrics = health_metrics.withColumn(\n",
    "        \"health_score\",\n",
    "        # TODO: Create a formula that combines:\n",
    "        # - Missing data percentage (lower is better)\n",
    "        # - Data variance (too low = stuck sensor, too high = noisy sensor)\n",
    "        # - Reporting regularity\n",
    "        # - Recent data availability\n",
    "        F.greatest(\n",
    "            F.lit(0.0),\n",
    "            F.least(\n",
    "                F.lit(100.0),\n",
    "                # Start with 100 and subtract penalties\n",
    "                F.lit(100.0) -\n",
    "                # Penalty for missing data (0-40 points lost)\n",
    "                F.coalesce(F.col(missing_pct_col), F.lit(0.0)) * 0.4 -\n",
    "                # Penalty for irregular reporting (0-30 points lost)\n",
    "                F.when(F.col(\"avg_time_diff_minutes\").isNull(), F.lit(0.0))\n",
    "                 .when(F.col(\"avg_time_diff_minutes\") > 60, F.lit(30.0))  # >1 hour gaps\n",
    "                 .when(F.col(\"avg_time_diff_minutes\") > 30, F.lit(15.0))  # >30 min gaps\n",
    "                 .when(F.col(\"avg_time_diff_minutes\") > 15, F.lit(5.0))   # >15 min gaps\n",
    "                 .otherwise(F.lit(0.0)) -\n",
    "                # Penalty for very low variance (stuck sensors) (0-20 points lost)\n",
    "                F.when(F.coalesce(F.col(variance_col), F.lit(1.0)) < 0.001, F.lit(20.0))\n",
    "                 .otherwise(F.lit(0.0)) -\n",
    "                # Penalty for very high variance (noisy sensors) (0-10 points lost)\n",
    "                F.when(F.coalesce(F.col(variance_col), F.lit(0.0)) > 1000, F.lit(10.0))\n",
    "                 .otherwise(F.lit(0.0))\n",
    "            )\n",
    "        ).cast(\"double\")\n",
    "        \n",
    "    )\n",
    "    \n",
    "    # TODO: Flag problematic sensors\n",
    "    health_metrics = health_metrics.withColumn(\n",
    "        \"status\",\n",
    "        F.when(F.col(\"health_score\") > 80, \"healthy\")\n",
    "         .when(F.col(\"health_score\") > 60, \"warning\")\n",
    "         .otherwise(\"critical\")\n",
    "    )\n",
    "    \n",
    "    return health_metrics\n",
    "\n",
    "# Ensure 'profiles' is defined before proceeding\n",
    "if 'profiles' not in locals():\n",
    "    raise ValueError(\"profiles is not defined. Please define the 'profiles' variable (dataset metadata) before running this cell.\")\n",
    "\n",
    "# Load datasets if not already loaded\n",
    "if 'datasets' not in locals():\n",
    "    datasets = {}\n",
    "    for name, meta in profiles.items():\n",
    "        try:\n",
    "            datasets[name] = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"{data_dir}/{name}.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load dataset '{name}': {e}\")\n",
    "\n",
    "# TODO: Analyze health for each sensor type\n",
    "print(\"üè• Analyzing sensor health across all datasets...\")\n",
    "\n",
    "sensor_health_results = {}\n",
    "\n",
    "# Traffic sensors\n",
    "if 'traffic' in datasets:\n",
    "    traffic_health = analyze_sensor_health(\n",
    "        datasets['traffic'], \n",
    "        'sensor_id', \n",
    "        ['vehicle_count', 'avg_speed']\n",
    "    )\n",
    "    sensor_health_results['traffic'] = traffic_health\n",
    "    \n",
    "    print(\"üöó Traffic Sensor Health Summary:\")\n",
    "    traffic_health.groupBy(\"status\").count().show()\n",
    "\n",
    "# TODO: Analyze other sensor types\n",
    "# Air quality sensors\n",
    "if 'air_quality' in datasets:\n",
    "    # TODO: Implement air quality sensor health analysis\n",
    "    air_quality_health=analyze_sensor_health(\n",
    "        datasets['air_quality'], \n",
    "        'sensor_id', \n",
    "        ['pm25', 'pm10', 'no2', 'co']\n",
    "    )\n",
    "    sensor_health_results['air_quality'] = air_quality_health\n",
    "    print(\"üå´Ô∏è Air Quality Sensor Health Summary:\")\n",
    "    air_quality_health.groupBy(\"status\").count().show()\n",
    "\n",
    "# Weather sensors  \n",
    "if 'weather' in datasets:\n",
    "    # TODO: Implement weather sensor health analysis\n",
    "    weather_health=analyze_sensor_health(\n",
    "        datasets['weather'], \n",
    "        'station_id', \n",
    "        ['temperature', 'humidity', 'wind_speed', 'precipitation']\n",
    "    )\n",
    "    sensor_health_results['weather'] = weather_health\n",
    "    print(\"üå¶Ô∏è Weather Sensor Health Summary:\")\n",
    "    weather_health.groupBy(\"status\").count().show()\n",
    "\n",
    "# Energy sensors\n",
    "if 'energy' in datasets:\n",
    "    # TODO: Implement energy sensor health analysis\n",
    "    energy_health=analyze_sensor_health(\n",
    "        datasets['energy'], \n",
    "        'meter_id', \n",
    "        ['power_consumption', 'voltage', 'current', 'power_factor']\n",
    "    )\n",
    "    sensor_health_results['energy'] = energy_health\n",
    "    print(\"‚ö° Energy Sensor Health Summary:\")\n",
    "    energy_health.groupBy(\"status\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7576e",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 2: MISSING DATA STRATEGY (Morning - 2 hours)\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a67e935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üï≥Ô∏è SECTION 2: MISSING DATA HANDLING STRATEGY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üï≥Ô∏è SECTION 2: MISSING DATA HANDLING STRATEGY\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e4f972",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 2.1: Missing Data Pattern Analysis (60 minutes)\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59e9fcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Missing Data Pattern Analysis\n",
      "----------------------------------------\n",
      "‚è∞ Temporal Missing Data Patterns:\n",
      "\n",
      "   Missing data by hour for location_lat:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|         4200|            0|        0.0|\n",
      "|   1|         4200|            0|        0.0|\n",
      "|   2|         4200|            0|        0.0|\n",
      "|   3|         4200|            0|        0.0|\n",
      "|   4|         4200|            0|        0.0|\n",
      "|   5|         4200|            0|        0.0|\n",
      "|   6|         4200|            0|        0.0|\n",
      "|   7|         4200|            0|        0.0|\n",
      "|   8|         4200|            0|        0.0|\n",
      "|   9|         4200|            0|        0.0|\n",
      "|  10|         4200|            0|        0.0|\n",
      "|  11|         4200|            0|        0.0|\n",
      "|  12|         4200|            0|        0.0|\n",
      "|  13|         4200|            0|        0.0|\n",
      "|  14|         4250|            0|        0.0|\n",
      "|  15|         4200|            0|        0.0|\n",
      "|  16|         4200|            0|        0.0|\n",
      "|  17|         4200|            0|        0.0|\n",
      "|  18|         4200|            0|        0.0|\n",
      "|  19|         4200|            0|        0.0|\n",
      "|  20|         4200|            0|        0.0|\n",
      "|  21|         4200|            0|        0.0|\n",
      "|  22|         4200|            0|        0.0|\n",
      "|  23|         4200|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for location_lon:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|         4200|            0|        0.0|\n",
      "|   1|         4200|            0|        0.0|\n",
      "|   2|         4200|            0|        0.0|\n",
      "|   3|         4200|            0|        0.0|\n",
      "|   4|         4200|            0|        0.0|\n",
      "|   5|         4200|            0|        0.0|\n",
      "|   6|         4200|            0|        0.0|\n",
      "|   7|         4200|            0|        0.0|\n",
      "|   8|         4200|            0|        0.0|\n",
      "|   9|         4200|            0|        0.0|\n",
      "|  10|         4200|            0|        0.0|\n",
      "|  11|         4200|            0|        0.0|\n",
      "|  12|         4200|            0|        0.0|\n",
      "|  13|         4200|            0|        0.0|\n",
      "|  14|         4250|            0|        0.0|\n",
      "|  15|         4200|            0|        0.0|\n",
      "|  16|         4200|            0|        0.0|\n",
      "|  17|         4200|            0|        0.0|\n",
      "|  18|         4200|            0|        0.0|\n",
      "|  19|         4200|            0|        0.0|\n",
      "|  20|         4200|            0|        0.0|\n",
      "|  21|         4200|            0|        0.0|\n",
      "|  22|         4200|            0|        0.0|\n",
      "|  23|         4200|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for vehicle_count:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|         4200|            0|        0.0|\n",
      "|   1|         4200|            0|        0.0|\n",
      "|   2|         4200|            0|        0.0|\n",
      "|   3|         4200|            0|        0.0|\n",
      "|   4|         4200|            0|        0.0|\n",
      "|   5|         4200|            0|        0.0|\n",
      "|   6|         4200|            0|        0.0|\n",
      "|   7|         4200|            0|        0.0|\n",
      "|   8|         4200|            0|        0.0|\n",
      "|   9|         4200|            0|        0.0|\n",
      "|  10|         4200|            0|        0.0|\n",
      "|  11|         4200|            0|        0.0|\n",
      "|  12|         4200|            0|        0.0|\n",
      "|  13|         4200|            0|        0.0|\n",
      "|  14|         4250|            0|        0.0|\n",
      "|  15|         4200|            0|        0.0|\n",
      "|  16|         4200|            0|        0.0|\n",
      "|  17|         4200|            0|        0.0|\n",
      "|  18|         4200|            0|        0.0|\n",
      "|  19|         4200|            0|        0.0|\n",
      "|  20|         4200|            0|        0.0|\n",
      "|  21|         4200|            0|        0.0|\n",
      "|  22|         4200|            0|        0.0|\n",
      "|  23|         4200|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "üì° Sensor-specific Missing Patterns:\n",
      "   Sensors with highest missing data:\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "|  sensor_id|total_readings|location_lat_missing|location_lon_missing|location_lat_missing_pct|location_lon_missing_pct|\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "|TRAFFIC_004|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_006|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_009|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_017|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_035|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_003|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_011|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_031|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_007|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_040|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "üîç Missing Data Pattern Analysis\n",
      "----------------------------------------\n",
      "‚è∞ Temporal Missing Data Patterns:\n",
      "\n",
      "   Missing data by hour for co:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|          560|            0|        0.0|\n",
      "|   1|          560|            0|        0.0|\n",
      "|   2|          560|            0|        0.0|\n",
      "|   3|          560|            0|        0.0|\n",
      "|   4|          560|            0|        0.0|\n",
      "|   5|          560|            0|        0.0|\n",
      "|   6|          560|            0|        0.0|\n",
      "|   7|          560|            0|        0.0|\n",
      "|   8|          560|            0|        0.0|\n",
      "|   9|          560|            0|        0.0|\n",
      "|  10|          560|            0|        0.0|\n",
      "|  11|          560|            0|        0.0|\n",
      "|  12|          560|            0|        0.0|\n",
      "|  13|          560|            0|        0.0|\n",
      "|  14|          580|            0|        0.0|\n",
      "|  15|          560|            0|        0.0|\n",
      "|  16|          560|            0|        0.0|\n",
      "|  17|          560|            0|        0.0|\n",
      "|  18|          560|            0|        0.0|\n",
      "|  19|          560|            0|        0.0|\n",
      "|  20|          560|            0|        0.0|\n",
      "|  21|          560|            0|        0.0|\n",
      "|  22|          560|            0|        0.0|\n",
      "|  23|          560|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for humidity:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|          560|            0|        0.0|\n",
      "|   1|          560|            0|        0.0|\n",
      "|   2|          560|            0|        0.0|\n",
      "|   3|          560|            0|        0.0|\n",
      "|   4|          560|            0|        0.0|\n",
      "|   5|          560|            0|        0.0|\n",
      "|   6|          560|            0|        0.0|\n",
      "|   7|          560|            0|        0.0|\n",
      "|   8|          560|            0|        0.0|\n",
      "|   9|          560|            0|        0.0|\n",
      "|  10|          560|            0|        0.0|\n",
      "|  11|          560|            0|        0.0|\n",
      "|  12|          560|            0|        0.0|\n",
      "|  13|          560|            0|        0.0|\n",
      "|  14|          580|            0|        0.0|\n",
      "|  15|          560|            0|        0.0|\n",
      "|  16|          560|            0|        0.0|\n",
      "|  17|          560|            0|        0.0|\n",
      "|  18|          560|            0|        0.0|\n",
      "|  19|          560|            0|        0.0|\n",
      "|  20|          560|            0|        0.0|\n",
      "|  21|          560|            0|        0.0|\n",
      "|  22|          560|            0|        0.0|\n",
      "|  23|          560|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for location_lat:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|          560|            0|        0.0|\n",
      "|   1|          560|            0|        0.0|\n",
      "|   2|          560|            0|        0.0|\n",
      "|   3|          560|            0|        0.0|\n",
      "|   4|          560|            0|        0.0|\n",
      "|   5|          560|            0|        0.0|\n",
      "|   6|          560|            0|        0.0|\n",
      "|   7|          560|            0|        0.0|\n",
      "|   8|          560|            0|        0.0|\n",
      "|   9|          560|            0|        0.0|\n",
      "|  10|          560|            0|        0.0|\n",
      "|  11|          560|            0|        0.0|\n",
      "|  12|          560|            0|        0.0|\n",
      "|  13|          560|            0|        0.0|\n",
      "|  14|          580|            0|        0.0|\n",
      "|  15|          560|            0|        0.0|\n",
      "|  16|          560|            0|        0.0|\n",
      "|  17|          560|            0|        0.0|\n",
      "|  18|          560|            0|        0.0|\n",
      "|  19|          560|            0|        0.0|\n",
      "|  20|          560|            0|        0.0|\n",
      "|  21|          560|            0|        0.0|\n",
      "|  22|          560|            0|        0.0|\n",
      "|  23|          560|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "üì° Sensor-specific Missing Patterns:\n",
      "   Sensors with highest missing data:\n",
      "+---------+--------------+----------+----------------+--------------+--------------------+\n",
      "|sensor_id|total_readings|co_missing|humidity_missing|co_missing_pct|humidity_missing_pct|\n",
      "+---------+--------------+----------+----------------+--------------+--------------------+\n",
      "|   AQ_003|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_016|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_012|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_002|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_004|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_006|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_009|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_015|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_010|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_013|           673|         0|               0|           0.0|                 0.0|\n",
      "+---------+--------------+----------+----------------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "üîç Missing Data Pattern Analysis\n",
      "----------------------------------------\n",
      "‚è∞ Temporal Missing Data Patterns:\n",
      "\n",
      "   Missing data by hour for location_lat:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|          140|            0|        0.0|\n",
      "|   1|          140|            0|        0.0|\n",
      "|   2|          140|            0|        0.0|\n",
      "|   3|          140|            0|        0.0|\n",
      "|   4|          140|            0|        0.0|\n",
      "|   5|          140|            0|        0.0|\n",
      "|   6|          140|            0|        0.0|\n",
      "|   7|          140|            0|        0.0|\n",
      "|   8|          140|            0|        0.0|\n",
      "|   9|          140|            0|        0.0|\n",
      "|  10|          140|            0|        0.0|\n",
      "|  11|          140|            0|        0.0|\n",
      "|  12|          140|            0|        0.0|\n",
      "|  13|          140|            0|        0.0|\n",
      "|  14|          150|            0|        0.0|\n",
      "|  15|          140|            0|        0.0|\n",
      "|  16|          140|            0|        0.0|\n",
      "|  17|          140|            0|        0.0|\n",
      "|  18|          140|            0|        0.0|\n",
      "|  19|          140|            0|        0.0|\n",
      "|  20|          140|            0|        0.0|\n",
      "|  21|          140|            0|        0.0|\n",
      "|  22|          140|            0|        0.0|\n",
      "|  23|          140|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for location_lon:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|          140|            0|        0.0|\n",
      "|   1|          140|            0|        0.0|\n",
      "|   2|          140|            0|        0.0|\n",
      "|   3|          140|            0|        0.0|\n",
      "|   4|          140|            0|        0.0|\n",
      "|   5|          140|            0|        0.0|\n",
      "|   6|          140|            0|        0.0|\n",
      "|   7|          140|            0|        0.0|\n",
      "|   8|          140|            0|        0.0|\n",
      "|   9|          140|            0|        0.0|\n",
      "|  10|          140|            0|        0.0|\n",
      "|  11|          140|            0|        0.0|\n",
      "|  12|          140|            0|        0.0|\n",
      "|  13|          140|            0|        0.0|\n",
      "|  14|          150|            0|        0.0|\n",
      "|  15|          140|            0|        0.0|\n",
      "|  16|          140|            0|        0.0|\n",
      "|  17|          140|            0|        0.0|\n",
      "|  18|          140|            0|        0.0|\n",
      "|  19|          140|            0|        0.0|\n",
      "|  20|          140|            0|        0.0|\n",
      "|  21|          140|            0|        0.0|\n",
      "|  22|          140|            0|        0.0|\n",
      "|  23|          140|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for temperature:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|          140|            0|        0.0|\n",
      "|   1|          140|            0|        0.0|\n",
      "|   2|          140|            0|        0.0|\n",
      "|   3|          140|            0|        0.0|\n",
      "|   4|          140|            0|        0.0|\n",
      "|   5|          140|            0|        0.0|\n",
      "|   6|          140|            0|        0.0|\n",
      "|   7|          140|            0|        0.0|\n",
      "|   8|          140|            0|        0.0|\n",
      "|   9|          140|            0|        0.0|\n",
      "|  10|          140|            0|        0.0|\n",
      "|  11|          140|            0|        0.0|\n",
      "|  12|          140|            0|        0.0|\n",
      "|  13|          140|            0|        0.0|\n",
      "|  14|          150|            0|        0.0|\n",
      "|  15|          140|            0|        0.0|\n",
      "|  16|          140|            0|        0.0|\n",
      "|  17|          140|            0|        0.0|\n",
      "|  18|          140|            0|        0.0|\n",
      "|  19|          140|            0|        0.0|\n",
      "|  20|          140|            0|        0.0|\n",
      "|  21|          140|            0|        0.0|\n",
      "|  22|          140|            0|        0.0|\n",
      "|  23|          140|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "üì° Sensor-specific Missing Patterns:\n",
      "   Sensors with highest missing data:\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "| station_id|total_readings|location_lat_missing|location_lon_missing|location_lat_missing_pct|location_lon_missing_pct|\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "|WEATHER_002|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_004|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_006|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_005|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_001|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_008|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_009|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_003|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_007|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_010|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "üîç Missing Data Pattern Analysis\n",
      "----------------------------------------\n",
      "‚è∞ Temporal Missing Data Patterns:\n",
      "\n",
      "   Missing data by hour for location_lat:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|         8400|            0|        0.0|\n",
      "|   1|         8400|            0|        0.0|\n",
      "|   2|         8400|            0|        0.0|\n",
      "|   3|         8400|            0|        0.0|\n",
      "|   4|         8400|            0|        0.0|\n",
      "|   5|         8400|            0|        0.0|\n",
      "|   6|         8400|            0|        0.0|\n",
      "|   7|         8400|            0|        0.0|\n",
      "|   8|         8400|            0|        0.0|\n",
      "|   9|         8400|            0|        0.0|\n",
      "|  10|         8400|            0|        0.0|\n",
      "|  11|         8400|            0|        0.0|\n",
      "|  12|         8400|            0|        0.0|\n",
      "|  13|         8400|            0|        0.0|\n",
      "|  14|         8600|            0|        0.0|\n",
      "|  15|         8400|            0|        0.0|\n",
      "|  16|         8400|            0|        0.0|\n",
      "|  17|         8400|            0|        0.0|\n",
      "|  18|         8400|            0|        0.0|\n",
      "|  19|         8400|            0|        0.0|\n",
      "|  20|         8400|            0|        0.0|\n",
      "|  21|         8400|            0|        0.0|\n",
      "|  22|         8400|            0|        0.0|\n",
      "|  23|         8400|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for location_lon:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|         8400|            0|        0.0|\n",
      "|   1|         8400|            0|        0.0|\n",
      "|   2|         8400|            0|        0.0|\n",
      "|   3|         8400|            0|        0.0|\n",
      "|   4|         8400|            0|        0.0|\n",
      "|   5|         8400|            0|        0.0|\n",
      "|   6|         8400|            0|        0.0|\n",
      "|   7|         8400|            0|        0.0|\n",
      "|   8|         8400|            0|        0.0|\n",
      "|   9|         8400|            0|        0.0|\n",
      "|  10|         8400|            0|        0.0|\n",
      "|  11|         8400|            0|        0.0|\n",
      "|  12|         8400|            0|        0.0|\n",
      "|  13|         8400|            0|        0.0|\n",
      "|  14|         8600|            0|        0.0|\n",
      "|  15|         8400|            0|        0.0|\n",
      "|  16|         8400|            0|        0.0|\n",
      "|  17|         8400|            0|        0.0|\n",
      "|  18|         8400|            0|        0.0|\n",
      "|  19|         8400|            0|        0.0|\n",
      "|  20|         8400|            0|        0.0|\n",
      "|  21|         8400|            0|        0.0|\n",
      "|  22|         8400|            0|        0.0|\n",
      "|  23|         8400|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for power_consumption:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|         8400|            0|        0.0|\n",
      "|   1|         8400|            0|        0.0|\n",
      "|   2|         8400|            0|        0.0|\n",
      "|   3|         8400|            0|        0.0|\n",
      "|   4|         8400|            0|        0.0|\n",
      "|   5|         8400|            0|        0.0|\n",
      "|   6|         8400|            0|        0.0|\n",
      "|   7|         8400|            0|        0.0|\n",
      "|   8|         8400|            0|        0.0|\n",
      "|   9|         8400|            0|        0.0|\n",
      "|  10|         8400|            0|        0.0|\n",
      "|  11|         8400|            0|        0.0|\n",
      "|  12|         8400|            0|        0.0|\n",
      "|  13|         8400|            0|        0.0|\n",
      "|  14|         8600|            0|        0.0|\n",
      "|  15|         8400|            0|        0.0|\n",
      "|  16|         8400|            0|        0.0|\n",
      "|  17|         8400|            0|        0.0|\n",
      "|  18|         8400|            0|        0.0|\n",
      "|  19|         8400|            0|        0.0|\n",
      "|  20|         8400|            0|        0.0|\n",
      "|  21|         8400|            0|        0.0|\n",
      "|  22|         8400|            0|        0.0|\n",
      "|  23|         8400|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "üì° Sensor-specific Missing Patterns:\n",
      "   Sensors with highest missing data:\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "|   meter_id|total_readings|location_lat_missing|location_lon_missing|location_lat_missing_pct|location_lon_missing_pct|\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "|ENERGY_0049|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0031|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0033|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0041|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0133|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0034|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0126|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0131|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0154|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0061|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "üéØ TASK: Understand patterns in missing data across time and sensors\n",
    "üí° HINT: Missing data in IoT often has temporal or spatial patterns\n",
    "üìö CONCEPTS: Missing data mechanisms, pattern analysis, imputation strategies\n",
    "\"\"\"\n",
    "\n",
    "def analyze_missing_patterns(df, time_col=\"timestamp\", sensor_col=None):\n",
    "    \"\"\"\n",
    "    Analyze patterns in missing data\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        time_col: Timestamp column\n",
    "        sensor_col: Sensor ID column (if applicable)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with missing data insights\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç Missing Data Pattern Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    patterns = {}\n",
    "    \n",
    "    # TODO: Temporal patterns in missing data\n",
    "    if time_col in df.columns:\n",
    "        print(\"‚è∞ Temporal Missing Data Patterns:\")\n",
    "        \n",
    "        # Create time-based aggregations\n",
    "        df_with_time_features = df.withColumn(\"hour\", F.hour(time_col)) \\\n",
    "                                 .withColumn(\"day_of_week\", F.dayofweek(time_col)) \\\n",
    "                                 .withColumn(\"date\", F.to_date(time_col))\n",
    "        \n",
    "        # TODO: Check missing data by hour of day\n",
    "        numeric_cols = [field.name for field in df.schema.fields \n",
    "                       if field.dataType in [IntegerType(), DoubleType(), FloatType()]]\n",
    "        \n",
    "        for col in numeric_cols[:3]:  # Analyze first 3 numeric columns\n",
    "            missing_by_hour = df_with_time_features.groupBy(\"hour\").agg(\n",
    "                F.count(\"*\").alias(\"total_records\"),\n",
    "                F.sum(F.when(F.col(col).isNull(), 1).otherwise(0)).alias(\"missing_count\")\n",
    "            ).withColumn(\n",
    "                \"missing_pct\", \n",
    "                (F.col(\"missing_count\") / F.col(\"total_records\")) * 100\n",
    "            ).orderBy(\"hour\")\n",
    "            \n",
    "            print(f\"\\n   Missing data by hour for {col}:\")\n",
    "            missing_by_hour.show(24)\n",
    "            \n",
    "            # TODO: Identify problematic hours\n",
    "            high_missing_hours = missing_by_hour.filter(F.col(\"missing_pct\") > 10)\n",
    "            if high_missing_hours.count() > 0:\n",
    "                print(f\"   ‚ö†Ô∏è High missing data hours for {col}:\")\n",
    "                high_missing_hours.show()\n",
    "    \n",
    "    # TODO: Sensor-specific missing patterns (if sensor column provided)\n",
    "    if sensor_col and sensor_col in df.columns:\n",
    "        print(f\"\\nüì° Sensor-specific Missing Patterns:\")\n",
    "        \n",
    "        # Calculate missing percentage per sensor\n",
    "        sensor_missing = df.groupBy(sensor_col).agg(\n",
    "            F.count(\"*\").alias(\"total_readings\")\n",
    "        )\n",
    "        \n",
    "        numeric_cols = [field.name for field in df.schema.fields \n",
    "                       if field.dataType in [IntegerType(), DoubleType(), FloatType()]]\n",
    "        \n",
    "        for col in numeric_cols[:2]:  # Analyze first 2 numeric columns\n",
    "            col_missing = df.groupBy(sensor_col).agg(\n",
    "                F.sum(F.when(F.col(col).isNull(), 1).otherwise(0)).alias(f\"{col}_missing\")\n",
    "            )\n",
    "            sensor_missing = sensor_missing.join(col_missing, sensor_col, \"left\")\n",
    "        \n",
    "        # Calculate percentages and identify problematic sensors\n",
    "        for col in numeric_cols[:2]:\n",
    "            sensor_missing = sensor_missing.withColumn(\n",
    "                f\"{col}_missing_pct\",\n",
    "                (F.col(f\"{col}_missing\") / F.col(\"total_readings\")) * 100\n",
    "            )\n",
    "        \n",
    "        print(\"   Sensors with highest missing data:\")\n",
    "        sensor_missing.orderBy(F.desc(f\"{numeric_cols[0]}_missing_pct\")).show(10)\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# TODO: Analyze missing patterns for each dataset\n",
    "for name, df in datasets.items():\n",
    "    if df is not None and name != 'zones':\n",
    "        try:\n",
    "            sensor_col = None\n",
    "            if 'sensor_id' in df.columns:\n",
    "                sensor_col = 'sensor_id'\n",
    "            elif 'station_id' in df.columns:\n",
    "                sensor_col = 'station_id'\n",
    "            elif 'meter_id' in df.columns:\n",
    "                sensor_col = 'meter_id'\n",
    "            \n",
    "            patterns = analyze_missing_patterns(df, sensor_col=sensor_col)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analyzing missing patterns for {name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d40ebb",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 2.2: Time Series Interpolation (60 minutes)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8e41eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "üéØ TASK: Implement interpolation strategies for time series gaps\n",
    "üí° HINT: Different gap sizes need different strategies\n",
    "üìö CONCEPTS: Linear interpolation, forward fill, seasonal patterns\n",
    "\"\"\"\n",
    "\n",
    "def interpolate_time_series_gaps(df, value_columns, time_col=\"timestamp\", sensor_col=None, max_gap_hours=6):\n",
    "    \"\"\"\n",
    "    Implement time series interpolation for missing values\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with time series data\n",
    "        value_columns: List of columns to interpolate\n",
    "        time_col: Timestamp column\n",
    "        sensor_col: Sensor ID column for per-sensor interpolation\n",
    "        max_gap_hours: Maximum gap size to interpolate (larger gaps left as missing)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with interpolated values\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîß Time Series Interpolation\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    result_df = df\n",
    "    \n",
    "    # TODO: For each sensor (if sensor_col provided), interpolate separately\n",
    "    if sensor_col:\n",
    "        # This is complex in Spark - we'll implement a simplified version\n",
    "        # In practice, you might use pandas UDFs or process sensor by sensor\n",
    "        \n",
    "        # TODO: Create window specification for each sensor\n",
    "        window_spec = Window.partitionBy(sensor_col).orderBy(time_col)\n",
    "        \n",
    "        for col in value_columns:\n",
    "            if col in df.columns:\n",
    "                print(f\"   Interpolating {col}...\")\n",
    "                \n",
    "                # TODO: Simple forward fill for small gaps\n",
    "                # Use lag and lead functions to fill gaps\n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{col}_filled\",\n",
    "                    F.when(\n",
    "                        F.col(col).isNull(),\n",
    "                        # TODO: Implement interpolation logic\n",
    "                        # For now, use last observation carried forward\n",
    "                        F.last(col, True).over(window_spec.rowsBetween(Window.unboundedPreceding, -1))\n",
    "                    ).otherwise(F.col(col))\n",
    "                )\n",
    "                \n",
    "                # TODO: Add metadata column to track what was interpolated\n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{col}_interpolated\",\n",
    "                    F.when(F.col(col).isNull() & F.col(f\"{col}_filled\").isNotNull(), True).otherwise(False)\n",
    "                )\n",
    "        \n",
    "    else:\n",
    "        # Global interpolation (simpler case)\n",
    "        window_spec = Window.orderBy(time_col)\n",
    "        \n",
    "        for col in value_columns:\n",
    "            if col in df.columns:\n",
    "                print(f\"   Interpolating {col} (global)...\")\n",
    "                \n",
    "                # Simple forward fill\n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{col}_filled\",\n",
    "                    F.when(\n",
    "                        F.col(col).isNull(),\n",
    "                        F.last(col, True).over(window_spec.rowsBetween(Window.unboundedPreceding, -1))\n",
    "                    ).otherwise(F.col(col))\n",
    "                )\n",
    "    \n",
    "    # TODO: Report interpolation statistics\n",
    "    print(\"üìä Interpolation Summary:\")\n",
    "    for col in value_columns:\n",
    "        if f\"{col}_filled\" in result_df.columns:\n",
    "            interpolated_count = result_df.filter(F.col(f\"{col}_interpolated\") == True).count()\n",
    "            total_missing = df.filter(F.col(col).isNull()).count()\n",
    "            print(f\"   {col}: {interpolated_count}/{total_missing} missing values interpolated\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb159851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöó Testing interpolation on traffic data...\n",
      "\n",
      "üîß Time Series Interpolation\n",
      "------------------------------\n",
      "   Interpolating vehicle_count...\n",
      "   Interpolating avg_speed...\n",
      "üìä Interpolation Summary:\n",
      "   vehicle_count: 0/0 missing values interpolated\n",
      "   avg_speed: 0/0 missing values interpolated\n",
      "\n",
      "üìä Before/After Interpolation Comparison:\n",
      "+----------------------------+---------------------------+\n",
      "|vehicle_count_missing_before|vehicle_count_missing_after|\n",
      "+----------------------------+---------------------------+\n",
      "|                           0|                          0|\n",
      "+----------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: Test interpolation on traffic data\n",
    "if 'traffic' in datasets:\n",
    "    print(\"üöó Testing interpolation on traffic data...\")\n",
    "    traffic_interpolated = interpolate_time_series_gaps(\n",
    "        datasets['traffic'],\n",
    "        ['vehicle_count', 'avg_speed'],\n",
    "        sensor_col='sensor_id'\n",
    "    )\n",
    "    \n",
    "    # Show before/after comparison\n",
    "    print(\"\\nüìä Before/After Interpolation Comparison:\")\n",
    "    comparison = datasets['traffic'].agg(\n",
    "        F.sum(F.when(F.col(\"vehicle_count\").isNull(), 1).otherwise(0)).alias(\"vehicle_count_missing_before\")\n",
    "    ).join(\n",
    "        traffic_interpolated.agg(\n",
    "            F.sum(F.when(F.col(\"vehicle_count_filled\").isNull(), 1).otherwise(0)).alias(\"vehicle_count_missing_after\")\n",
    "        )\n",
    "    )\n",
    "    comparison.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c5c3a",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 3: OUTLIER DETECTION & TREATMENT (Afternoon - 2 hours)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a17a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ SECTION 3: OUTLIER DETECTION & TREATMENT\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ SECTION 3: OUTLIER DETECTION & TREATMENT\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79ab4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_statistical_outliers(df, columns, methods=['iqr', 'zscore'], sensor_col=None):\n",
    "    \"\"\"\n",
    "    Detect outliers using statistical methods\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        columns: List of numeric columns to check\n",
    "        methods: List of detection methods to use\n",
    "        sensor_col: Sensor ID column for per-sensor analysis\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with outlier flags added\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Statistical Outlier Detection\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    result_df = df\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"   Analyzing {col}...\")\n",
    "        \n",
    "        # TODO: IQR Method\n",
    "        if 'iqr' in methods:\n",
    "            # Calculate quartiles\n",
    "            if sensor_col:\n",
    "                # Per-sensor IQR (more complex)\n",
    "                # For simplicity, we'll do global IQR first\n",
    "                pass\n",
    "                \n",
    "\n",
    "            # Global IQR calculation\n",
    "            quartiles = df.select(\n",
    "                F.expr(f\"percentile_approx({col}, 0.25)\").alias(\"q1\"),\n",
    "                F.expr(f\"percentile_approx({col}, 0.75)\").alias(\"q3\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            q1, q3 = quartiles['q1'], quartiles['q3']\n",
    "            if q1 is not None and q3 is not None:\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                \n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{col}_outlier_iqr\",\n",
    "                    F.when(\n",
    "                        (F.col(col) < lower_bound) | (F.col(col) > upper_bound),\n",
    "                        True\n",
    "                    ).otherwise(False)\n",
    "                )\n",
    "                \n",
    "                outlier_count = result_df.filter(F.col(f\"{col}_outlier_iqr\") == True).count()\n",
    "                print(f\"      IQR method: {outlier_count} outliers detected\")\n",
    "        \n",
    "        # TODO: Z-Score Method  \n",
    "        if 'zscore' in methods:\n",
    "            # Calculate mean and standard deviation\n",
    "            stats = df.select(\n",
    "                F.mean(col).alias(\"mean_val\"),\n",
    "                F.stddev(col).alias(\"stddev_val\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            mean_val, stddev_val = stats['mean_val'], stats['stddev_val']\n",
    "            if mean_val is not None and stddev_val is not None and stddev_val > 0:\n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{col}_zscore\",\n",
    "                    F.abs((F.col(col) - mean_val) / stddev_val)\n",
    "                ).withColumn(\n",
    "                    f\"{col}_outlier_zscore\",\n",
    "                    F.when(F.col(f\"{col}_zscore\") > 3, True).otherwise(False)\n",
    "                )\n",
    "                \n",
    "                outlier_count = result_df.filter(F.col(f\"{col}_outlier_zscore\") == True).count()\n",
    "                print(f\"      Z-Score method: {outlier_count} outliers detected\")\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bdffff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Domain-specific outlier rules\n",
    "def detect_domain_outliers(df, dataset_type):\n",
    "    \"\"\"\n",
    "    Apply domain-specific outlier detection rules\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        dataset_type: Type of sensor data (traffic, air_quality, weather, energy)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with domain-specific outlier flags\n",
    "    \"\"\"\n",
    "    print(f\"\\nüè≠ Domain-Specific Outlier Detection: {dataset_type}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    result_df = df\n",
    "    \n",
    "    if dataset_type == 'traffic':\n",
    "        # TODO: Traffic-specific rules\n",
    "        if 'avg_speed' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"speed_outlier_domain\",\n",
    "                F.when(\n",
    "                    # Speed should be reasonable for the road type\n",
    "                    (F.col(\"avg_speed\") < 0) | (F.col(\"avg_speed\") > 120),  # Impossible speeds\n",
    "                    True\n",
    "                ).otherwise(False)\n",
    "            )\n",
    "        \n",
    "        if 'vehicle_count' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"vehicle_count_outlier_domain\", \n",
    "                F.when(\n",
    "                    F.col(\"vehicle_count\") < 0,  # Negative vehicle count impossible\n",
    "                    True\n",
    "                ).otherwise(False)\n",
    "            )\n",
    "            \n",
    "    elif dataset_type == 'air_quality':\n",
    "        # TODO: Air quality specific rules\n",
    "        if 'pm25' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"pm25_outlier_domain\",\n",
    "                F.when(\n",
    "                    (F.col(\"pm25\") < 0) | (F.col(\"pm25\") > 500),  # PM2.5 reasonable range\n",
    "                    True\n",
    "                ).otherwise(False)\n",
    "            )\n",
    "        \n",
    "        # TODO: Add more air quality rules\n",
    "        \n",
    "    elif dataset_type == 'weather':\n",
    "        # TODO: Weather-specific rules  \n",
    "        if 'temperature' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"temperature_outlier_domain\",\n",
    "                F.when(\n",
    "                    (F.col(\"temperature\") < -50) | (F.col(\"temperature\") > 60),  # Extreme temperatures\n",
    "                    True\n",
    "                ).otherwise(False)\n",
    "            )\n",
    "            \n",
    "    elif dataset_type == 'energy':\n",
    "        # TODO: Energy-specific rules\n",
    "        if 'power_consumption' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"power_outlier_domain\",\n",
    "                F.when(\n",
    "                    F.col(\"power_consumption\") < 0,  # Negative power consumption\n",
    "                    True\n",
    "                ).otherwise(False)\n",
    "            )\n",
    "    \n",
    "    # Count domain outliers\n",
    "    outlier_cols = [col for col in result_df.columns if col.endswith('_outlier_domain')]\n",
    "    if outlier_cols:\n",
    "        for col in outlier_cols:\n",
    "            outlier_count = result_df.filter(F.col(col) == True).count()\n",
    "            print(f\"   {col}: {outlier_count} outliers detected\")\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3dc48a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Testing outlier detection across all datasets...\n",
      "\n",
      "üîç Statistical Outlier Detection\n",
      "-----------------------------------\n",
      "   Analyzing vehicle_count...\n",
      "      IQR method: 3067 outliers detected\n",
      "      Z-Score method: 1950 outliers detected\n",
      "   Analyzing avg_speed...\n",
      "      IQR method: 124 outliers detected\n",
      "      Z-Score method: 77 outliers detected\n",
      "\n",
      "üè≠ Domain-Specific Outlier Detection: traffic\n",
      "---------------------------------------------\n",
      "   speed_outlier_domain: 1 outliers detected\n",
      "   vehicle_count_outlier_domain: 0 outliers detected\n",
      "\n",
      "‚úÖ Outlier detection completed for traffic\n",
      "\n",
      "üîç Statistical Outlier Detection\n",
      "-----------------------------------\n",
      "   Analyzing co...\n",
      "      IQR method: 97 outliers detected\n",
      "      Z-Score method: 29 outliers detected\n",
      "   Analyzing humidity...\n",
      "      IQR method: 0 outliers detected\n",
      "      Z-Score method: 0 outliers detected\n",
      "   Analyzing no2...\n",
      "      IQR method: 80 outliers detected\n",
      "      Z-Score method: 28 outliers detected\n",
      "\n",
      "üè≠ Domain-Specific Outlier Detection: air_quality\n",
      "---------------------------------------------\n",
      "   pm25_outlier_domain: 0 outliers detected\n",
      "\n",
      "‚úÖ Outlier detection completed for air_quality\n",
      "\n",
      "üîç Statistical Outlier Detection\n",
      "-----------------------------------\n",
      "   Analyzing temperature...\n",
      "      IQR method: 35 outliers detected\n",
      "      Z-Score method: 11 outliers detected\n",
      "   Analyzing humidity...\n",
      "      IQR method: 0 outliers detected\n",
      "      Z-Score method: 0 outliers detected\n",
      "   Analyzing wind_speed...\n",
      "      IQR method: 154 outliers detected\n",
      "      Z-Score method: 65 outliers detected\n",
      "\n",
      "üè≠ Domain-Specific Outlier Detection: weather\n",
      "---------------------------------------------\n",
      "   temperature_outlier_domain: 0 outliers detected\n",
      "\n",
      "‚úÖ Outlier detection completed for weather\n",
      "\n",
      "üîç Statistical Outlier Detection\n",
      "-----------------------------------\n",
      "   Analyzing power_consumption...\n",
      "      IQR method: 0 outliers detected\n",
      "      Z-Score method: 0 outliers detected\n",
      "   Analyzing voltage...\n",
      "      IQR method: 1406 outliers detected\n",
      "      Z-Score method: 501 outliers detected\n",
      "   Analyzing current...\n",
      "      IQR method: 0 outliers detected\n",
      "      Z-Score method: 0 outliers detected\n",
      "\n",
      "üè≠ Domain-Specific Outlier Detection: energy\n",
      "---------------------------------------------\n",
      "   power_outlier_domain: 0 outliers detected\n",
      "\n",
      "‚úÖ Outlier detection completed for energy\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test outlier detection on all datasets\n",
    "print(\"üéØ Testing outlier detection across all datasets...\")\n",
    "\n",
    "outlier_results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if df is not None and name != 'zones':\n",
    "        try:\n",
    "            # Get numeric columns for statistical outlier detection\n",
    "            numeric_cols = [field.name for field in df.schema.fields \n",
    "                           if field.dataType in [IntegerType(), DoubleType(), FloatType()] \n",
    "                           and field.name not in ['location_lat', 'location_lon']]\n",
    "            \n",
    "            if numeric_cols:\n",
    "                # Statistical outlier detection\n",
    "                df_with_outliers = detect_statistical_outliers(df, numeric_cols[:3])  # First 3 columns\n",
    "                \n",
    "                # Domain-specific outlier detection\n",
    "                df_with_all_outliers = detect_domain_outliers(df_with_outliers, name)\n",
    "                \n",
    "                outlier_results[name] = df_with_all_outliers\n",
    "                \n",
    "                print(f\"\\n‚úÖ Outlier detection completed for {name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in outlier detection for {name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d00f7c6",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 3.2: Outlier Treatment Strategies (60 minutes)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6f67840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöó Testing multiple treatment strategies on traffic data...\n",
      "\n",
      "üîß Outlier Treatment Strategy: cap\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   vehicle_count_outlier_iqr: 3067 outliers capped\n",
      "   vehicle_count_outlier_zscore: 1950 outliers capped\n",
      "   avg_speed_outlier_iqr: 124 outliers capped\n",
      "   avg_speed_outlier_zscore: 77 outliers capped\n",
      "   vehicle_count_outlier_domain: 0 outliers capped\n",
      "\n",
      "üîß Outlier Treatment Strategy: impute\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   vehicle_count_outlier_iqr: 3067 outliers imputed\n",
      "   vehicle_count_outlier_zscore: 1950 outliers imputed\n",
      "   avg_speed_outlier_iqr: 124 outliers imputed\n",
      "   avg_speed_outlier_zscore: 77 outliers imputed\n",
      "   vehicle_count_outlier_domain: 0 outliers imputed\n",
      "\n",
      "üîß Outlier Treatment Strategy: flag_only\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   vehicle_count_outlier_iqr: 3067 outliers flagged_only\n",
      "   vehicle_count_outlier_zscore: 1950 outliers flagged_only\n",
      "   avg_speed_outlier_iqr: 124 outliers flagged_only\n",
      "   avg_speed_outlier_zscore: 77 outliers flagged_only\n",
      "   vehicle_count_outlier_domain: 0 outliers flagged_only\n",
      "‚úÖ Outlier treatment completed for traffic data\n",
      "\n",
      "üå´Ô∏è Testing multiple treatment strategies on air quality data...\n",
      "\n",
      "üîß Outlier Treatment Strategy: remove\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   co_outlier_iqr: 97 outliers removed\n",
      "   co_outlier_zscore: 29 outliers removed\n",
      "   humidity_outlier_iqr: 0 outliers removed\n",
      "   humidity_outlier_zscore: 0 outliers removed\n",
      "   no2_outlier_iqr: 80 outliers removed\n",
      "   no2_outlier_zscore: 28 outliers removed\n",
      "   pm25_outlier_domain: 0 outliers removed\n",
      "\n",
      "üîß Outlier Treatment Strategy: cap\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   co_outlier_iqr: 97 outliers capped\n",
      "   co_outlier_zscore: 29 outliers capped\n",
      "   humidity_outlier_iqr: 0 outliers capped\n",
      "   humidity_outlier_zscore: 0 outliers capped\n",
      "   no2_outlier_iqr: 80 outliers capped\n",
      "   no2_outlier_zscore: 28 outliers capped\n",
      "   pm25_outlier_domain: 0 outliers capped\n",
      "\n",
      "üîß Outlier Treatment Strategy: impute\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   co_outlier_iqr: 97 outliers imputed\n",
      "   co_outlier_zscore: 29 outliers imputed\n",
      "   humidity_outlier_iqr: 0 outliers imputed\n",
      "   humidity_outlier_zscore: 0 outliers imputed\n",
      "   no2_outlier_iqr: 80 outliers imputed\n",
      "   no2_outlier_zscore: 28 outliers imputed\n",
      "   pm25_outlier_domain: 0 outliers imputed\n",
      "\n",
      "üîß Outlier Treatment Strategy: flag_only\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   co_outlier_iqr: 97 outliers flagged_only\n",
      "   co_outlier_zscore: 29 outliers flagged_only\n",
      "   humidity_outlier_iqr: 0 outliers flagged_only\n",
      "   humidity_outlier_zscore: 0 outliers flagged_only\n",
      "   no2_outlier_iqr: 80 outliers flagged_only\n",
      "   no2_outlier_zscore: 28 outliers flagged_only\n",
      "   pm25_outlier_domain: 0 outliers flagged_only\n",
      "‚úÖ Outlier treatment completed for air quality data\n",
      "\n",
      "üå¶Ô∏è Testing multiple treatment strategies on weather data...\n",
      "\n",
      "üîß Outlier Treatment Strategy: cap\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   temperature_outlier_iqr: 35 outliers capped\n",
      "   temperature_outlier_zscore: 11 outliers capped\n",
      "   humidity_outlier_iqr: 0 outliers capped\n",
      "   humidity_outlier_zscore: 0 outliers capped\n",
      "   wind_speed_outlier_iqr: 154 outliers capped\n",
      "   wind_speed_outlier_zscore: 65 outliers capped\n",
      "   temperature_outlier_domain: 0 outliers capped\n",
      "\n",
      "üîß Outlier Treatment Strategy: impute\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   temperature_outlier_iqr: 35 outliers imputed\n",
      "   temperature_outlier_zscore: 11 outliers imputed\n",
      "   humidity_outlier_iqr: 0 outliers imputed\n",
      "   humidity_outlier_zscore: 0 outliers imputed\n",
      "   wind_speed_outlier_iqr: 154 outliers imputed\n",
      "   wind_speed_outlier_zscore: 65 outliers imputed\n",
      "   temperature_outlier_domain: 0 outliers imputed\n",
      "\n",
      "üîß Outlier Treatment Strategy: flag_only\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   temperature_outlier_iqr: 35 outliers flagged_only\n",
      "   temperature_outlier_zscore: 11 outliers flagged_only\n",
      "   humidity_outlier_iqr: 0 outliers flagged_only\n",
      "   humidity_outlier_zscore: 0 outliers flagged_only\n",
      "   wind_speed_outlier_iqr: 154 outliers flagged_only\n",
      "   wind_speed_outlier_zscore: 65 outliers flagged_only\n",
      "   temperature_outlier_domain: 0 outliers flagged_only\n",
      "‚úÖ Outlier treatment completed for weather data\n",
      "\n",
      "‚ö° Testing multiple treatment strategies on energy data...\n",
      "\n",
      "üîß Outlier Treatment Strategy: remove\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   power_consumption_outlier_iqr: 0 outliers removed\n",
      "   power_consumption_outlier_zscore: 0 outliers removed\n",
      "   voltage_outlier_iqr: 1406 outliers removed\n",
      "   voltage_outlier_zscore: 501 outliers removed\n",
      "   current_outlier_iqr: 0 outliers removed\n",
      "   current_outlier_zscore: 0 outliers removed\n",
      "\n",
      "üîß Outlier Treatment Strategy: cap\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   power_consumption_outlier_iqr: 0 outliers capped\n",
      "   power_consumption_outlier_zscore: 0 outliers capped\n",
      "   voltage_outlier_iqr: 1406 outliers capped\n",
      "   voltage_outlier_zscore: 501 outliers capped\n",
      "   current_outlier_iqr: 0 outliers capped\n",
      "   current_outlier_zscore: 0 outliers capped\n",
      "\n",
      "üîß Outlier Treatment Strategy: impute\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   power_consumption_outlier_iqr: 0 outliers imputed\n",
      "   power_consumption_outlier_zscore: 0 outliers imputed\n",
      "   voltage_outlier_iqr: 1406 outliers imputed\n",
      "   voltage_outlier_zscore: 501 outliers imputed\n",
      "   current_outlier_iqr: 0 outliers imputed\n",
      "   current_outlier_zscore: 0 outliers imputed\n",
      "\n",
      "üîß Outlier Treatment Strategy: flag_only\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   power_consumption_outlier_iqr: 0 outliers flagged_only\n",
      "   power_consumption_outlier_zscore: 0 outliers flagged_only\n",
      "   voltage_outlier_iqr: 1406 outliers flagged_only\n",
      "   voltage_outlier_zscore: 501 outliers flagged_only\n",
      "   current_outlier_iqr: 0 outliers flagged_only\n",
      "   current_outlier_zscore: 0 outliers flagged_only\n",
      "‚úÖ Outlier treatment completed for energy data\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "üéØ TASK: Implement different strategies for handling detected outliers\n",
    "üí° HINT: Consider business impact when choosing treatment methods\n",
    "üìö CONCEPTS: Capping, removal, imputation, flagging\n",
    "\"\"\"\n",
    "\n",
    "def treat_outliers(df, treatment_strategy='cap', outlier_columns=None):\n",
    "    \"\"\"\n",
    "    Apply outlier treatment strategies\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with outlier flags\n",
    "        treatment_strategy: 'cap', 'remove', 'impute', or 'flag_only'\n",
    "        outlier_columns: List of outlier flag columns to process\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with outliers treated\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîß Outlier Treatment Strategy: {treatment_strategy}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    result_df = df\n",
    "    \n",
    "    if outlier_columns is None:\n",
    "        # Find all outlier flag columns\n",
    "        outlier_columns = [col for col in df.columns if '_outlier_' in col]\n",
    "    \n",
    "    treatment_stats = {}\n",
    "    \n",
    "    for outlier_col in outlier_columns:\n",
    "        # Extract the original column name\n",
    "        original_col = outlier_col.split('_outlier_')[0]\n",
    "        \n",
    "        if original_col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        outlier_count = df.filter(F.col(outlier_col) == True).count()\n",
    "        treatment_stats[outlier_col] = {'original_outliers': outlier_count}\n",
    "        \n",
    "        if treatment_strategy == 'remove':\n",
    "            # TODO: Remove outlier rows\n",
    "            result_df = result_df.filter(F.col(outlier_col) == False)\n",
    "            treatment_stats[outlier_col]['action'] = 'removed'\n",
    "            \n",
    "        elif treatment_strategy == 'cap':\n",
    "            # TODO: Cap outliers to reasonable bounds\n",
    "            # Calculate 5th and 95th percentiles for capping\n",
    "            percentiles = df.select(\n",
    "                F.expr(f\"percentile_approx({original_col}, 0.05)\").alias(\"p5\"),\n",
    "                F.expr(f\"percentile_approx({original_col}, 0.95)\").alias(\"p95\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            p5, p95 = percentiles['p5'], percentiles['p95']\n",
    "            if p5 is not None and p95 is not None:\n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{original_col}_capped\",\n",
    "                    F.when(F.col(outlier_col) == True,\n",
    "                           F.when(F.col(original_col) < p5, p5)\n",
    "                            .when(F.col(original_col) > p95, p95)\n",
    "                            .otherwise(F.col(original_col))\n",
    "                    ).otherwise(F.col(original_col))\n",
    "                )\n",
    "                treatment_stats[outlier_col]['action'] = 'capped'\n",
    "                \n",
    "        elif treatment_strategy == 'impute':\n",
    "            # TODO: Replace outliers with median/mean\n",
    "            median_val = df.filter(F.col(outlier_col) == False).select(\n",
    "                F.expr(f\"percentile_approx({original_col}, 0.5)\").alias(\"median\")\n",
    "            ).collect()[0]['median']\n",
    "            \n",
    "            if median_val is not None:\n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{original_col}_imputed\",\n",
    "                    F.when(F.col(outlier_col) == True, median_val)\n",
    "                     .otherwise(F.col(original_col))\n",
    "                )\n",
    "                treatment_stats[outlier_col]['action'] = 'imputed'\n",
    "                \n",
    "        elif treatment_strategy == 'flag_only':\n",
    "            # TODO: Just keep the outlier flags for downstream analysis\n",
    "            treatment_stats[outlier_col]['action'] = 'flagged_only'\n",
    "    \n",
    "    # Print treatment summary\n",
    "    print(\"üìä Treatment Summary:\")\n",
    "    for col, stats in treatment_stats.items():\n",
    "        action = stats.get('action', 'none')\n",
    "        count = stats.get('original_outliers', 0)\n",
    "        print(f\"   {col}: {count} outliers {action}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# TODO: Test all treatment strategies on different datasets\n",
    "treatment_results = {}\n",
    "\n",
    "if 'traffic' in outlier_results:\n",
    "    print(\"\\nüöó Testing multiple treatment strategies on traffic data...\")\n",
    "    \n",
    "    # Test capping strategy\n",
    "    traffic_capped = treat_outliers(\n",
    "        outlier_results['traffic'], \n",
    "        treatment_strategy='cap'\n",
    "    )\n",
    "    treatment_results['traffic_capped'] = traffic_capped\n",
    "    \n",
    "    # Test imputation strategy\n",
    "    traffic_imputed = treat_outliers(\n",
    "        outlier_results['traffic'], \n",
    "        treatment_strategy='impute'\n",
    "    )\n",
    "    treatment_results['traffic_imputed'] = traffic_imputed\n",
    "    \n",
    "    # Test flagging only\n",
    "    traffic_flagged = treat_outliers(\n",
    "        outlier_results['traffic'], \n",
    "        treatment_strategy='flag_only'\n",
    "    )\n",
    "    treatment_results['traffic_flagged'] = traffic_flagged\n",
    "    \n",
    "    print(\"‚úÖ Outlier treatment completed for traffic data\")\n",
    "\n",
    "if 'air_quality' in outlier_results:\n",
    "    print(\"\\nüå´Ô∏è Testing multiple treatment strategies on air quality data...\")\n",
    "    # Test removal strategy\n",
    "    air_quality_removed = treat_outliers(\n",
    "        outlier_results['air_quality'], \n",
    "        treatment_strategy='remove'\n",
    "    )\n",
    "    treatment_results['air_quality_removed'] = air_quality_removed  \n",
    "\n",
    "    # Test capping strategy\n",
    "    air_quality_capped = treat_outliers(\n",
    "        outlier_results['air_quality'], \n",
    "        treatment_strategy='cap'\n",
    "    )\n",
    "    treatment_results['air_quality_capped'] = air_quality_capped\n",
    "\n",
    "    # Test imputation strategy\n",
    "    air_quality_imputed = treat_outliers(\n",
    "        outlier_results['air_quality'], \n",
    "        treatment_strategy='impute'\n",
    "    )\n",
    "    treatment_results['air_quality_imputed'] = air_quality_imputed\n",
    "\n",
    "    # Test flagging only\n",
    "    air_quality_flagged = treat_outliers(\n",
    "        outlier_results['air_quality'], \n",
    "        treatment_strategy='flag_only'\n",
    "    )\n",
    "    treatment_results['air_quality_flagged'] = air_quality_flagged\n",
    "\n",
    "    print(\"‚úÖ Outlier treatment completed for air quality data\")\n",
    "\n",
    "if 'weather' in outlier_results:\n",
    "    print(\"\\nüå¶Ô∏è Testing multiple treatment strategies on weather data...\")\n",
    "    # Test capping strategy\n",
    "    weather_capped = treat_outliers(\n",
    "        outlier_results['weather'], \n",
    "        treatment_strategy='cap'\n",
    "    )\n",
    "    treatment_results['weather_capped'] = weather_capped\n",
    "\n",
    "    # Test imputation strategy\n",
    "    weather_imputed = treat_outliers(\n",
    "        outlier_results['weather'], \n",
    "        treatment_strategy='impute'\n",
    "    )\n",
    "    treatment_results['weather_imputed'] = weather_imputed\n",
    "\n",
    "    # Test flagging only\n",
    "    weather_flagged = treat_outliers(\n",
    "        outlier_results['weather'], \n",
    "        treatment_strategy='flag_only'\n",
    "    )\n",
    "    treatment_results['weather_flagged'] = weather_flagged\n",
    "\n",
    "    print(\"‚úÖ Outlier treatment completed for weather data\")\n",
    "\n",
    "if 'energy' in outlier_results: \n",
    "    print(\"\\n‚ö° Testing multiple treatment strategies on energy data...\")\n",
    "    # Test removal strategy\n",
    "    energy_removed = treat_outliers(\n",
    "        outlier_results['energy'], \n",
    "        treatment_strategy='remove'\n",
    "    )\n",
    "    treatment_results['energy_removed'] = energy_removed  \n",
    "\n",
    "    # Test capping strategy\n",
    "    energy_capped = treat_outliers(\n",
    "        outlier_results['energy'], \n",
    "        treatment_strategy='cap'\n",
    "    )\n",
    "    treatment_results['energy_capped'] = energy_capped\n",
    "\n",
    "    # Test imputation strategy\n",
    "    energy_imputed = treat_outliers(\n",
    "        outlier_results['energy'], \n",
    "        treatment_strategy='impute'\n",
    "    )\n",
    "    treatment_results['energy_imputed'] = energy_imputed\n",
    "\n",
    "    # Test flagging only\n",
    "    energy_flagged = treat_outliers(\n",
    "        outlier_results['energy'], \n",
    "        treatment_strategy='flag_only'\n",
    "    )\n",
    "    treatment_results['energy_flagged'] = energy_flagged\n",
    "\n",
    "    print(\"‚úÖ Outlier treatment completed for energy data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276deceb",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 4: DATA STANDARDIZATION (Afternoon - 2 hours)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25c480d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìè SECTION 4: DATA STANDARDIZATION\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìè SECTION 4: DATA STANDARDIZATION\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a439cab",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 4.1: Unit Standardization (60 minutes)\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7eae78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "üéØ TASK: Ensure consistent units across all measurements\n",
    "üí° HINT: Different sensors might use different units for similar measurements\n",
    "üìö CONCEPTS: Unit conversion, standardization, metadata management\n",
    "\"\"\"\n",
    "\n",
    "def standardize_measurement_units(df, sensor_type):\n",
    "    \"\"\"\n",
    "    Standardize measurement units for a sensor type\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with sensor measurements\n",
    "        sensor_type: Type of sensor (traffic, air_quality, weather, energy)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with standardized units\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìè Standardizing units for {sensor_type} sensors\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    result_df = df\n",
    "    conversions_applied = []\n",
    "    \n",
    "    if sensor_type == 'traffic':\n",
    "        # TODO: Standardize speed to km/h (if needed)\n",
    "        if 'avg_speed' in df.columns:\n",
    "            # Assume data is already in km/h, but add validation\n",
    "            result_df = result_df.withColumn(\n",
    "                \"avg_speed_kmh\", \n",
    "                F.col(\"avg_speed\")  # Already in km/h\n",
    "            ).withColumn(\n",
    "                \"avg_speed_unit\",\n",
    "                F.lit(\"km/h\")\n",
    "            )\n",
    "            conversions_applied.append(\"Speed: km/h\")\n",
    "            \n",
    "    elif sensor_type == 'air_quality':\n",
    "        # TODO: Standardize pollution measurements to Œºg/m¬≥\n",
    "        if 'pm25' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"pm25_ugm3\",\n",
    "                F.col(\"pm25\")  # Assume already in Œºg/m¬≥\n",
    "            ).withColumn(\n",
    "                \"pm25_unit\",\n",
    "                F.lit(\"Œºg/m¬≥\")\n",
    "            )\n",
    "            conversions_applied.append(\"PM2.5: Œºg/m¬≥\")\n",
    "            \n",
    "        # TODO: Temperature to Celsius\n",
    "        if 'temperature' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"temperature_celsius\",\n",
    "                F.col(\"temperature\")  # Assume already in Celsius\n",
    "            ).withColumn(\n",
    "                \"temperature_unit\",\n",
    "                F.lit(\"¬∞C\")\n",
    "            )\n",
    "            conversions_applied.append(\"Temperature: ¬∞C\")\n",
    "            \n",
    "    elif sensor_type == 'weather':\n",
    "        # TODO: Standardize weather measurements\n",
    "        if 'temperature' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"temperature_celsius\",\n",
    "                F.col(\"temperature\")\n",
    "            ).withColumn(\n",
    "                \"temperature_unit\",\n",
    "                F.lit(\"¬∞C\")\n",
    "            )\n",
    "            \n",
    "        if 'wind_speed' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"wind_speed_kmh\",\n",
    "                F.col(\"wind_speed\")  # Assume already in km/h\n",
    "            ).withColumn(\n",
    "                \"wind_speed_unit\", \n",
    "                F.lit(\"km/h\")\n",
    "            )\n",
    "            conversions_applied.append(\"Wind Speed: km/h\")\n",
    "            \n",
    "        if 'pressure' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"pressure_hpa\",\n",
    "                F.col(\"pressure\")  # Assume already in hPa\n",
    "            ).withColumn(\n",
    "                \"pressure_unit\",\n",
    "                F.lit(\"hPa\")\n",
    "            )\n",
    "            conversions_applied.append(\"Pressure: hPa\")\n",
    "            \n",
    "    elif sensor_type == 'energy':\n",
    "        # TODO: Standardize energy measurements\n",
    "        if 'power_consumption' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"power_consumption_kw\",\n",
    "                F.col(\"power_consumption\")  # Assume already in kW\n",
    "            ).withColumn(\n",
    "                \"power_unit\",\n",
    "                F.lit(\"kW\")\n",
    "            )\n",
    "            conversions_applied.append(\"Power: kW\")\n",
    "            \n",
    "        if 'voltage' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"voltage_v\",\n",
    "                F.col(\"voltage\")  # Assume already in V\n",
    "            ).withColumn(\n",
    "                \"voltage_unit\",\n",
    "                F.lit(\"V\")\n",
    "            )\n",
    "            conversions_applied.append(\"Voltage: V\")\n",
    "    \n",
    "    print(\"üîÑ Unit conversions applied:\")\n",
    "    for conversion in conversions_applied:\n",
    "        print(f\"   ‚úÖ {conversion}\")\n",
    "        \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef2ec811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìè Standardizing units for traffic sensors\n",
      "----------------------------------------\n",
      "üîÑ Unit conversions applied:\n",
      "   ‚úÖ Speed: km/h\n",
      "‚úÖ Unit standardization completed for traffic\n",
      "\n",
      "üìè Standardizing units for air_quality sensors\n",
      "----------------------------------------\n",
      "üîÑ Unit conversions applied:\n",
      "   ‚úÖ PM2.5: Œºg/m¬≥\n",
      "   ‚úÖ Temperature: ¬∞C\n",
      "‚úÖ Unit standardization completed for air_quality\n",
      "\n",
      "üìè Standardizing units for weather sensors\n",
      "----------------------------------------\n",
      "üîÑ Unit conversions applied:\n",
      "   ‚úÖ Wind Speed: km/h\n",
      "   ‚úÖ Pressure: hPa\n",
      "‚úÖ Unit standardization completed for weather\n",
      "\n",
      "üìè Standardizing units for energy sensors\n",
      "----------------------------------------\n",
      "üîÑ Unit conversions applied:\n",
      "   ‚úÖ Power: kW\n",
      "   ‚úÖ Voltage: V\n",
      "‚úÖ Unit standardization completed for energy\n"
     ]
    }
   ],
   "source": [
    "# TODO: Apply unit standardization to all datasets\n",
    "standardized_datasets = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if df is not None and name != 'zones':\n",
    "        try:\n",
    "            standardized_df = standardize_measurement_units(df, name)\n",
    "            standardized_datasets[name] = standardized_df\n",
    "            print(f\"‚úÖ Unit standardization completed for {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error standardizing {name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4a5026",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 4.2: Data Lineage Tracking (60 minutes)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "531a3bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "üéØ TASK: Track data transformation history and quality metadata\n",
    "üí° HINT: Important for understanding data processing steps and debugging\n",
    "üìö CONCEPTS: Data lineage, metadata management, audit trails\n",
    "\"\"\"\n",
    "\n",
    "def add_data_lineage(df, transformations_applied, data_quality_score=None):\n",
    "    \"\"\"\n",
    "    Add data lineage and quality tracking columns\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to augment\n",
    "        transformations_applied: List of transformation descriptions\n",
    "        data_quality_score: Overall quality score (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with lineage metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìã Adding Data Lineage Tracking\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    result_df = df\n",
    "    \n",
    "    # TODO: Add processing timestamp\n",
    "    result_df = result_df.withColumn(\n",
    "        \"processed_at\",\n",
    "        F.current_timestamp()\n",
    "    )\n",
    "    \n",
    "    # TODO: Add transformation history\n",
    "    transformations_json = str(transformations_applied)  # Convert to string for Spark\n",
    "    result_df = result_df.withColumn(\n",
    "        \"transformations_applied\",\n",
    "        F.lit(transformations_json)\n",
    "    )\n",
    "    \n",
    "    # TODO: Add data quality score\n",
    "    if data_quality_score is not None:\n",
    "        result_df = result_df.withColumn(\n",
    "            \"data_quality_score\",\n",
    "            F.lit(data_quality_score)\n",
    "        )\n",
    "    \n",
    "    # TODO: Add data completeness score per row\n",
    "    # Calculate percentage of non-null values per row\n",
    "    numeric_cols = [field.name for field in df.schema.fields \n",
    "                   if field.dataType in [IntegerType(), DoubleType(), FloatType()]]\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        # Count non-null numeric columns per row\n",
    "        non_null_expressions = [F.when(F.col(col).isNotNull(), 1).otherwise(0) for col in numeric_cols]\n",
    "        from functools import reduce\n",
    "        from operator import add\n",
    "        \n",
    "        if len(non_null_expressions) > 1:\n",
    "            non_null_count = reduce(add, non_null_expressions)\n",
    "        else:\n",
    "            non_null_count = non_null_expressions[0]\n",
    "            \n",
    "        total_numeric_cols = len(numeric_cols)\n",
    "        \n",
    "        result_df = result_df.withColumn(\n",
    "            \"row_completeness_score\",\n",
    "            non_null_count / total_numeric_cols\n",
    "        )\n",
    "    \n",
    "    # TODO: Add record-level quality flags\n",
    "    outlier_flag_cols = [col for col in df.columns if '_outlier_' in col]\n",
    "\n",
    "    if outlier_flag_cols:\n",
    "        # Create expression to check if ANY outlier flag is True\n",
    "        from functools import reduce\n",
    "        outlier_expressions = [F.col(col) for col in outlier_flag_cols]\n",
    "        if len(outlier_expressions) > 1:\n",
    "            has_outliers_expr = reduce(lambda a, b: a | b, outlier_expressions)\n",
    "        else:\n",
    "            has_outliers_expr = outlier_expressions[0]\n",
    "        \n",
    "        result_df = result_df.withColumn(\n",
    "            \"has_outliers\",\n",
    "            F.coalesce(has_outliers_expr, F.lit(False))\n",
    "        )\n",
    "    else:\n",
    "        result_df = result_df.withColumn(\n",
    "        \"has_outliers\",\n",
    "        # Check if any outlier flag columns are True\n",
    "        # This is a simplified version - in practice you'd check all outlier flag columns\n",
    "        F.lit(False)  # Placeholder\n",
    "        )\n",
    "\n",
    "    interpolated_flag_cols = [col for col in df.columns if '_interpolated_' in col]\n",
    "    if interpolated_flag_cols:\n",
    "        # Create expression to check if ANY interpolated flag is True\n",
    "        from functools import reduce\n",
    "        interpolated_expressions = [F.col(col) for col in interpolated_flag_cols]\n",
    "        if len(interpolated_expressions) > 1:\n",
    "            has_interpolated_expr = reduce(lambda a, b: a | b, interpolated_expressions)\n",
    "        else:\n",
    "            has_interpolated_expr = interpolated_expressions[0]\n",
    "\n",
    "        result_df = result_df.withColumn(\n",
    "            \"has_interpolated_values\",\n",
    "            F.coalesce(has_interpolated_expr, F.lit(False))\n",
    "        )\n",
    "    else:\n",
    "        result_df = result_df.withColumn(\n",
    "            \"has_interpolated_values\",\n",
    "            # Check if any interpolated flag columns are True\n",
    "            F.lit(False)  # Placeholder\n",
    "        )\n",
    "    \n",
    "    print(\"üìä Lineage metadata added:\")\n",
    "    print(\"   ‚úÖ Processing timestamp\")\n",
    "    print(\"   ‚úÖ Transformation history\")\n",
    "    print(\"   ‚úÖ Data quality scores\")\n",
    "    print(\"   ‚úÖ Record-level quality flags\")\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ef34f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Adding Data Lineage Tracking\n",
      "-----------------------------------\n",
      "üìä Lineage metadata added:\n",
      "   ‚úÖ Processing timestamp\n",
      "   ‚úÖ Transformation history\n",
      "   ‚úÖ Data quality scores\n",
      "   ‚úÖ Record-level quality flags\n",
      "‚úÖ Lineage tracking added to traffic\n",
      "\n",
      "üìã Adding Data Lineage Tracking\n",
      "-----------------------------------\n",
      "üìä Lineage metadata added:\n",
      "   ‚úÖ Processing timestamp\n",
      "   ‚úÖ Transformation history\n",
      "   ‚úÖ Data quality scores\n",
      "   ‚úÖ Record-level quality flags\n",
      "‚úÖ Lineage tracking added to air_quality\n",
      "\n",
      "üìã Adding Data Lineage Tracking\n",
      "-----------------------------------\n",
      "üìä Lineage metadata added:\n",
      "   ‚úÖ Processing timestamp\n",
      "   ‚úÖ Transformation history\n",
      "   ‚úÖ Data quality scores\n",
      "   ‚úÖ Record-level quality flags\n",
      "‚úÖ Lineage tracking added to weather\n",
      "\n",
      "üìã Adding Data Lineage Tracking\n",
      "-----------------------------------\n",
      "üìä Lineage metadata added:\n",
      "   ‚úÖ Processing timestamp\n",
      "   ‚úÖ Transformation history\n",
      "   ‚úÖ Data quality scores\n",
      "   ‚úÖ Record-level quality flags\n",
      "‚úÖ Lineage tracking added to energy\n"
     ]
    }
   ],
   "source": [
    "# TODO: Add lineage to all processed datasets\n",
    "final_datasets = {}\n",
    "\n",
    "for name, df in standardized_datasets.items():\n",
    "    try:\n",
    "        # Create transformation history for this dataset\n",
    "        transformations = [\n",
    "            \"missing_data_analysis\",\n",
    "            \"outlier_detection\",\n",
    "            \"unit_standardization\"\n",
    "        ]\n",
    "        \n",
    "        # Calculate a simple quality score based on completeness\n",
    "        total_rows = df.count()\n",
    "        if total_rows > 0:\n",
    "            # Simplified quality score - you could make this more sophisticated\n",
    "            quality_score = 0.85  # Placeholder\n",
    "        else:\n",
    "            quality_score = 0.0\n",
    "        \n",
    "        final_df = add_data_lineage(df, transformations, quality_score)\n",
    "        final_datasets[name] = final_df\n",
    "        \n",
    "        print(f\"‚úÖ Lineage tracking added to {name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding lineage to {name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00b9d5",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# DAY 2 DELIVERABLES & VALIDATION\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "232fee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìã DAY 2 COMPLETION CHECKLIST\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã DAY 2 COMPLETION CHECKLIST\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de2b52c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ COMPLETION STATUS:\n",
      "   ‚úÖ Data Profiling Completed\n",
      "   ‚úÖ Sensor Health Analyzed\n",
      "   ‚úÖ Missing Data Patterns Identified\n",
      "   ‚úÖ Interpolation Implemented\n",
      "   ‚úÖ Outlier Detection Working\n",
      "   ‚úÖ Outlier Treatment Applied\n",
      "   ‚úÖ Units Standardized\n",
      "   ‚úÖ Data Lineage Tracked\n",
      "   ‚úÖ Quality Scores Calculated\n",
      "\n",
      "üìä Overall Completion: 100.0%\n",
      "üéâ Great progress! You're ready for Day 3!\n",
      "\n",
      "üìà KEY INSIGHTS FROM DAY 2:\n",
      "- Data quality patterns identified across sensors\n",
      "- Missing data handling strategies implemented\n",
      "- Outlier detection and treatment procedures established\n",
      "- Standardized data formats for consistent analysis\n"
     ]
    }
   ],
   "source": [
    "def validate_day2_completion():\n",
    "    \"\"\"Validate that Day 2 objectives have been met\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"data_profiling_completed\": False,\n",
    "        \"sensor_health_analyzed\": False,\n",
    "        \"missing_data_patterns_identified\": False,\n",
    "        \"interpolation_implemented\": False,\n",
    "        \"outlier_detection_working\": False,\n",
    "        \"outlier_treatment_applied\": False,\n",
    "        \"units_standardized\": False,\n",
    "        \"data_lineage_tracked\": False,\n",
    "        \"quality_scores_calculated\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if profiling was completed\n",
    "        if 'profiles' in globals() and len(profiles) > 0:\n",
    "            checklist[\"data_profiling_completed\"] = True\n",
    "            \n",
    "        # Check if sensor health analysis was done\n",
    "        if 'sensor_health_results' in globals() and len(sensor_health_results) > 0:\n",
    "            checklist[\"sensor_health_analyzed\"] = True\n",
    "            \n",
    "        # Check if missing data patterns were analyzed\n",
    "        if 'analyze_missing_patterns' in globals():\n",
    "            checklist[\"missing_data_patterns_identified\"] = True\n",
    "\n",
    "        # Check if interpolation was implemented AND executed\n",
    "        if 'interpolate_time_series_gaps' in globals() and 'traffic_interpolated' in globals():\n",
    "            checklist[\"interpolation_implemented\"] = True\n",
    "            \n",
    "        # Check if outlier detection was implemented\n",
    "        if 'outlier_results' in globals() and len(outlier_results) > 0:\n",
    "            checklist[\"outlier_detection_working\"] = True\n",
    "        \n",
    "        # Check if outlier treatment was applied\n",
    "        if 'treatment_results' in globals() and len(treatment_results) > 0:\n",
    "            checklist[\"outlier_treatment_applied\"] = True\n",
    "        \n",
    "        # Check if unit standardization was applied\n",
    "        if 'standardized_datasets' in globals() and len(standardized_datasets) > 0:\n",
    "            checklist[\"units_standardized\"] = True\n",
    "        \n",
    "        # Check if data lineage tracking was added\n",
    "        if 'final_datasets' in globals() and len(final_datasets) > 0:\n",
    "            checklist[\"data_lineage_tracked\"] = True\n",
    "        \n",
    "        # Check if quality scores were calculated\n",
    "        if (checklist[\"data_lineage_tracked\"]==True and\n",
    "            any('data_quality_score' in df.columns for df in final_datasets.values())) or \\\n",
    "           checklist[\"data_profiling_completed\"]==True or \\\n",
    "           (checklist[\"sensor_health_analyzed\"]==True and\n",
    "            any('health_score' in df.columns for df in sensor_health_results.values())):\n",
    "            checklist[\"quality_scores_calculated\"] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation error: {str(e)}\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"‚úÖ COMPLETION STATUS:\")\n",
    "    for item, status in checklist.items():\n",
    "        status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "        print(f\"   {status_icon} {item.replace('_', ' ').title()}\")\n",
    "    \n",
    "    import builtins\n",
    "    completion_rate = builtins.sum(checklist.values()) / len(checklist) * 100\n",
    "    print(f\"\\nüìä Overall Completion: {completion_rate:.1f}%\")\n",
    "    \n",
    "    if completion_rate >= 70:\n",
    "        print(\"üéâ Great progress! You're ready for Day 3!\")\n",
    "        print(\"\\nüìà KEY INSIGHTS FROM DAY 2:\")\n",
    "        print(\"- Data quality patterns identified across sensors\")\n",
    "        print(\"- Missing data handling strategies implemented\") \n",
    "        print(\"- Outlier detection and treatment procedures established\")\n",
    "        print(\"- Standardized data formats for consistent analysis\")\n",
    "    else:\n",
    "        print(\"üìù Please review incomplete items before proceeding to Day 3.\")\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# Run the validation\n",
    "completion_status = validate_day2_completion()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
