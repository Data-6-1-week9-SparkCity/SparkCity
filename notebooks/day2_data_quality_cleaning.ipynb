{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72b2eb0",
   "metadata": {},
   "source": [
    "# Day 2: Data Quality & Cleaning Pipeline\n",
    "# Smart City IoT Analytics Pipeline\n",
    "\n",
    "\"\"\"\n",
    "üéØ LEARNING OBJECTIVES:\n",
    "- Implement comprehensive data quality assessment\n",
    "- Design cleaning procedures for IoT sensor data\n",
    "- Handle missing values and outliers appropriately\n",
    "- Create reusable data quality functions\n",
    "\n",
    "üìÖ SCHEDULE:\n",
    "Morning (4 hours):\n",
    "1. Data Quality Assessment (2 hours)\n",
    "2. Missing Data Strategy (2 hours)\n",
    "\n",
    "Afternoon (4 hours):\n",
    "3. Outlier Detection & Treatment (2 hours)\n",
    "4. Data Standardization (2 hours)\n",
    "\n",
    "‚úÖ DELIVERABLES:\n",
    "- Data quality assessment report\n",
    "- Comprehensive cleaning pipeline\n",
    "- Outlier detection and treatment functions\n",
    "- Standardized datasets ready for analysis\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337c103",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79adbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Machine learning imports (for outlier detection)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d73575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/05 11:26:17 WARN Utils: Your hostname, Zipcoders-MacBook-Pro-3.local, resolves to a loopback address: 127.0.0.1; using 192.168.87.79 instead (on interface en0)\n",
      "25/09/05 11:26:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/05 11:26:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/05 11:26:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/09/05 11:26:18 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/09/05 11:26:18 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created new Spark session\n",
      "üîß Day 2: Data Quality & Cleaning Pipeline\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session (should already exist from Day 1)\n",
    "try:\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(\"‚úÖ Using existing Spark session\")\n",
    "except:\n",
    "    spark = (SparkSession.builder\n",
    "             .appName(\"SmartCityIoTPipeline-Day2\")\n",
    "             .master(\"local[*]\")\n",
    "             .config(\"spark.driver.memory\", \"4g\")\n",
    "             .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "             .getOrCreate())\n",
    "    print(\"‚úÖ Created new Spark session\")\n",
    "\n",
    "print(\"üîß Day 2: Data Quality & Cleaning Pipeline\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e4c99",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 1: COMPREHENSIVE DATA PROFILING (Morning - 2 hours)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f8d4d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä SECTION 1: COMPREHENSIVE DATA PROFILING\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä SECTION 1: COMPREHENSIVE DATA PROFILING\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0657fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All datasets loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load cleaned data from Day 1 (or reload if needed)\n",
    "data_dir = \"../data/raw\"\n",
    "\n",
    "# TODO 1.1: Load all datasets with error handling\n",
    "def load_all_datasets():\n",
    "    \"\"\"Load all sensor datasets with consistent error handling\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    try:\n",
    "        # Load each dataset\n",
    "        datasets['zones'] = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/city_zones.csv\")\n",
    "        datasets['traffic'] = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/traffic_sensors.csv\")\n",
    "        datasets['air_quality'] = spark.read.option(\"multiline\", \"true\").json(f\"{data_dir}/air_quality.json\")\n",
    "        datasets['weather'] = spark.read.parquet(f\"{data_dir}/weather_data.parquet\")\n",
    "        datasets['energy'] = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/energy_meters.csv\")\n",
    "        \n",
    "        # Convert timestamp columns to proper format\n",
    "        for name, df in datasets.items():\n",
    "            if name != 'zones' and 'timestamp' in df.columns:\n",
    "                datasets[name] = df.withColumn(\"timestamp\", F.to_timestamp(F.col(\"timestamp\")))\n",
    "        \n",
    "        print(\"‚úÖ All datasets loaded successfully\")\n",
    "        return datasets\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading datasets: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "datasets = load_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16fff8b",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 1.2: Advanced Data Quality Metrics (45 minutes)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49945abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting comprehensive data profiling...\n",
      "\n",
      "üîç Comprehensive Profile: zones\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "üìà Numeric Column Analysis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/05 11:26:25 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|summary|             lat_min|             lat_max|            lon_min|            lon_max|        population|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|  count|                   8|                   8|                  8|                  8|                 8|\n",
      "|   mean|  40.730000000000004|             40.7525| -73.99125000000001|          -73.97125|           21250.0|\n",
      "| stddev|0.023904572186687328|0.028157719063465373|0.02474873734153055|0.02474873734153458|14260.334598358582|\n",
      "|    min|                40.7|               40.72|             -74.02|              -74.0|              5000|\n",
      "|    max|               40.76|                40.8|             -73.96|             -73.94|             45000|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "\n",
      "\n",
      "üîç Comprehensive Profile: traffic\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "‚è∞ Temporal Analysis:\n",
      "   Time Range: 2025-08-29 09:08:21.865183 to 2025-09-05 09:08:21.865183\n",
      "   Records with timestamps: 100,850\n",
      "   Expected interval: 5 minutes\n",
      "üìÖ Data Freshness: Latest record is 2.3 hours old\n",
      "üìà Numeric Column Analysis:\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|summary|        location_lat|       location_lon|     vehicle_count|         avg_speed|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|  count|              100850|             100850|            100850|            100850|\n",
      "|   mean|   40.75500412062866| -73.96322662910207|22.542786316311354| 45.64098785926878|\n",
      "| stddev|0.029220190304060856|0.03369872817690664|13.839913187144107|17.011286205956203|\n",
      "|    min|   40.70081285057606|  -74.0179577354858|                 0|               5.0|\n",
      "|    max|   40.79941290676354| -73.90685413438007|                90| 119.2368051238285|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "\n",
      "üìÇ Categorical Column Analysis:\n",
      "   sensor_id: 50 distinct values\n",
      "   congestion_level: 3 distinct values\n",
      "      Top values:\n",
      "+----------------+-----+\n",
      "|congestion_level|count|\n",
      "+----------------+-----+\n",
      "|medium          |63782|\n",
      "|low             |20053|\n",
      "|high            |17015|\n",
      "+----------------+-----+\n",
      "\n",
      "   road_type: 4 distinct values\n",
      "      Top values:\n",
      "+-----------+-----+\n",
      "|road_type  |count|\n",
      "+-----------+-----+\n",
      "|highway    |30255|\n",
      "|commercial |28238|\n",
      "|arterial   |26221|\n",
      "|residential|16136|\n",
      "+-----------+-----+\n",
      "\n",
      "\n",
      "üîç Comprehensive Profile: air_quality\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "‚è∞ Temporal Analysis:\n",
      "   Time Range: 2025-08-29 09:08:21.865183 to 2025-09-05 09:08:21.865183\n",
      "   Records with timestamps: 13,460\n",
      "   Expected interval: 15 minutes\n",
      "üìÖ Data Freshness: Latest record is 2.3 hours old\n",
      "üìà Numeric Column Analysis:\n",
      "+-------+------------------+------------------+-------------------+-------------------+------------------+------------------+-----------------+-------------------+\n",
      "|summary|                co|          humidity|       location_lat|       location_lon|               no2|              pm10|             pm25|        temperature|\n",
      "+-------+------------------+------------------+-------------------+-------------------+------------------+------------------+-----------------+-------------------+\n",
      "|  count|             13460|             13460|              13460|              13460|             13460|             13460|            13460|              13460|\n",
      "|   mean|1.2875130384289266|54.826182260494996| 40.745297971528956| -73.95941349710336|  32.1504618354429| 42.80334884664813|26.97403312381407| 19.920914396221626|\n",
      "| stddev|0.4308440673649061|14.332142580372237|0.03200395142024157|0.03462471085758109|10.853107898470956|13.174128864512817|8.650390737615393|  8.031755472044066|\n",
      "|    min|               0.0| 30.00292899016368|  40.70135724630687| -74.01714036414015|               0.0|               0.0|              0.0|-13.491271650999714|\n",
      "|    max|3.0235870023107405| 79.98913254535276|  40.79520347439802| -73.90111712768562| 79.89717615532693| 90.96201589939444|57.89319558810176|  50.52690941361086|\n",
      "+-------+------------------+------------------+-------------------+-------------------+------------------+------------------+-----------------+-------------------+\n",
      "\n",
      "üìÇ Categorical Column Analysis:\n",
      "   sensor_id: 20 distinct values\n",
      "\n",
      "üîç Comprehensive Profile: weather\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "‚è∞ Temporal Analysis:\n",
      "   Time Range: 2025-08-29 09:08:21.865183 to 2025-09-05 09:08:21.865183\n",
      "   Records with timestamps: 3,370\n",
      "   Expected interval: 30 minutes\n",
      "üìÖ Data Freshness: Latest record is 2.3 hours old\n",
      "üìà Numeric Column Analysis:\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+--------------------+-------------------+------------------+\n",
      "|summary|        location_lat|       location_lon|       temperature|          humidity|          wind_speed|      wind_direction|      precipitation|          pressure|\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+--------------------+-------------------+------------------+\n",
      "|  count|                3370|               3370|              3370|              3370|                3370|                3370|               3370|              3370|\n",
      "|   mean|   40.75513648400018| -73.93923096074455|19.911912653214014| 59.87100917239904|   7.929256132440461|  180.97061373578057|0.05306158454425665| 1013.136207608922|\n",
      "| stddev|0.028984609103931636|0.03123274588108529| 3.430067546762053|15.013023289553832|   7.952782342079477|  104.86450288055956|0.22478625520085693|10.103309351985942|\n",
      "|    min|   40.71333337560931| -74.00624363114778| 9.314022303879696|              20.0|6.265628498756273E-5|0.038498855443562796|                0.0| 978.4432151391723|\n",
      "|    max|   40.79330117098755| -73.90324777011922|  33.3500104717534|             100.0|   69.98453785200658|  359.85559901535805|  2.844568829313417| 1051.576041089785|\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+--------------------+-------------------+------------------+\n",
      "\n",
      "üìÇ Categorical Column Analysis:\n",
      "   station_id: 10 distinct values\n",
      "      Top values:\n",
      "+-----------+-----+\n",
      "|station_id |count|\n",
      "+-----------+-----+\n",
      "|WEATHER_002|337  |\n",
      "|WEATHER_004|337  |\n",
      "|WEATHER_006|337  |\n",
      "|WEATHER_005|337  |\n",
      "|WEATHER_001|337  |\n",
      "+-----------+-----+\n",
      "\n",
      "\n",
      "üîç Comprehensive Profile: energy\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "‚è∞ Temporal Analysis:\n",
      "   Time Range: 2025-08-29 09:08:21.865183 to 2025-09-05 09:08:21.865183\n",
      "   Records with timestamps: 201,800\n",
      "   Expected interval: 10 minutes\n",
      "üìÖ Data Freshness: Latest record is 2.3 hours old\n",
      "üìà Numeric Column Analysis:\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|summary|       location_lat|       location_lon| power_consumption|           voltage|           current|        power_factor|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|             201800|             201800|            201800|            201800|            201800|              201800|\n",
      "|   mean|  40.75242724328286|  -73.9627357967259| 18.26316099880441|240.00823206210796| 76.12580243588519|   0.900013156309099|\n",
      "| stddev|0.03053636528449424|0.03380770173714468|18.578138934626217| 4.995942695194844| 77.46911203777647|0.028857755619309342|\n",
      "|    min|  40.70069358761313|  -74.0198187693212| 1.680094147697381|216.46925988462675| 6.648289482591334|  0.8500000747623474|\n",
      "|    max|  40.79948014467343| -73.90067425664958| 64.99856411105199|  263.928189380369|290.46087984351004|   0.949999711719659|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "\n",
      "üìÇ Categorical Column Analysis:\n",
      "   meter_id: 200 distinct values\n",
      "   building_type: 5 distinct values\n",
      "      Top values:\n",
      "+-------------+-----+\n",
      "|building_type|count|\n",
      "+-------------+-----+\n",
      "|office       |48432|\n",
      "|commercial   |45405|\n",
      "|retail       |36324|\n",
      "|industrial   |36324|\n",
      "|residential  |35315|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "üéØ TASK: Create comprehensive data quality profiling functions\n",
    "üí° HINT: Look beyond basic missing values - consider temporal patterns, distributions\n",
    "üìö CONCEPTS: Data profiling, quality metrics, statistical validation\n",
    "\"\"\"\n",
    "\n",
    "def comprehensive_data_profile(df, dataset_name, time_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive data quality profile\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to profile\n",
    "        dataset_name: Name for reporting\n",
    "        time_col: Timestamp column name (can be None for non-time series data)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with quality metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Comprehensive Profile: {dataset_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_rows = df.count()\n",
    "    total_cols = len(df.columns)\n",
    "    \n",
    "    profile = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'total_rows': total_rows,\n",
    "        'total_columns': total_cols,\n",
    "        'memory_usage_mb': 0,  # Estimate\n",
    "        'quality_issues': []\n",
    "    }\n",
    "    \n",
    "    # TODO: Calculate missing value patterns\n",
    "    print(\"üìã Missing Value Analysis:\")\n",
    "    missing_analysis = {}\n",
    "    for col in df.columns:\n",
    "        missing_count = df.filter(F.col(col).isNull()).count()\n",
    "        missing_pct = (missing_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "        missing_analysis[col] = {'count': missing_count, 'percentage': missing_pct}\n",
    "        \n",
    "        if missing_pct > 5:  # Flag columns with >5% missing\n",
    "            profile['quality_issues'].append(f\"High missing values in {col}: {missing_pct:.2f}%\")\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            print(f\"   {col}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    profile['missing_analysis'] = missing_analysis\n",
    "    \n",
    "    # TODO: Temporal data gaps (if timestamp column exists and is not None)\n",
    "    if time_col and time_col in df.columns:\n",
    "        print(\"‚è∞ Temporal Analysis:\")\n",
    "        \n",
    "        # Get time range\n",
    "        time_stats = df.agg(\n",
    "            F.min(time_col).alias('min_time'),\n",
    "            F.max(time_col).alias('max_time'),\n",
    "            F.count(time_col).alias('time_count')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"   Time Range: {time_stats['min_time']} to {time_stats['max_time']}\")\n",
    "        print(f\"   Records with timestamps: {time_stats['time_count']:,}\")\n",
    "        \n",
    "        # TODO: Check for temporal gaps\n",
    "        # Calculate expected vs actual record counts\n",
    "        if dataset_name == 'traffic':\n",
    "            expected_interval_minutes = 5\n",
    "        elif dataset_name == 'air_quality':\n",
    "            expected_interval_minutes = 15\n",
    "        elif dataset_name == 'weather':\n",
    "            expected_interval_minutes = 30\n",
    "        elif dataset_name == 'energy':\n",
    "            expected_interval_minutes = 10\n",
    "        else:\n",
    "            expected_interval_minutes = 15\n",
    "        \n",
    "        print(f\"   Expected interval: {expected_interval_minutes} minutes\")\n",
    "        \n",
    "        # TODO: Data freshness (for time series data)\n",
    "        latest_record = df.agg(F.max(time_col).alias('latest')).collect()[0]['latest']\n",
    "        if latest_record:\n",
    "            from datetime import datetime\n",
    "            hours_old = (datetime.now() - latest_record).total_seconds() / 3600\n",
    "            print(f\"üìÖ Data Freshness: Latest record is {hours_old:.1f} hours old\")\n",
    "    \n",
    "    # TODO: Numeric column distributions\n",
    "    numeric_cols = [field.name for field in df.schema.fields \n",
    "                   if field.dataType in [IntegerType(), DoubleType(), FloatType(), LongType()]]\n",
    "    \n",
    "    if numeric_cols:\n",
    "        print(\"üìà Numeric Column Analysis:\")\n",
    "        # Get basic statistics for numeric columns\n",
    "        stats_df = df.select(numeric_cols).describe()\n",
    "        stats_df.show()\n",
    "        \n",
    "        # TODO: Check for suspicious patterns in numeric data\n",
    "        for col in numeric_cols:\n",
    "            if col not in ['location_lat', 'location_lon']:\n",
    "                # Check for columns with very low variance (potentially stuck sensors)\n",
    "                variance_check = df.agg(F.variance(col).alias('variance')).collect()[0]['variance']\n",
    "                if variance_check is not None and variance_check < 0.001:\n",
    "                    profile['quality_issues'].append(f\"Very low variance in {col}: {variance_check}\")\n",
    "    \n",
    "    # TODO: Categorical column analysis\n",
    "    categorical_cols = [field.name for field in df.schema.fields \n",
    "                       if field.dataType == StringType() and field.name not in [time_col] if time_col]\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(\"üìÇ Categorical Column Analysis:\")\n",
    "        for col in categorical_cols:\n",
    "            distinct_count = df.select(col).distinct().count()\n",
    "            print(f\"   {col}: {distinct_count} distinct values\")\n",
    "            \n",
    "            # Show top values\n",
    "            if distinct_count < 20:\n",
    "                top_values = df.groupBy(col).count().orderBy(F.desc(\"count\")).limit(5)\n",
    "                print(f\"      Top values:\")\n",
    "                top_values.show(5, truncate=False)\n",
    "    \n",
    "    # TODO: Check for duplicate records\n",
    "    duplicate_count = total_rows - df.dropDuplicates().count()\n",
    "    if duplicate_count > 0:\n",
    "        profile['quality_issues'].append(f\"Duplicate records found: {duplicate_count}\")\n",
    "        print(f\"üîÑ Duplicate Records: {duplicate_count:,}\")\n",
    "    \n",
    "    return profile\n",
    "\n",
    "      \n",
    "\n",
    "# TODO: Profile all datasets\n",
    "print(\"üîç Starting comprehensive data profiling...\")\n",
    "\n",
    "profiles = {}\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        try:\n",
    "            # Check if the dataset has a timestamp column\n",
    "            if name != 'zones' and 'timestamp' in df.columns:\n",
    "                profiles[name] = comprehensive_data_profile(df, name, time_col=\"timestamp\")\n",
    "            else:\n",
    "                # For datasets without timestamp column (like zones), don't pass time_col\n",
    "                profiles[name] = comprehensive_data_profile(df, name, time_col=None)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error profiling {name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d6f93",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 1.3: Sensor Health Analysis (45 minutes)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473a8028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè• Analyzing sensor health across all datasets...\n",
      "\n",
      "üè• Sensor Health Analysis\n",
      "------------------------------\n",
      "üöó Traffic Sensor Health Summary:\n",
      "+-------+-----+\n",
      "| status|count|\n",
      "+-------+-----+\n",
      "|healthy|   50|\n",
      "+-------+-----+\n",
      "\n",
      "\n",
      "üè• Sensor Health Analysis\n",
      "------------------------------\n",
      "üå´Ô∏è Air Quality Sensor Health Summary:\n",
      "+-------+-----+\n",
      "| status|count|\n",
      "+-------+-----+\n",
      "|healthy|   20|\n",
      "+-------+-----+\n",
      "\n",
      "\n",
      "üè• Sensor Health Analysis\n",
      "------------------------------\n",
      "üå¶Ô∏è Weather Sensor Health Summary:\n",
      "+-------+-----+\n",
      "| status|count|\n",
      "+-------+-----+\n",
      "|healthy|   10|\n",
      "+-------+-----+\n",
      "\n",
      "\n",
      "üè• Sensor Health Analysis\n",
      "------------------------------\n",
      "‚ö° Energy Sensor Health Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| status|count|\n",
      "+-------+-----+\n",
      "|healthy|  200|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "üéØ TASK: Identify sensors with potential operational issues\n",
    "üí° HINT: Look for patterns that indicate sensor malfunctions\n",
    "üìö CONCEPTS: Sensor diagnostics, operational monitoring, health scoring\n",
    "\"\"\"\n",
    "\n",
    "def analyze_sensor_health(df, sensor_id_col, value_cols, time_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Analyze individual sensor health and identify problematic sensors\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with sensor data\n",
    "        sensor_id_col: Column name for sensor ID\n",
    "        value_cols: List of measurement columns to analyze\n",
    "        time_col: Timestamp column\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with sensor health metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nüè• Sensor Health Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # TODO: Calculate health metrics per sensor\n",
    "    window_spec = Window.partitionBy(sensor_id_col)\n",
    "    \n",
    "    health_metrics = df.groupBy(sensor_id_col).agg(\n",
    "        F.count(\"*\").alias(\"total_readings\"),\n",
    "        F.min(time_col).alias(\"first_reading\"),\n",
    "        F.max(time_col).alias(\"last_reading\")\n",
    "    )\n",
    "    \n",
    "    # TODO: Add missing data percentage per sensor\n",
    "    for col in value_cols:\n",
    "        missing_col_name = f\"{col}_missing_pct\"\n",
    "        \n",
    "        # Calculate missing percentage per sensor using separate aggregation\n",
    "        missing_stats = df.groupBy(sensor_id_col).agg(\n",
    "            F.count(\"*\").alias(\"total_count\"),\n",
    "            F.sum(F.when(F.col(col).isNull(), 1).otherwise(0)).alias(\"missing_count\")\n",
    "        ).withColumn(\n",
    "            missing_col_name,\n",
    "            (F.col(\"missing_count\") / F.col(\"total_count\") * 100).cast(\"double\")\n",
    "        ).select(sensor_id_col, missing_col_name)\n",
    "        \n",
    "        # Join the missing percentage back to health_metrics\n",
    "        health_metrics = health_metrics.join(missing_stats, sensor_id_col, \"left\")\n",
    "\n",
    "    # TODO: Add variance analysis (detect stuck sensors)\n",
    "    for col in value_cols:\n",
    "        if col in df.columns:\n",
    "            variance_col_name = f\"{col}_variance\"\n",
    "            sensor_variance = df.groupBy(sensor_id_col).agg(\n",
    "                F.variance(col).alias(variance_col_name)\n",
    "            )\n",
    "            health_metrics = health_metrics.join(sensor_variance, sensor_id_col, \"left\")\n",
    "    \n",
    "    # TODO: Calculate data gaps (irregular reporting)\n",
    "    # This is more complex - calculate time differences between consecutive readings\n",
    "    if time_col in df.columns:\n",
    "        time_diff_col = \"time_diff_minutes\"\n",
    "        df_with_lag = df.withColumn(\n",
    "            \"prev_time\",\n",
    "            F.lag(time_col).over(Window.partitionBy(sensor_id_col).orderBy(time_col))\n",
    "        ).withColumn(\n",
    "            time_diff_col,\n",
    "            (F.unix_timestamp(time_col) - F.unix_timestamp(\"prev_time\")) / 60\n",
    "        )\n",
    "        \n",
    "        avg_time_diff = df_with_lag.groupBy(sensor_id_col).agg(\n",
    "            F.avg(time_diff_col).alias(\"avg_time_diff_minutes\"),\n",
    "            F.max(time_diff_col).alias(\"max_time_diff_minutes\")\n",
    "        )\n",
    "        \n",
    "        health_metrics = health_metrics.join(avg_time_diff, sensor_id_col, \"left\")\n",
    "    \n",
    "    # TODO: Create overall health score\n",
    "    # Combine multiple factors into a single health score (0-100)\n",
    "    first_value_col = value_cols[0] if value_cols else \"vehicle_count\"\n",
    "    missing_pct_col = f\"{first_value_col}_missing_pct\"\n",
    "    variance_col = f\"{first_value_col}_variance\"\n",
    "    \n",
    "    health_metrics = health_metrics.withColumn(\n",
    "        \"health_score\",\n",
    "        # TODO: Create a formula that combines:\n",
    "        # - Missing data percentage (lower is better)\n",
    "        # - Data variance (too low = stuck sensor, too high = noisy sensor)\n",
    "        # - Reporting regularity\n",
    "        # - Recent data availability\n",
    "        F.greatest(\n",
    "            F.lit(0.0),\n",
    "            F.least(\n",
    "                F.lit(100.0),\n",
    "                # Start with 100 and subtract penalties\n",
    "                F.lit(100.0) -\n",
    "                # Penalty for missing data (0-40 points lost)\n",
    "                F.coalesce(F.col(missing_pct_col), F.lit(0.0)) * 0.4 -\n",
    "                # Penalty for irregular reporting (0-30 points lost)\n",
    "                F.when(F.col(\"avg_time_diff_minutes\").isNull(), F.lit(0.0))\n",
    "                 .when(F.col(\"avg_time_diff_minutes\") > 60, F.lit(30.0))  # >1 hour gaps\n",
    "                 .when(F.col(\"avg_time_diff_minutes\") > 30, F.lit(15.0))  # >30 min gaps\n",
    "                 .when(F.col(\"avg_time_diff_minutes\") > 15, F.lit(5.0))   # >15 min gaps\n",
    "                 .otherwise(F.lit(0.0)) -\n",
    "                # Penalty for very low variance (stuck sensors) (0-20 points lost)\n",
    "                F.when(F.coalesce(F.col(variance_col), F.lit(1.0)) < 0.001, F.lit(20.0))\n",
    "                 .otherwise(F.lit(0.0)) -\n",
    "                # Penalty for very high variance (noisy sensors) (0-10 points lost)\n",
    "                F.when(F.coalesce(F.col(variance_col), F.lit(0.0)) > 1000, F.lit(10.0))\n",
    "                 .otherwise(F.lit(0.0))\n",
    "            )\n",
    "        ).cast(\"double\")\n",
    "        \n",
    "    )\n",
    "    \n",
    "    # TODO: Flag problematic sensors\n",
    "    health_metrics = health_metrics.withColumn(\n",
    "        \"status\",\n",
    "        F.when(F.col(\"health_score\") > 80, \"healthy\")\n",
    "         .when(F.col(\"health_score\") > 60, \"warning\")\n",
    "         .otherwise(\"critical\")\n",
    "    )\n",
    "    \n",
    "    return health_metrics\n",
    "\n",
    "# TODO: Analyze health for each sensor type\n",
    "print(\"üè• Analyzing sensor health across all datasets...\")\n",
    "\n",
    "sensor_health_results = {}\n",
    "\n",
    "# Traffic sensors\n",
    "if 'traffic' in datasets:\n",
    "    traffic_health = analyze_sensor_health(\n",
    "        datasets['traffic'], \n",
    "        'sensor_id', \n",
    "        ['vehicle_count', 'avg_speed']\n",
    "    )\n",
    "    sensor_health_results['traffic'] = traffic_health\n",
    "    \n",
    "    print(\"üöó Traffic Sensor Health Summary:\")\n",
    "    traffic_health.groupBy(\"status\").count().show()\n",
    "\n",
    "# TODO: Analyze other sensor types\n",
    "# Air quality sensors\n",
    "if 'air_quality' in datasets:\n",
    "    # TODO: Implement air quality sensor health analysis\n",
    "    air_quality_health=analyze_sensor_health(\n",
    "        datasets['air_quality'], \n",
    "        'sensor_id', \n",
    "        ['pm25', 'pm10', 'no2', 'co']\n",
    "    )\n",
    "    sensor_health_results['air_quality'] = air_quality_health\n",
    "    print(\"üå´Ô∏è Air Quality Sensor Health Summary:\")\n",
    "    air_quality_health.groupBy(\"status\").count().show()\n",
    "\n",
    "# Weather sensors  \n",
    "if 'weather' in datasets:\n",
    "    # TODO: Implement weather sensor health analysis\n",
    "    weather_health=analyze_sensor_health(\n",
    "        datasets['weather'], \n",
    "        'station_id', \n",
    "        ['temperature', 'humidity', 'wind_speed', 'precipitation']\n",
    "    )\n",
    "    sensor_health_results['weather'] = weather_health\n",
    "    print(\"üå¶Ô∏è Weather Sensor Health Summary:\")\n",
    "    weather_health.groupBy(\"status\").count().show()\n",
    "\n",
    "# Energy sensors\n",
    "if 'energy' in datasets:\n",
    "    # TODO: Implement energy sensor health analysis\n",
    "    energy_health=analyze_sensor_health(\n",
    "        datasets['energy'], \n",
    "        'meter_id', \n",
    "        ['power_consumption', 'voltage', 'current', 'power_factor']\n",
    "    )\n",
    "    sensor_health_results['energy'] = energy_health\n",
    "    print(\"‚ö° Energy Sensor Health Summary:\")\n",
    "    energy_health.groupBy(\"status\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7576e",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 2: MISSING DATA STRATEGY (Morning - 2 hours)\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a67e935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üï≥Ô∏è SECTION 2: MISSING DATA HANDLING STRATEGY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üï≥Ô∏è SECTION 2: MISSING DATA HANDLING STRATEGY\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e4f972",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 2.1: Missing Data Pattern Analysis (60 minutes)\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e9fcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Missing Data Pattern Analysis\n",
      "----------------------------------------\n",
      "‚è∞ Temporal Missing Data Patterns:\n",
      "\n",
      "   Missing data by hour for location_lat:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|         4200|            0|        0.0|\n",
      "|   1|         4200|            0|        0.0|\n",
      "|   2|         4200|            0|        0.0|\n",
      "|   3|         4200|            0|        0.0|\n",
      "|   4|         4200|            0|        0.0|\n",
      "|   5|         4200|            0|        0.0|\n",
      "|   6|         4200|            0|        0.0|\n",
      "|   7|         4200|            0|        0.0|\n",
      "|   8|         4200|            0|        0.0|\n",
      "|   9|         4250|            0|        0.0|\n",
      "|  10|         4200|            0|        0.0|\n",
      "|  11|         4200|            0|        0.0|\n",
      "|  12|         4200|            0|        0.0|\n",
      "|  13|         4200|            0|        0.0|\n",
      "|  14|         4200|            0|        0.0|\n",
      "|  15|         4200|            0|        0.0|\n",
      "|  16|         4200|            0|        0.0|\n",
      "|  17|         4200|            0|        0.0|\n",
      "|  18|         4200|            0|        0.0|\n",
      "|  19|         4200|            0|        0.0|\n",
      "|  20|         4200|            0|        0.0|\n",
      "|  21|         4200|            0|        0.0|\n",
      "|  22|         4200|            0|        0.0|\n",
      "|  23|         4200|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for location_lon:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|         4200|            0|        0.0|\n",
      "|   1|         4200|            0|        0.0|\n",
      "|   2|         4200|            0|        0.0|\n",
      "|   3|         4200|            0|        0.0|\n",
      "|   4|         4200|            0|        0.0|\n",
      "|   5|         4200|            0|        0.0|\n",
      "|   6|         4200|            0|        0.0|\n",
      "|   7|         4200|            0|        0.0|\n",
      "|   8|         4200|            0|        0.0|\n",
      "|   9|         4250|            0|        0.0|\n",
      "|  10|         4200|            0|        0.0|\n",
      "|  11|         4200|            0|        0.0|\n",
      "|  12|         4200|            0|        0.0|\n",
      "|  13|         4200|            0|        0.0|\n",
      "|  14|         4200|            0|        0.0|\n",
      "|  15|         4200|            0|        0.0|\n",
      "|  16|         4200|            0|        0.0|\n",
      "|  17|         4200|            0|        0.0|\n",
      "|  18|         4200|            0|        0.0|\n",
      "|  19|         4200|            0|        0.0|\n",
      "|  20|         4200|            0|        0.0|\n",
      "|  21|         4200|            0|        0.0|\n",
      "|  22|         4200|            0|        0.0|\n",
      "|  23|         4200|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for vehicle_count:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|         4200|            0|        0.0|\n",
      "|   1|         4200|            0|        0.0|\n",
      "|   2|         4200|            0|        0.0|\n",
      "|   3|         4200|            0|        0.0|\n",
      "|   4|         4200|            0|        0.0|\n",
      "|   5|         4200|            0|        0.0|\n",
      "|   6|         4200|            0|        0.0|\n",
      "|   7|         4200|            0|        0.0|\n",
      "|   8|         4200|            0|        0.0|\n",
      "|   9|         4250|            0|        0.0|\n",
      "|  10|         4200|            0|        0.0|\n",
      "|  11|         4200|            0|        0.0|\n",
      "|  12|         4200|            0|        0.0|\n",
      "|  13|         4200|            0|        0.0|\n",
      "|  14|         4200|            0|        0.0|\n",
      "|  15|         4200|            0|        0.0|\n",
      "|  16|         4200|            0|        0.0|\n",
      "|  17|         4200|            0|        0.0|\n",
      "|  18|         4200|            0|        0.0|\n",
      "|  19|         4200|            0|        0.0|\n",
      "|  20|         4200|            0|        0.0|\n",
      "|  21|         4200|            0|        0.0|\n",
      "|  22|         4200|            0|        0.0|\n",
      "|  23|         4200|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "üì° Sensor-specific Missing Patterns:\n",
      "   Sensors with highest missing data:\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "|  sensor_id|total_readings|location_lat_missing|location_lon_missing|location_lat_missing_pct|location_lon_missing_pct|\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "|TRAFFIC_004|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_006|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_009|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_017|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_035|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_003|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_011|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_031|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_007|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "|TRAFFIC_040|          2017|                   0|                   0|                     0.0|                     0.0|\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "üîç Missing Data Pattern Analysis\n",
      "----------------------------------------\n",
      "‚è∞ Temporal Missing Data Patterns:\n",
      "\n",
      "   Missing data by hour for co:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|          560|            0|        0.0|\n",
      "|   1|          560|            0|        0.0|\n",
      "|   2|          560|            0|        0.0|\n",
      "|   3|          560|            0|        0.0|\n",
      "|   4|          560|            0|        0.0|\n",
      "|   5|          560|            0|        0.0|\n",
      "|   6|          560|            0|        0.0|\n",
      "|   7|          560|            0|        0.0|\n",
      "|   8|          560|            0|        0.0|\n",
      "|   9|          580|            0|        0.0|\n",
      "|  10|          560|            0|        0.0|\n",
      "|  11|          560|            0|        0.0|\n",
      "|  12|          560|            0|        0.0|\n",
      "|  13|          560|            0|        0.0|\n",
      "|  14|          560|            0|        0.0|\n",
      "|  15|          560|            0|        0.0|\n",
      "|  16|          560|            0|        0.0|\n",
      "|  17|          560|            0|        0.0|\n",
      "|  18|          560|            0|        0.0|\n",
      "|  19|          560|            0|        0.0|\n",
      "|  20|          560|            0|        0.0|\n",
      "|  21|          560|            0|        0.0|\n",
      "|  22|          560|            0|        0.0|\n",
      "|  23|          560|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for humidity:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|          560|            0|        0.0|\n",
      "|   1|          560|            0|        0.0|\n",
      "|   2|          560|            0|        0.0|\n",
      "|   3|          560|            0|        0.0|\n",
      "|   4|          560|            0|        0.0|\n",
      "|   5|          560|            0|        0.0|\n",
      "|   6|          560|            0|        0.0|\n",
      "|   7|          560|            0|        0.0|\n",
      "|   8|          560|            0|        0.0|\n",
      "|   9|          580|            0|        0.0|\n",
      "|  10|          560|            0|        0.0|\n",
      "|  11|          560|            0|        0.0|\n",
      "|  12|          560|            0|        0.0|\n",
      "|  13|          560|            0|        0.0|\n",
      "|  14|          560|            0|        0.0|\n",
      "|  15|          560|            0|        0.0|\n",
      "|  16|          560|            0|        0.0|\n",
      "|  17|          560|            0|        0.0|\n",
      "|  18|          560|            0|        0.0|\n",
      "|  19|          560|            0|        0.0|\n",
      "|  20|          560|            0|        0.0|\n",
      "|  21|          560|            0|        0.0|\n",
      "|  22|          560|            0|        0.0|\n",
      "|  23|          560|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for location_lat:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|          560|            0|        0.0|\n",
      "|   1|          560|            0|        0.0|\n",
      "|   2|          560|            0|        0.0|\n",
      "|   3|          560|            0|        0.0|\n",
      "|   4|          560|            0|        0.0|\n",
      "|   5|          560|            0|        0.0|\n",
      "|   6|          560|            0|        0.0|\n",
      "|   7|          560|            0|        0.0|\n",
      "|   8|          560|            0|        0.0|\n",
      "|   9|          580|            0|        0.0|\n",
      "|  10|          560|            0|        0.0|\n",
      "|  11|          560|            0|        0.0|\n",
      "|  12|          560|            0|        0.0|\n",
      "|  13|          560|            0|        0.0|\n",
      "|  14|          560|            0|        0.0|\n",
      "|  15|          560|            0|        0.0|\n",
      "|  16|          560|            0|        0.0|\n",
      "|  17|          560|            0|        0.0|\n",
      "|  18|          560|            0|        0.0|\n",
      "|  19|          560|            0|        0.0|\n",
      "|  20|          560|            0|        0.0|\n",
      "|  21|          560|            0|        0.0|\n",
      "|  22|          560|            0|        0.0|\n",
      "|  23|          560|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "üì° Sensor-specific Missing Patterns:\n",
      "   Sensors with highest missing data:\n",
      "+---------+--------------+----------+----------------+--------------+--------------------+\n",
      "|sensor_id|total_readings|co_missing|humidity_missing|co_missing_pct|humidity_missing_pct|\n",
      "+---------+--------------+----------+----------------+--------------+--------------------+\n",
      "|   AQ_003|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_016|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_012|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_002|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_004|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_006|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_009|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_015|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_010|           673|         0|               0|           0.0|                 0.0|\n",
      "|   AQ_013|           673|         0|               0|           0.0|                 0.0|\n",
      "+---------+--------------+----------+----------------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "üîç Missing Data Pattern Analysis\n",
      "----------------------------------------\n",
      "‚è∞ Temporal Missing Data Patterns:\n",
      "\n",
      "   Missing data by hour for location_lat:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|          140|            0|        0.0|\n",
      "|   1|          140|            0|        0.0|\n",
      "|   2|          140|            0|        0.0|\n",
      "|   3|          140|            0|        0.0|\n",
      "|   4|          140|            0|        0.0|\n",
      "|   5|          140|            0|        0.0|\n",
      "|   6|          140|            0|        0.0|\n",
      "|   7|          140|            0|        0.0|\n",
      "|   8|          140|            0|        0.0|\n",
      "|   9|          150|            0|        0.0|\n",
      "|  10|          140|            0|        0.0|\n",
      "|  11|          140|            0|        0.0|\n",
      "|  12|          140|            0|        0.0|\n",
      "|  13|          140|            0|        0.0|\n",
      "|  14|          140|            0|        0.0|\n",
      "|  15|          140|            0|        0.0|\n",
      "|  16|          140|            0|        0.0|\n",
      "|  17|          140|            0|        0.0|\n",
      "|  18|          140|            0|        0.0|\n",
      "|  19|          140|            0|        0.0|\n",
      "|  20|          140|            0|        0.0|\n",
      "|  21|          140|            0|        0.0|\n",
      "|  22|          140|            0|        0.0|\n",
      "|  23|          140|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for location_lon:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|          140|            0|        0.0|\n",
      "|   1|          140|            0|        0.0|\n",
      "|   2|          140|            0|        0.0|\n",
      "|   3|          140|            0|        0.0|\n",
      "|   4|          140|            0|        0.0|\n",
      "|   5|          140|            0|        0.0|\n",
      "|   6|          140|            0|        0.0|\n",
      "|   7|          140|            0|        0.0|\n",
      "|   8|          140|            0|        0.0|\n",
      "|   9|          150|            0|        0.0|\n",
      "|  10|          140|            0|        0.0|\n",
      "|  11|          140|            0|        0.0|\n",
      "|  12|          140|            0|        0.0|\n",
      "|  13|          140|            0|        0.0|\n",
      "|  14|          140|            0|        0.0|\n",
      "|  15|          140|            0|        0.0|\n",
      "|  16|          140|            0|        0.0|\n",
      "|  17|          140|            0|        0.0|\n",
      "|  18|          140|            0|        0.0|\n",
      "|  19|          140|            0|        0.0|\n",
      "|  20|          140|            0|        0.0|\n",
      "|  21|          140|            0|        0.0|\n",
      "|  22|          140|            0|        0.0|\n",
      "|  23|          140|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for temperature:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|          140|            0|        0.0|\n",
      "|   1|          140|            0|        0.0|\n",
      "|   2|          140|            0|        0.0|\n",
      "|   3|          140|            0|        0.0|\n",
      "|   4|          140|            0|        0.0|\n",
      "|   5|          140|            0|        0.0|\n",
      "|   6|          140|            0|        0.0|\n",
      "|   7|          140|            0|        0.0|\n",
      "|   8|          140|            0|        0.0|\n",
      "|   9|          150|            0|        0.0|\n",
      "|  10|          140|            0|        0.0|\n",
      "|  11|          140|            0|        0.0|\n",
      "|  12|          140|            0|        0.0|\n",
      "|  13|          140|            0|        0.0|\n",
      "|  14|          140|            0|        0.0|\n",
      "|  15|          140|            0|        0.0|\n",
      "|  16|          140|            0|        0.0|\n",
      "|  17|          140|            0|        0.0|\n",
      "|  18|          140|            0|        0.0|\n",
      "|  19|          140|            0|        0.0|\n",
      "|  20|          140|            0|        0.0|\n",
      "|  21|          140|            0|        0.0|\n",
      "|  22|          140|            0|        0.0|\n",
      "|  23|          140|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "üì° Sensor-specific Missing Patterns:\n",
      "   Sensors with highest missing data:\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "| station_id|total_readings|location_lat_missing|location_lon_missing|location_lat_missing_pct|location_lon_missing_pct|\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "|WEATHER_002|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_004|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_006|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_005|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_001|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_008|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_009|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_003|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_007|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "|WEATHER_010|           337|                   0|                   0|                     0.0|                     0.0|\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "üîç Missing Data Pattern Analysis\n",
      "----------------------------------------\n",
      "‚è∞ Temporal Missing Data Patterns:\n",
      "\n",
      "   Missing data by hour for location_lat:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|         8400|            0|        0.0|\n",
      "|   1|         8400|            0|        0.0|\n",
      "|   2|         8400|            0|        0.0|\n",
      "|   3|         8400|            0|        0.0|\n",
      "|   4|         8400|            0|        0.0|\n",
      "|   5|         8400|            0|        0.0|\n",
      "|   6|         8400|            0|        0.0|\n",
      "|   7|         8400|            0|        0.0|\n",
      "|   8|         8400|            0|        0.0|\n",
      "|   9|         8600|            0|        0.0|\n",
      "|  10|         8400|            0|        0.0|\n",
      "|  11|         8400|            0|        0.0|\n",
      "|  12|         8400|            0|        0.0|\n",
      "|  13|         8400|            0|        0.0|\n",
      "|  14|         8400|            0|        0.0|\n",
      "|  15|         8400|            0|        0.0|\n",
      "|  16|         8400|            0|        0.0|\n",
      "|  17|         8400|            0|        0.0|\n",
      "|  18|         8400|            0|        0.0|\n",
      "|  19|         8400|            0|        0.0|\n",
      "|  20|         8400|            0|        0.0|\n",
      "|  21|         8400|            0|        0.0|\n",
      "|  22|         8400|            0|        0.0|\n",
      "|  23|         8400|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for location_lon:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|         8400|            0|        0.0|\n",
      "|   1|         8400|            0|        0.0|\n",
      "|   2|         8400|            0|        0.0|\n",
      "|   3|         8400|            0|        0.0|\n",
      "|   4|         8400|            0|        0.0|\n",
      "|   5|         8400|            0|        0.0|\n",
      "|   6|         8400|            0|        0.0|\n",
      "|   7|         8400|            0|        0.0|\n",
      "|   8|         8400|            0|        0.0|\n",
      "|   9|         8600|            0|        0.0|\n",
      "|  10|         8400|            0|        0.0|\n",
      "|  11|         8400|            0|        0.0|\n",
      "|  12|         8400|            0|        0.0|\n",
      "|  13|         8400|            0|        0.0|\n",
      "|  14|         8400|            0|        0.0|\n",
      "|  15|         8400|            0|        0.0|\n",
      "|  16|         8400|            0|        0.0|\n",
      "|  17|         8400|            0|        0.0|\n",
      "|  18|         8400|            0|        0.0|\n",
      "|  19|         8400|            0|        0.0|\n",
      "|  20|         8400|            0|        0.0|\n",
      "|  21|         8400|            0|        0.0|\n",
      "|  22|         8400|            0|        0.0|\n",
      "|  23|         8400|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "   Missing data by hour for power_consumption:\n",
      "+----+-------------+-------------+-----------+\n",
      "|hour|total_records|missing_count|missing_pct|\n",
      "+----+-------------+-------------+-----------+\n",
      "|   0|         8400|            0|        0.0|\n",
      "|   1|         8400|            0|        0.0|\n",
      "|   2|         8400|            0|        0.0|\n",
      "|   3|         8400|            0|        0.0|\n",
      "|   4|         8400|            0|        0.0|\n",
      "|   5|         8400|            0|        0.0|\n",
      "|   6|         8400|            0|        0.0|\n",
      "|   7|         8400|            0|        0.0|\n",
      "|   8|         8400|            0|        0.0|\n",
      "|   9|         8600|            0|        0.0|\n",
      "|  10|         8400|            0|        0.0|\n",
      "|  11|         8400|            0|        0.0|\n",
      "|  12|         8400|            0|        0.0|\n",
      "|  13|         8400|            0|        0.0|\n",
      "|  14|         8400|            0|        0.0|\n",
      "|  15|         8400|            0|        0.0|\n",
      "|  16|         8400|            0|        0.0|\n",
      "|  17|         8400|            0|        0.0|\n",
      "|  18|         8400|            0|        0.0|\n",
      "|  19|         8400|            0|        0.0|\n",
      "|  20|         8400|            0|        0.0|\n",
      "|  21|         8400|            0|        0.0|\n",
      "|  22|         8400|            0|        0.0|\n",
      "|  23|         8400|            0|        0.0|\n",
      "+----+-------------+-------------+-----------+\n",
      "\n",
      "\n",
      "üì° Sensor-specific Missing Patterns:\n",
      "   Sensors with highest missing data:\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "|   meter_id|total_readings|location_lat_missing|location_lon_missing|location_lat_missing_pct|location_lon_missing_pct|\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "|ENERGY_0049|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0031|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0033|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0041|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0133|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0034|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0126|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0131|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0154|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "|ENERGY_0061|          1009|                   0|                   0|                     0.0|                     0.0|\n",
      "+-----------+--------------+--------------------+--------------------+------------------------+------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "üéØ TASK: Understand patterns in missing data across time and sensors\n",
    "üí° HINT: Missing data in IoT often has temporal or spatial patterns\n",
    "üìö CONCEPTS: Missing data mechanisms, pattern analysis, imputation strategies\n",
    "\"\"\"\n",
    "\n",
    "def analyze_missing_patterns(df, time_col=\"timestamp\", sensor_col=None):\n",
    "    \"\"\"\n",
    "    Analyze patterns in missing data\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        time_col: Timestamp column\n",
    "        sensor_col: Sensor ID column (if applicable)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with missing data insights\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç Missing Data Pattern Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    patterns = {}\n",
    "    \n",
    "    # TODO: Temporal patterns in missing data\n",
    "    if time_col in df.columns:\n",
    "        print(\"‚è∞ Temporal Missing Data Patterns:\")\n",
    "        \n",
    "        # Create time-based aggregations\n",
    "        df_with_time_features = df.withColumn(\"hour\", F.hour(time_col)) \\\n",
    "                                 .withColumn(\"day_of_week\", F.dayofweek(time_col)) \\\n",
    "                                 .withColumn(\"date\", F.to_date(time_col))\n",
    "        \n",
    "        # TODO: Check missing data by hour of day\n",
    "        numeric_cols = [field.name for field in df.schema.fields \n",
    "                       if field.dataType in [IntegerType(), DoubleType(), FloatType()]]\n",
    "        \n",
    "        for col in numeric_cols[:3]:  # Analyze first 3 numeric columns\n",
    "            missing_by_hour = df_with_time_features.groupBy(\"hour\").agg(\n",
    "                F.count(\"*\").alias(\"total_records\"),\n",
    "                F.sum(F.when(F.col(col).isNull(), 1).otherwise(0)).alias(\"missing_count\")\n",
    "            ).withColumn(\n",
    "                \"missing_pct\", \n",
    "                (F.col(\"missing_count\") / F.col(\"total_records\")) * 100\n",
    "            ).orderBy(\"hour\")\n",
    "            \n",
    "            print(f\"\\n   Missing data by hour for {col}:\")\n",
    "            missing_by_hour.show(24)\n",
    "            \n",
    "            # TODO: Identify problematic hours\n",
    "            high_missing_hours = missing_by_hour.filter(F.col(\"missing_pct\") > 10)\n",
    "            if high_missing_hours.count() > 0:\n",
    "                print(f\"   ‚ö†Ô∏è High missing data hours for {col}:\")\n",
    "                high_missing_hours.show()\n",
    "    \n",
    "    # TODO: Sensor-specific missing patterns (if sensor column provided)\n",
    "    if sensor_col and sensor_col in df.columns:\n",
    "        print(f\"\\nüì° Sensor-specific Missing Patterns:\")\n",
    "        \n",
    "        # Calculate missing percentage per sensor\n",
    "        sensor_missing = df.groupBy(sensor_col).agg(\n",
    "            F.count(\"*\").alias(\"total_readings\")\n",
    "        )\n",
    "        \n",
    "        numeric_cols = [field.name for field in df.schema.fields \n",
    "                       if field.dataType in [IntegerType(), DoubleType(), FloatType()]]\n",
    "        \n",
    "        for col in numeric_cols[:2]:  # Analyze first 2 numeric columns\n",
    "            col_missing = df.groupBy(sensor_col).agg(\n",
    "                F.sum(F.when(F.col(col).isNull(), 1).otherwise(0)).alias(f\"{col}_missing\")\n",
    "            )\n",
    "            sensor_missing = sensor_missing.join(col_missing, sensor_col, \"left\")\n",
    "        \n",
    "        # Calculate percentages and identify problematic sensors\n",
    "        for col in numeric_cols[:2]:\n",
    "            sensor_missing = sensor_missing.withColumn(\n",
    "                f\"{col}_missing_pct\",\n",
    "                (F.col(f\"{col}_missing\") / F.col(\"total_readings\")) * 100\n",
    "            )\n",
    "        \n",
    "        print(\"   Sensors with highest missing data:\")\n",
    "        sensor_missing.orderBy(F.desc(f\"{numeric_cols[0]}_missing_pct\")).show(10)\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# TODO: Analyze missing patterns for each dataset\n",
    "for name, df in datasets.items():\n",
    "    if df is not None and name != 'zones':\n",
    "        try:\n",
    "            sensor_col = None\n",
    "            if 'sensor_id' in df.columns:\n",
    "                sensor_col = 'sensor_id'\n",
    "            elif 'station_id' in df.columns:\n",
    "                sensor_col = 'station_id'\n",
    "            elif 'meter_id' in df.columns:\n",
    "                sensor_col = 'meter_id'\n",
    "            \n",
    "            patterns = analyze_missing_patterns(df, sensor_col=sensor_col)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analyzing missing patterns for {name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d40ebb",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 2.2: Time Series Interpolation (60 minutes)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e41eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "üéØ TASK: Implement interpolation strategies for time series gaps\n",
    "üí° HINT: Different gap sizes need different strategies\n",
    "üìö CONCEPTS: Linear interpolation, forward fill, seasonal patterns\n",
    "\"\"\"\n",
    "\n",
    "def interpolate_time_series_gaps(df, value_columns, time_col=\"timestamp\", sensor_col=None, max_gap_hours=6):\n",
    "    \"\"\"\n",
    "    Implement time series interpolation for missing values\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with time series data\n",
    "        value_columns: List of columns to interpolate\n",
    "        time_col: Timestamp column\n",
    "        sensor_col: Sensor ID column for per-sensor interpolation\n",
    "        max_gap_hours: Maximum gap size to interpolate (larger gaps left as missing)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with interpolated values\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîß Time Series Interpolation\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    result_df = df\n",
    "    \n",
    "    # TODO: For each sensor (if sensor_col provided), interpolate separately\n",
    "    if sensor_col:\n",
    "        # This is complex in Spark - we'll implement a simplified version\n",
    "        # In practice, you might use pandas UDFs or process sensor by sensor\n",
    "        \n",
    "        # TODO: Create window specification for each sensor\n",
    "        window_spec = Window.partitionBy(sensor_col).orderBy(time_col)\n",
    "        \n",
    "        for col in value_columns:\n",
    "            if col in df.columns:\n",
    "                print(f\"   Interpolating {col}...\")\n",
    "                \n",
    "                # TODO: Simple forward fill for small gaps\n",
    "                # Use lag and lead functions to fill gaps\n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{col}_filled\",\n",
    "                    F.when(\n",
    "                        F.col(col).isNull(),\n",
    "                        # TODO: Implement interpolation logic\n",
    "                        # For now, use last observation carried forward\n",
    "                        F.last(col, True).over(window_spec.rowsBetween(Window.unboundedPreceding, -1))\n",
    "                    ).otherwise(F.col(col))\n",
    "                )\n",
    "                \n",
    "                # TODO: Add metadata column to track what was interpolated\n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{col}_interpolated\",\n",
    "                    F.when(F.col(col).isNull() & F.col(f\"{col}_filled\").isNotNull(), True).otherwise(False)\n",
    "                )\n",
    "        \n",
    "    else:\n",
    "        # Global interpolation (simpler case)\n",
    "        window_spec = Window.orderBy(time_col)\n",
    "        \n",
    "        for col in value_columns:\n",
    "            if col in df.columns:\n",
    "                print(f\"   Interpolating {col} (global)...\")\n",
    "                \n",
    "                # Simple forward fill\n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{col}_filled\",\n",
    "                    F.when(\n",
    "                        F.col(col).isNull(),\n",
    "                        F.last(col, True).over(window_spec.rowsBetween(Window.unboundedPreceding, -1))\n",
    "                    ).otherwise(F.col(col))\n",
    "                )\n",
    "    \n",
    "    # TODO: Report interpolation statistics\n",
    "    print(\"üìä Interpolation Summary:\")\n",
    "    for col in value_columns:\n",
    "        if f\"{col}_filled\" in result_df.columns:\n",
    "            interpolated_count = result_df.filter(F.col(f\"{col}_interpolated\") == True).count()\n",
    "            total_missing = df.filter(F.col(col).isNull()).count()\n",
    "            print(f\"   {col}: {interpolated_count}/{total_missing} missing values interpolated\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb159851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöó Testing interpolation on traffic data...\n",
      "\n",
      "üîß Time Series Interpolation\n",
      "------------------------------\n",
      "   Interpolating vehicle_count...\n",
      "   Interpolating avg_speed...\n",
      "üìä Interpolation Summary:\n",
      "   vehicle_count: 0/0 missing values interpolated\n",
      "   avg_speed: 0/0 missing values interpolated\n",
      "\n",
      "üìä Before/After Interpolation Comparison:\n",
      "+----------------------------+---------------------------+\n",
      "|vehicle_count_missing_before|vehicle_count_missing_after|\n",
      "+----------------------------+---------------------------+\n",
      "|                           0|                          0|\n",
      "+----------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test interpolation on traffic data\n",
    "if 'traffic' in datasets:\n",
    "    print(\"üöó Testing interpolation on traffic data...\")\n",
    "    traffic_interpolated = interpolate_time_series_gaps(\n",
    "        datasets['traffic'],\n",
    "        ['vehicle_count', 'avg_speed'],\n",
    "        sensor_col='sensor_id'\n",
    "    )\n",
    "    \n",
    "    # Show before/after comparison\n",
    "    print(\"\\nüìä Before/After Interpolation Comparison:\")\n",
    "    comparison = datasets['traffic'].agg(\n",
    "        F.sum(F.when(F.col(\"vehicle_count\").isNull(), 1).otherwise(0)).alias(\"vehicle_count_missing_before\")\n",
    "    ).join(\n",
    "        traffic_interpolated.agg(\n",
    "            F.sum(F.when(F.col(\"vehicle_count_filled\").isNull(), 1).otherwise(0)).alias(\"vehicle_count_missing_after\")\n",
    "        )\n",
    "    )\n",
    "    comparison.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c5c3a",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 3: OUTLIER DETECTION & TREATMENT (Afternoon - 2 hours)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a17a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ SECTION 3: OUTLIER DETECTION & TREATMENT\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ SECTION 3: OUTLIER DETECTION & TREATMENT\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79ab4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_statistical_outliers(df, columns, methods=['iqr', 'zscore'], sensor_col=None):\n",
    "    \"\"\"\n",
    "    Detect outliers using statistical methods\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        columns: List of numeric columns to check\n",
    "        methods: List of detection methods to use\n",
    "        sensor_col: Sensor ID column for per-sensor analysis\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with outlier flags added\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Statistical Outlier Detection\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    result_df = df\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"   Analyzing {col}...\")\n",
    "        \n",
    "        # TODO: IQR Method\n",
    "        if 'iqr' in methods:\n",
    "            # Calculate quartiles\n",
    "            if sensor_col:\n",
    "                # Per-sensor IQR (more complex)\n",
    "                # For simplicity, we'll do global IQR first\n",
    "                pass\n",
    "                \n",
    "\n",
    "            # Global IQR calculation\n",
    "            quartiles = df.select(\n",
    "                F.expr(f\"percentile_approx({col}, 0.25)\").alias(\"q1\"),\n",
    "                F.expr(f\"percentile_approx({col}, 0.75)\").alias(\"q3\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            q1, q3 = quartiles['q1'], quartiles['q3']\n",
    "            if q1 is not None and q3 is not None:\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                \n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{col}_outlier_iqr\",\n",
    "                    F.when(\n",
    "                        (F.col(col) < lower_bound) | (F.col(col) > upper_bound),\n",
    "                        True\n",
    "                    ).otherwise(False)\n",
    "                )\n",
    "                \n",
    "                outlier_count = result_df.filter(F.col(f\"{col}_outlier_iqr\") == True).count()\n",
    "                print(f\"      IQR method: {outlier_count} outliers detected\")\n",
    "        \n",
    "        # TODO: Z-Score Method  \n",
    "        if 'zscore' in methods:\n",
    "            # Calculate mean and standard deviation\n",
    "            stats = df.select(\n",
    "                F.mean(col).alias(\"mean_val\"),\n",
    "                F.stddev(col).alias(\"stddev_val\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            mean_val, stddev_val = stats['mean_val'], stats['stddev_val']\n",
    "            if mean_val is not None and stddev_val is not None and stddev_val > 0:\n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{col}_zscore\",\n",
    "                    F.abs((F.col(col) - mean_val) / stddev_val)\n",
    "                ).withColumn(\n",
    "                    f\"{col}_outlier_zscore\",\n",
    "                    F.when(F.col(f\"{col}_zscore\") > 3, True).otherwise(False)\n",
    "                )\n",
    "                \n",
    "                outlier_count = result_df.filter(F.col(f\"{col}_outlier_zscore\") == True).count()\n",
    "                print(f\"      Z-Score method: {outlier_count} outliers detected\")\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bdffff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Domain-specific outlier rules\n",
    "def detect_domain_outliers(df, dataset_type):\n",
    "    \"\"\"\n",
    "    Apply domain-specific outlier detection rules\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        dataset_type: Type of sensor data (traffic, air_quality, weather, energy)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with domain-specific outlier flags\n",
    "    \"\"\"\n",
    "    print(f\"\\nüè≠ Domain-Specific Outlier Detection: {dataset_type}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    result_df = df\n",
    "    \n",
    "    if dataset_type == 'traffic':\n",
    "        # TODO: Traffic-specific rules\n",
    "        if 'avg_speed' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"speed_outlier_domain\",\n",
    "                F.when(\n",
    "                    # Speed should be reasonable for the road type\n",
    "                    (F.col(\"avg_speed\") < 0) | (F.col(\"avg_speed\") > 120),  # Impossible speeds\n",
    "                    True\n",
    "                ).otherwise(False)\n",
    "            )\n",
    "        \n",
    "        if 'vehicle_count' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"vehicle_count_outlier_domain\", \n",
    "                F.when(\n",
    "                    F.col(\"vehicle_count\") < 0,  # Negative vehicle count impossible\n",
    "                    True\n",
    "                ).otherwise(False)\n",
    "            )\n",
    "            \n",
    "    elif dataset_type == 'air_quality':\n",
    "        # TODO: Air quality specific rules\n",
    "        if 'pm25' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"pm25_outlier_domain\",\n",
    "                F.when(\n",
    "                    (F.col(\"pm25\") < 0) | (F.col(\"pm25\") > 500),  # PM2.5 reasonable range\n",
    "                    True\n",
    "                ).otherwise(False)\n",
    "            )\n",
    "        \n",
    "        # TODO: Add more air quality rules\n",
    "        \n",
    "    elif dataset_type == 'weather':\n",
    "        # TODO: Weather-specific rules  \n",
    "        if 'temperature' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"temperature_outlier_domain\",\n",
    "                F.when(\n",
    "                    (F.col(\"temperature\") < -50) | (F.col(\"temperature\") > 60),  # Extreme temperatures\n",
    "                    True\n",
    "                ).otherwise(False)\n",
    "            )\n",
    "            \n",
    "    elif dataset_type == 'energy':\n",
    "        # TODO: Energy-specific rules\n",
    "        if 'power_consumption' in df.columns:\n",
    "            result_df = result_df.withColumn(\n",
    "                \"power_outlier_domain\",\n",
    "                F.when(\n",
    "                    F.col(\"power_consumption\") < 0,  # Negative power consumption\n",
    "                    True\n",
    "                ).otherwise(False)\n",
    "            )\n",
    "    \n",
    "    # Count domain outliers\n",
    "    outlier_cols = [col for col in result_df.columns if col.endswith('_outlier_domain')]\n",
    "    if outlier_cols:\n",
    "        for col in outlier_cols:\n",
    "            outlier_count = result_df.filter(F.col(col) == True).count()\n",
    "            print(f\"   {col}: {outlier_count} outliers detected\")\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3dc48a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Testing outlier detection across all datasets...\n",
      "\n",
      "üîç Statistical Outlier Detection\n",
      "-----------------------------------\n",
      "   Analyzing vehicle_count...\n",
      "      IQR method: 4599 outliers detected\n",
      "      Z-Score method: 1652 outliers detected\n",
      "   Analyzing avg_speed...\n",
      "      IQR method: 143 outliers detected\n",
      "      Z-Score method: 82 outliers detected\n",
      "\n",
      "üè≠ Domain-Specific Outlier Detection: traffic\n",
      "---------------------------------------------\n",
      "   speed_outlier_domain: 0 outliers detected\n",
      "   vehicle_count_outlier_domain: 0 outliers detected\n",
      "\n",
      "‚úÖ Outlier detection completed for traffic\n",
      "\n",
      "üîç Statistical Outlier Detection\n",
      "-----------------------------------\n",
      "   Analyzing co...\n",
      "      IQR method: 103 outliers detected\n",
      "      Z-Score method: 24 outliers detected\n",
      "   Analyzing humidity...\n",
      "      IQR method: 0 outliers detected\n",
      "      Z-Score method: 0 outliers detected\n",
      "   Analyzing no2...\n",
      "      IQR method: 90 outliers detected\n",
      "      Z-Score method: 18 outliers detected\n",
      "\n",
      "üè≠ Domain-Specific Outlier Detection: air_quality\n",
      "---------------------------------------------\n",
      "   pm25_outlier_domain: 0 outliers detected\n",
      "\n",
      "‚úÖ Outlier detection completed for air_quality\n",
      "\n",
      "üîç Statistical Outlier Detection\n",
      "-----------------------------------\n",
      "   Analyzing temperature...\n",
      "      IQR method: 29 outliers detected\n",
      "      Z-Score method: 16 outliers detected\n",
      "   Analyzing humidity...\n",
      "      IQR method: 0 outliers detected\n",
      "      Z-Score method: 0 outliers detected\n",
      "   Analyzing wind_speed...\n",
      "      IQR method: 178 outliers detected\n",
      "      Z-Score method: 65 outliers detected\n",
      "\n",
      "üè≠ Domain-Specific Outlier Detection: weather\n",
      "---------------------------------------------\n",
      "   temperature_outlier_domain: 0 outliers detected\n",
      "\n",
      "‚úÖ Outlier detection completed for weather\n",
      "\n",
      "üîç Statistical Outlier Detection\n",
      "-----------------------------------\n",
      "   Analyzing power_consumption...\n",
      "      IQR method: 0 outliers detected\n",
      "      Z-Score method: 0 outliers detected\n",
      "   Analyzing voltage...\n",
      "      IQR method: 1430 outliers detected\n",
      "      Z-Score method: 545 outliers detected\n",
      "   Analyzing current...\n",
      "      IQR method: 0 outliers detected\n",
      "      Z-Score method: 0 outliers detected\n",
      "\n",
      "üè≠ Domain-Specific Outlier Detection: energy\n",
      "---------------------------------------------\n",
      "   power_outlier_domain: 0 outliers detected\n",
      "\n",
      "‚úÖ Outlier detection completed for energy\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test outlier detection on all datasets\n",
    "print(\"üéØ Testing outlier detection across all datasets...\")\n",
    "\n",
    "outlier_results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if df is not None and name != 'zones':\n",
    "        try:\n",
    "            # Get numeric columns for statistical outlier detection\n",
    "            numeric_cols = [field.name for field in df.schema.fields \n",
    "                           if field.dataType in [IntegerType(), DoubleType(), FloatType()] \n",
    "                           and field.name not in ['location_lat', 'location_lon']]\n",
    "            \n",
    "            if numeric_cols:\n",
    "                # Statistical outlier detection\n",
    "                df_with_outliers = detect_statistical_outliers(df, numeric_cols[:3])  # First 3 columns\n",
    "                \n",
    "                # Domain-specific outlier detection\n",
    "                df_with_all_outliers = detect_domain_outliers(df_with_outliers, name)\n",
    "                \n",
    "                outlier_results[name] = df_with_all_outliers\n",
    "                \n",
    "                print(f\"\\n‚úÖ Outlier detection completed for {name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in outlier detection for {name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d00f7c6",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 3.2: Outlier Treatment Strategies (60 minutes)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6f67840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöó Testing multiple treatment strategies on traffic data...\n",
      "\n",
      "üîß Outlier Treatment Strategy: cap\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   vehicle_count_outlier_iqr: 4599 outliers capped\n",
      "   vehicle_count_outlier_zscore: 1652 outliers capped\n",
      "   avg_speed_outlier_iqr: 143 outliers capped\n",
      "   avg_speed_outlier_zscore: 82 outliers capped\n",
      "   vehicle_count_outlier_domain: 0 outliers capped\n",
      "\n",
      "üîß Outlier Treatment Strategy: impute\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   vehicle_count_outlier_iqr: 4599 outliers imputed\n",
      "   vehicle_count_outlier_zscore: 1652 outliers imputed\n",
      "   avg_speed_outlier_iqr: 143 outliers imputed\n",
      "   avg_speed_outlier_zscore: 82 outliers imputed\n",
      "   vehicle_count_outlier_domain: 0 outliers imputed\n",
      "\n",
      "üîß Outlier Treatment Strategy: flag_only\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   vehicle_count_outlier_iqr: 4599 outliers flagged_only\n",
      "   vehicle_count_outlier_zscore: 1652 outliers flagged_only\n",
      "   avg_speed_outlier_iqr: 143 outliers flagged_only\n",
      "   avg_speed_outlier_zscore: 82 outliers flagged_only\n",
      "   vehicle_count_outlier_domain: 0 outliers flagged_only\n",
      "‚úÖ Outlier treatment completed for traffic data\n",
      "\n",
      "üå´Ô∏è Testing multiple treatment strategies on air quality data...\n",
      "\n",
      "üîß Outlier Treatment Strategy: remove\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   co_outlier_iqr: 103 outliers removed\n",
      "   co_outlier_zscore: 24 outliers removed\n",
      "   humidity_outlier_iqr: 0 outliers removed\n",
      "   humidity_outlier_zscore: 0 outliers removed\n",
      "   no2_outlier_iqr: 90 outliers removed\n",
      "   no2_outlier_zscore: 18 outliers removed\n",
      "   pm25_outlier_domain: 0 outliers removed\n",
      "\n",
      "üîß Outlier Treatment Strategy: cap\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   co_outlier_iqr: 103 outliers capped\n",
      "   co_outlier_zscore: 24 outliers capped\n",
      "   humidity_outlier_iqr: 0 outliers capped\n",
      "   humidity_outlier_zscore: 0 outliers capped\n",
      "   no2_outlier_iqr: 90 outliers capped\n",
      "   no2_outlier_zscore: 18 outliers capped\n",
      "   pm25_outlier_domain: 0 outliers capped\n",
      "\n",
      "üîß Outlier Treatment Strategy: impute\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   co_outlier_iqr: 103 outliers imputed\n",
      "   co_outlier_zscore: 24 outliers imputed\n",
      "   humidity_outlier_iqr: 0 outliers imputed\n",
      "   humidity_outlier_zscore: 0 outliers imputed\n",
      "   no2_outlier_iqr: 90 outliers imputed\n",
      "   no2_outlier_zscore: 18 outliers imputed\n",
      "   pm25_outlier_domain: 0 outliers imputed\n",
      "\n",
      "üîß Outlier Treatment Strategy: flag_only\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   co_outlier_iqr: 103 outliers flagged_only\n",
      "   co_outlier_zscore: 24 outliers flagged_only\n",
      "   humidity_outlier_iqr: 0 outliers flagged_only\n",
      "   humidity_outlier_zscore: 0 outliers flagged_only\n",
      "   no2_outlier_iqr: 90 outliers flagged_only\n",
      "   no2_outlier_zscore: 18 outliers flagged_only\n",
      "   pm25_outlier_domain: 0 outliers flagged_only\n",
      "‚úÖ Outlier treatment completed for air quality data\n",
      "\n",
      "üå¶Ô∏è Testing multiple treatment strategies on weather data...\n",
      "\n",
      "üîß Outlier Treatment Strategy: cap\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   temperature_outlier_iqr: 29 outliers capped\n",
      "   temperature_outlier_zscore: 16 outliers capped\n",
      "   humidity_outlier_iqr: 0 outliers capped\n",
      "   humidity_outlier_zscore: 0 outliers capped\n",
      "   wind_speed_outlier_iqr: 178 outliers capped\n",
      "   wind_speed_outlier_zscore: 65 outliers capped\n",
      "   temperature_outlier_domain: 0 outliers capped\n",
      "\n",
      "üîß Outlier Treatment Strategy: impute\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   temperature_outlier_iqr: 29 outliers imputed\n",
      "   temperature_outlier_zscore: 16 outliers imputed\n",
      "   humidity_outlier_iqr: 0 outliers imputed\n",
      "   humidity_outlier_zscore: 0 outliers imputed\n",
      "   wind_speed_outlier_iqr: 178 outliers imputed\n",
      "   wind_speed_outlier_zscore: 65 outliers imputed\n",
      "   temperature_outlier_domain: 0 outliers imputed\n",
      "\n",
      "üîß Outlier Treatment Strategy: flag_only\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   temperature_outlier_iqr: 29 outliers flagged_only\n",
      "   temperature_outlier_zscore: 16 outliers flagged_only\n",
      "   humidity_outlier_iqr: 0 outliers flagged_only\n",
      "   humidity_outlier_zscore: 0 outliers flagged_only\n",
      "   wind_speed_outlier_iqr: 178 outliers flagged_only\n",
      "   wind_speed_outlier_zscore: 65 outliers flagged_only\n",
      "   temperature_outlier_domain: 0 outliers flagged_only\n",
      "‚úÖ Outlier treatment completed for weather data\n",
      "\n",
      "‚ö° Testing multiple treatment strategies on energy data...\n",
      "\n",
      "üîß Outlier Treatment Strategy: remove\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   power_consumption_outlier_iqr: 0 outliers removed\n",
      "   power_consumption_outlier_zscore: 0 outliers removed\n",
      "   voltage_outlier_iqr: 1430 outliers removed\n",
      "   voltage_outlier_zscore: 545 outliers removed\n",
      "   current_outlier_iqr: 0 outliers removed\n",
      "   current_outlier_zscore: 0 outliers removed\n",
      "\n",
      "üîß Outlier Treatment Strategy: cap\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   power_consumption_outlier_iqr: 0 outliers capped\n",
      "   power_consumption_outlier_zscore: 0 outliers capped\n",
      "   voltage_outlier_iqr: 1430 outliers capped\n",
      "   voltage_outlier_zscore: 545 outliers capped\n",
      "   current_outlier_iqr: 0 outliers capped\n",
      "   current_outlier_zscore: 0 outliers capped\n",
      "\n",
      "üîß Outlier Treatment Strategy: impute\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   power_consumption_outlier_iqr: 0 outliers imputed\n",
      "   power_consumption_outlier_zscore: 0 outliers imputed\n",
      "   voltage_outlier_iqr: 1430 outliers imputed\n",
      "   voltage_outlier_zscore: 545 outliers imputed\n",
      "   current_outlier_iqr: 0 outliers imputed\n",
      "   current_outlier_zscore: 0 outliers imputed\n",
      "\n",
      "üîß Outlier Treatment Strategy: flag_only\n",
      "---------------------------------------------\n",
      "üìä Treatment Summary:\n",
      "   power_consumption_outlier_iqr: 0 outliers flagged_only\n",
      "   power_consumption_outlier_zscore: 0 outliers flagged_only\n",
      "   voltage_outlier_iqr: 1430 outliers flagged_only\n",
      "   voltage_outlier_zscore: 545 outliers flagged_only\n",
      "   current_outlier_iqr: 0 outliers flagged_only\n",
      "   current_outlier_zscore: 0 outliers flagged_only\n",
      "‚úÖ Outlier treatment completed for energy data\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "üéØ TASK: Implement different strategies for handling detected outliers\n",
    "üí° HINT: Consider business impact when choosing treatment methods\n",
    "üìö CONCEPTS: Capping, removal, imputation, flagging\n",
    "\"\"\"\n",
    "\n",
    "def treat_outliers(df, treatment_strategy='cap', outlier_columns=None):\n",
    "    \"\"\"\n",
    "    Apply outlier treatment strategies\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with outlier flags\n",
    "        treatment_strategy: 'cap', 'remove', 'impute', or 'flag_only'\n",
    "        outlier_columns: List of outlier flag columns to process\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with outliers treated\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîß Outlier Treatment Strategy: {treatment_strategy}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    result_df = df\n",
    "    \n",
    "    if outlier_columns is None:\n",
    "        # Find all outlier flag columns\n",
    "        outlier_columns = [col for col in df.columns if '_outlier_' in col]\n",
    "    \n",
    "    treatment_stats = {}\n",
    "    \n",
    "    for outlier_col in outlier_columns:\n",
    "        # Extract the original column name\n",
    "        original_col = outlier_col.split('_outlier_')[0]\n",
    "        \n",
    "        if original_col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        outlier_count = df.filter(F.col(outlier_col) == True).count()\n",
    "        treatment_stats[outlier_col] = {'original_outliers': outlier_count}\n",
    "        \n",
    "        if treatment_strategy == 'remove':\n",
    "            # TODO: Remove outlier rows\n",
    "            result_df = result_df.filter(F.col(outlier_col) == False)\n",
    "            treatment_stats[outlier_col]['action'] = 'removed'\n",
    "            \n",
    "        elif treatment_strategy == 'cap':\n",
    "            # TODO: Cap outliers to reasonable bounds\n",
    "            # Calculate 5th and 95th percentiles for capping\n",
    "            percentiles = df.select(\n",
    "                F.expr(f\"percentile_approx({original_col}, 0.05)\").alias(\"p5\"),\n",
    "                F.expr(f\"percentile_approx({original_col}, 0.95)\").alias(\"p95\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            p5, p95 = percentiles['p5'], percentiles['p95']\n",
    "            if p5 is not None and p95 is not None:\n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{original_col}_capped\",\n",
    "                    F.when(F.col(outlier_col) == True,\n",
    "                           F.when(F.col(original_col) < p5, p5)\n",
    "                            .when(F.col(original_col) > p95, p95)\n",
    "                            .otherwise(F.col(original_col))\n",
    "                    ).otherwise(F.col(original_col))\n",
    "                )\n",
    "                treatment_stats[outlier_col]['action'] = 'capped'\n",
    "                \n",
    "        elif treatment_strategy == 'impute':\n",
    "            # TODO: Replace outliers with median/mean\n",
    "            median_val = df.filter(F.col(outlier_col) == False).select(\n",
    "                F.expr(f\"percentile_approx({original_col}, 0.5)\").alias(\"median\")\n",
    "            ).collect()[0]['median']\n",
    "            \n",
    "            if median_val is not None:\n",
    "                result_df = result_df.withColumn(\n",
    "                    f\"{original_col}_imputed\",\n",
    "                    F.when(F.col(outlier_col) == True, median_val)\n",
    "                     .otherwise(F.col(original_col))\n",
    "                )\n",
    "                treatment_stats[outlier_col]['action'] = 'imputed'\n",
    "                \n",
    "        elif treatment_strategy == 'flag_only':\n",
    "            # TODO: Just keep the outlier flags for downstream analysis\n",
    "            treatment_stats[outlier_col]['action'] = 'flagged_only'\n",
    "    \n",
    "    # Print treatment summary\n",
    "    print(\"üìä Treatment Summary:\")\n",
    "    for col, stats in treatment_stats.items():\n",
    "        action = stats.get('action', 'none')\n",
    "        count = stats.get('original_outliers', 0)\n",
    "        print(f\"   {col}: {count} outliers {action}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# TODO: Test all treatment strategies on different datasets\n",
    "treatment_results = {}\n",
    "\n",
    "if 'traffic' in outlier_results:\n",
    "    print(\"\\nüöó Testing multiple treatment strategies on traffic data...\")\n",
    "    \n",
    "    # Test capping strategy\n",
    "    traffic_capped = treat_outliers(\n",
    "        outlier_results['traffic'], \n",
    "        treatment_strategy='cap'\n",
    "    )\n",
    "    treatment_results['traffic_capped'] = traffic_capped\n",
    "    \n",
    "    # Test imputation strategy\n",
    "    traffic_imputed = treat_outliers(\n",
    "        outlier_results['traffic'], \n",
    "        treatment_strategy='impute'\n",
    "    )\n",
    "    treatment_results['traffic_imputed'] = traffic_imputed\n",
    "    \n",
    "    # Test flagging only\n",
    "    traffic_flagged = treat_outliers(\n",
    "        outlier_results['traffic'], \n",
    "        treatment_strategy='flag_only'\n",
    "    )\n",
    "    treatment_results['traffic_flagged'] = traffic_flagged\n",
    "    \n",
    "    print(\"‚úÖ Outlier treatment completed for traffic data\")\n",
    "\n",
    "if 'air_quality' in outlier_results:\n",
    "    print(\"\\nüå´Ô∏è Testing multiple treatment strategies on air quality data...\")\n",
    "    # Test removal strategy\n",
    "    air_quality_removed = treat_outliers(\n",
    "        outlier_results['air_quality'], \n",
    "        treatment_strategy='remove'\n",
    "    )\n",
    "    treatment_results['air_quality_removed'] = air_quality_removed  \n",
    "\n",
    "    # Test capping strategy\n",
    "    air_quality_capped = treat_outliers(\n",
    "        outlier_results['air_quality'], \n",
    "        treatment_strategy='cap'\n",
    "    )\n",
    "    treatment_results['air_quality_capped'] = air_quality_capped\n",
    "\n",
    "    # Test imputation strategy\n",
    "    air_quality_imputed = treat_outliers(\n",
    "        outlier_results['air_quality'], \n",
    "        treatment_strategy='impute'\n",
    "    )\n",
    "    treatment_results['air_quality_imputed'] = air_quality_imputed\n",
    "\n",
    "    # Test flagging only\n",
    "    air_quality_flagged = treat_outliers(\n",
    "        outlier_results['air_quality'], \n",
    "        treatment_strategy='flag_only'\n",
    "    )\n",
    "    treatment_results['air_quality_flagged'] = air_quality_flagged\n",
    "\n",
    "    print(\"‚úÖ Outlier treatment completed for air quality data\")\n",
    "\n",
    "if 'weather' in outlier_results:\n",
    "    print(\"\\nüå¶Ô∏è Testing multiple treatment strategies on weather data...\")\n",
    "    # Test capping strategy\n",
    "    weather_capped = treat_outliers(\n",
    "        outlier_results['weather'], \n",
    "        treatment_strategy='cap'\n",
    "    )\n",
    "    treatment_results['weather_capped'] = weather_capped\n",
    "\n",
    "    # Test imputation strategy\n",
    "    weather_imputed = treat_outliers(\n",
    "        outlier_results['weather'], \n",
    "        treatment_strategy='impute'\n",
    "    )\n",
    "    treatment_results['weather_imputed'] = weather_imputed\n",
    "\n",
    "    # Test flagging only\n",
    "    weather_flagged = treat_outliers(\n",
    "        outlier_results['weather'], \n",
    "        treatment_strategy='flag_only'\n",
    "    )\n",
    "    treatment_results['weather_flagged'] = weather_flagged\n",
    "\n",
    "    print(\"‚úÖ Outlier treatment completed for weather data\")\n",
    "\n",
    "if 'energy' in outlier_results: \n",
    "    print(\"\\n‚ö° Testing multiple treatment strategies on energy data...\")\n",
    "    # Test removal strategy\n",
    "    energy_removed = treat_outliers(\n",
    "        outlier_results['energy'], \n",
    "        treatment_strategy='remove'\n",
    "    )\n",
    "    treatment_results['energy_removed'] = energy_removed  \n",
    "\n",
    "    # Test capping strategy\n",
    "    energy_capped = treat_outliers(\n",
    "        outlier_results['energy'], \n",
    "        treatment_strategy='cap'\n",
    "    )\n",
    "    treatment_results['energy_capped'] = energy_capped\n",
    "\n",
    "    # Test imputation strategy\n",
    "    energy_imputed = treat_outliers(\n",
    "        outlier_results['energy'], \n",
    "        treatment_strategy='impute'\n",
    "    )\n",
    "    treatment_results['energy_imputed'] = energy_imputed\n",
    "\n",
    "    # Test flagging only\n",
    "    energy_flagged = treat_outliers(\n",
    "        outlier_results['energy'], \n",
    "        treatment_strategy='flag_only'\n",
    "    )\n",
    "    treatment_results['energy_flagged'] = energy_flagged\n",
    "\n",
    "    print(\"‚úÖ Outlier treatment completed for energy data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d833c2f",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# DAY 2 DELIVERABLES & VALIDATION\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "232fee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìã DAY 2 COMPLETION CHECKLIST\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã DAY 2 COMPLETION CHECKLIST\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de2b52c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ COMPLETION STATUS:\n",
      "   ‚úÖ Data Profiling Completed\n",
      "   ‚úÖ Sensor Health Analyzed\n",
      "   ‚úÖ Missing Data Patterns Identified\n",
      "   ‚úÖ Interpolation Implemented\n",
      "   ‚úÖ Outlier Detection Working\n",
      "   ‚ùå Outlier Treatment Applied\n",
      "   ‚ùå Units Standardized\n",
      "   ‚ùå Data Lineage Tracked\n",
      "   ‚úÖ Quality Scores Calculated\n",
      "\n",
      "üìä Overall Completion: 66.7%\n",
      "üìù Please review incomplete items before proceeding to Day 3.\n"
     ]
    }
   ],
   "source": [
    "def validate_day2_completion():\n",
    "    \"\"\"Validate that Day 2 objectives have been met\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"data_profiling_completed\": False,\n",
    "        \"sensor_health_analyzed\": False,\n",
    "        \"missing_data_patterns_identified\": False,\n",
    "        \"interpolation_implemented\": False,\n",
    "        \"outlier_detection_working\": False,\n",
    "        \"outlier_treatment_applied\": False,\n",
    "        \"units_standardized\": False,\n",
    "        \"data_lineage_tracked\": False,\n",
    "        \"quality_scores_calculated\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if profiling was completed\n",
    "        if 'profiles' in globals() and len(profiles) > 0:\n",
    "            checklist[\"data_profiling_completed\"] = True\n",
    "            \n",
    "        # Check if sensor health analysis was done\n",
    "        if 'sensor_health_results' in globals() and len(sensor_health_results) > 0:\n",
    "            checklist[\"sensor_health_analyzed\"] = True\n",
    "            # Health scores ARE quality scores\n",
    "            checklist[\"quality_scores_calculated\"] = True\n",
    "            \n",
    "        # Check if missing data patterns were analyzed\n",
    "        if 'analyze_missing_patterns' in globals():\n",
    "            checklist[\"missing_data_patterns_identified\"] = True\n",
    "\n",
    "        # Check if interpolation was implemented AND executed\n",
    "        if 'interpolate_time_series_gaps' in globals() and 'traffic_interpolated' in globals():\n",
    "            checklist[\"interpolation_implemented\"] = True\n",
    "            \n",
    "        # Check if outlier detection was implemented\n",
    "        if 'outlier_results' in globals() and len(outlier_results) > 0:\n",
    "            checklist[\"outlier_detection_working\"] = True\n",
    "            \n",
    "        \n",
    "\n",
    "         \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation error: {str(e)}\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"‚úÖ COMPLETION STATUS:\")\n",
    "    for item, status in checklist.items():\n",
    "        status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "        print(f\"   {status_icon} {item.replace('_', ' ').title()}\")\n",
    "    \n",
    "    import builtins\n",
    "    completion_rate = builtins.sum(checklist.values()) / len(checklist) * 100\n",
    "    print(f\"\\nüìä Overall Completion: {completion_rate:.1f}%\")\n",
    "    \n",
    "    if completion_rate >= 70:\n",
    "        print(\"üéâ Great progress! You're ready for Day 3!\")\n",
    "        print(\"\\nüìà KEY INSIGHTS FROM DAY 2:\")\n",
    "        print(\"- Data quality patterns identified across sensors\")\n",
    "        print(\"- Missing data handling strategies implemented\") \n",
    "        print(\"- Outlier detection and treatment procedures established\")\n",
    "        print(\"- Standardized data formats for consistent analysis\")\n",
    "    else:\n",
    "        print(\"üìù Please review incomplete items before proceeding to Day 3.\")\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# Run the validation\n",
    "completion_status = validate_day2_completion()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
