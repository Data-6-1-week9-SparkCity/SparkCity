{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72b2eb0",
   "metadata": {},
   "source": [
    "# Day 2: Data Quality & Cleaning Pipeline\n",
    "# Smart City IoT Analytics Pipeline\n",
    "\n",
    "\"\"\"\n",
    "üéØ LEARNING OBJECTIVES:\n",
    "- Implement comprehensive data quality assessment\n",
    "- Design cleaning procedures for IoT sensor data\n",
    "- Handle missing values and outliers appropriately\n",
    "- Create reusable data quality functions\n",
    "\n",
    "üìÖ SCHEDULE:\n",
    "Morning (4 hours):\n",
    "1. Data Quality Assessment (2 hours)\n",
    "2. Missing Data Strategy (2 hours)\n",
    "\n",
    "Afternoon (4 hours):\n",
    "3. Outlier Detection & Treatment (2 hours)\n",
    "4. Data Standardization (2 hours)\n",
    "\n",
    "‚úÖ DELIVERABLES:\n",
    "- Data quality assessment report\n",
    "- Comprehensive cleaning pipeline\n",
    "- Outlier detection and treatment functions\n",
    "- Standardized datasets ready for analysis\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337c103",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79adbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Machine learning imports (for outlier detection)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d73575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/04 21:12:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/04 21:12:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created new Spark session\n",
      "üîß Day 2: Data Quality & Cleaning Pipeline\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session (should already exist from Day 1)\n",
    "try:\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(\"‚úÖ Using existing Spark session\")\n",
    "except:\n",
    "    spark = (SparkSession.builder\n",
    "             .appName(\"SmartCityIoTPipeline-Day2\")\n",
    "             .master(\"local[*]\")\n",
    "             .config(\"spark.driver.memory\", \"4g\")\n",
    "             .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "             .getOrCreate())\n",
    "    print(\"‚úÖ Created new Spark session\")\n",
    "\n",
    "print(\"üîß Day 2: Data Quality & Cleaning Pipeline\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e4c99",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 1: COMPREHENSIVE DATA PROFILING (Morning - 2 hours)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f8d4d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä SECTION 1: COMPREHENSIVE DATA PROFILING\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä SECTION 1: COMPREHENSIVE DATA PROFILING\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0657fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All datasets loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data from Day 1 (or reload if needed)\n",
    "data_dir = \"../data/raw\"\n",
    "\n",
    "# TODO 1.1: Load all datasets with error handling\n",
    "def load_all_datasets():\n",
    "    \"\"\"Load all sensor datasets with consistent error handling\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    try:\n",
    "        # Load each dataset\n",
    "        datasets['zones'] = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/city_zones.csv\")\n",
    "        datasets['traffic'] = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/traffic_sensors.csv\")\n",
    "        datasets['air_quality'] = spark.read.json(f\"{data_dir}/air_quality.json\")\n",
    "        datasets['weather'] = spark.read.parquet(f\"{data_dir}/weather_data.parquet\")\n",
    "        datasets['energy'] = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/energy_meters.csv\")\n",
    "        \n",
    "        # Convert timestamp columns to proper format\n",
    "        for name, df in datasets.items():\n",
    "            if name != 'zones' and 'timestamp' in df.columns:\n",
    "                datasets[name] = df.withColumn(\"timestamp\", F.to_timestamp(F.col(\"timestamp\")))\n",
    "        \n",
    "        print(\"‚úÖ All datasets loaded successfully\")\n",
    "        return datasets\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading datasets: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "datasets = load_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16fff8b",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 1.2: Advanced Data Quality Metrics (45 minutes)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "üéØ TASK: Create comprehensive data quality profiling functions\n",
    "üí° HINT: Look beyond basic missing values - consider temporal patterns, distributions\n",
    "üìö CONCEPTS: Data profiling, quality metrics, statistical validation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49945abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting comprehensive data profiling...\n",
      "\n",
      "üîç Comprehensive Profile: zones\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "üìà Numeric Column Analysis:\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|summary|             lat_min|             lat_max|            lon_min|            lon_max|        population|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|  count|                   8|                   8|                  8|                  8|                 8|\n",
      "|   mean|  40.730000000000004|             40.7525| -73.99125000000001|          -73.97125|           21250.0|\n",
      "| stddev|0.023904572186687328|0.028157719063465373|0.02474873734153055|0.02474873734153458|14260.334598358582|\n",
      "|    min|                40.7|               40.72|             -74.02|              -74.0|              5000|\n",
      "|    max|               40.76|                40.8|             -73.96|             -73.94|             45000|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "\n",
      "\n",
      "üîç Comprehensive Profile: traffic\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ Temporal Analysis:\n",
      "   Time Range: 2025-08-28 19:13:45.641653 to 2025-09-04 19:13:45.641653\n",
      "   Records with timestamps: 100,850\n",
      "   Expected interval: 5 minutes\n",
      "üìÖ Data Freshness: Latest record is 13.3 hours old\n",
      "üìà Numeric Column Analysis:\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|summary|        location_lat|       location_lon|     vehicle_count|         avg_speed|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|  count|              100850|             100850|            100850|            100850|\n",
      "|   mean|   40.75011790265806| -73.95663481997272|22.197045116509667|45.601101580647274|\n",
      "| stddev|0.027328503495008007|0.03287764405886609|13.561857125784131| 17.04167685328942|\n",
      "|    min|  40.700157401736284| -74.01290766686316|                 0|               5.0|\n",
      "|    max|  40.798402010459405| -73.90082117344016|                91| 116.4255545170856|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "\n",
      "üìÇ Categorical Column Analysis:\n",
      "   sensor_id: 50 distinct values\n",
      "   congestion_level: 3 distinct values\n",
      "      Top values:\n",
      "+----------------+-----+\n",
      "|congestion_level|count|\n",
      "+----------------+-----+\n",
      "|medium          |63749|\n",
      "|low             |20124|\n",
      "|high            |16977|\n",
      "+----------------+-----+\n",
      "\n",
      "   road_type: 4 distinct values\n",
      "      Top values:\n",
      "+-----------+-----+\n",
      "|road_type  |count|\n",
      "+-----------+-----+\n",
      "|highway    |28238|\n",
      "|arterial   |28238|\n",
      "|commercial |26221|\n",
      "|residential|18153|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Comprehensive Profile: air_quality\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "‚ùå Error profiling air_quality: [UNSUPPORTED_FEATURE.QUERY_ONLY_CORRUPT_RECORD_COLUMN] The feature is not supported: Queries from raw JSON/CSV/XML files are disallowed when the\n",
      "referenced columns only include the internal corrupt record column\n",
      "(named `_corrupt_record` by default). For example:\n",
      "`spark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()`\n",
      "and `spark.read.schema(schema).json(file).select(\"_corrupt_record\").show()`.\n",
      "Instead, you can cache or save the parsed results and then send the same query.\n",
      "For example, `val df = spark.read.schema(schema).json(file).cache()` and then\n",
      "`df.filter($\"_corrupt_record\".isNotNull).count()`. SQLSTATE: 0A000\n",
      "\n",
      "üîç Comprehensive Profile: weather\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n",
      "‚è∞ Temporal Analysis:\n",
      "   Time Range: 2025-08-28 19:13:45.641653 to 2025-09-04 19:13:45.641653\n",
      "   Records with timestamps: 3,370\n",
      "   Expected interval: 30 minutes\n",
      "üìÖ Data Freshness: Latest record is 13.3 hours old\n",
      "üìà Numeric Column Analysis:\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+-------------------+--------------------+------------------+\n",
      "|summary|        location_lat|       location_lon|       temperature|          humidity|          wind_speed|     wind_direction|       precipitation|          pressure|\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+-------------------+--------------------+------------------+\n",
      "|  count|                3370|               3370|              3370|              3370|                3370|               3370|                3370|              3370|\n",
      "|   mean|  40.744659328860365| -73.97135064318793|19.810921196145923| 60.00735104978684|   8.115559392808636| 179.07501014213207|0.059748613682613014| 1012.938419331309|\n",
      "| stddev|0.035050829601692896|0.04174693811588695|3.2053776129492166|14.637398030894772|    8.07924377014336| 103.66153873620159|  0.2475344933641174|10.035478736930868|\n",
      "|    min|  40.700431377211714| -74.01364897626706| 7.465013216576269|              20.0|0.002807241069954...|0.19449606013004495|                 0.0| 976.6862164596702|\n",
      "|    max|   40.79644724145399| -73.90420441177432|30.076456657829173|             100.0|   62.42099178181472|  359.9192660727401|   3.268515396201009|1046.5364453753104|\n",
      "+-------+--------------------+-------------------+------------------+------------------+--------------------+-------------------+--------------------+------------------+\n",
      "\n",
      "üìÇ Categorical Column Analysis:\n",
      "   station_id: 10 distinct values\n",
      "      Top values:\n",
      "+-----------+-----+\n",
      "|station_id |count|\n",
      "+-----------+-----+\n",
      "|WEATHER_002|337  |\n",
      "|WEATHER_004|337  |\n",
      "|WEATHER_006|337  |\n",
      "|WEATHER_005|337  |\n",
      "|WEATHER_001|337  |\n",
      "+-----------+-----+\n",
      "\n",
      "\n",
      "üîç Comprehensive Profile: energy\n",
      "--------------------------------------------------\n",
      "üìã Missing Value Analysis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ Temporal Analysis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time Range: 2025-08-28 19:13:45.641653 to 2025-09-04 19:13:45.641653\n",
      "   Records with timestamps: 201,800\n",
      "   Expected interval: 10 minutes\n",
      "üìÖ Data Freshness: Latest record is 13.3 hours old\n",
      "üìà Numeric Column Analysis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|summary|        location_lat|       location_lon| power_consumption|           voltage|           current|        power_factor|\n",
      "+-------+--------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|              201800|             201800|            201800|            201800|            201800|              201800|\n",
      "|   mean|   40.75114696746992| -73.95570226085518|17.661501426137587|239.99110505236544| 73.62718756676854|  0.9000784069428297|\n",
      "| stddev|0.029149474776993135|0.03696918385133131| 18.75357389149128| 5.010000846318629| 78.21712082409343|0.028898400050630813|\n",
      "|    min|   40.70003316266749| -74.01817156010618|1.6800237743084105| 218.3401346513567| 6.660334472337805|   0.850000401033654|\n",
      "|    max|   40.79986752880212| -73.90007563085238| 64.99993909041741|  264.016592785443|288.69565536070843|  0.9499999715633012|\n",
      "+-------+--------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "\n",
      "üìÇ Categorical Column Analysis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meter_id: 200 distinct values\n",
      "   building_type: 5 distinct values\n",
      "      Top values:\n",
      "+-------------+-----+\n",
      "|building_type|count|\n",
      "+-------------+-----+\n",
      "|residential  |48432|\n",
      "|commercial   |40360|\n",
      "|retail       |40360|\n",
      "|office       |36324|\n",
      "|industrial   |36324|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def comprehensive_data_profile(df, dataset_name, time_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive data quality profile\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to profile\n",
    "        dataset_name: Name for reporting\n",
    "        time_col: Timestamp column name (can be None for non-time series data)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with quality metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Comprehensive Profile: {dataset_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_rows = df.count()\n",
    "    total_cols = len(df.columns)\n",
    "    \n",
    "    profile = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'total_rows': total_rows,\n",
    "        'total_columns': total_cols,\n",
    "        'memory_usage_mb': 0,  # Estimate\n",
    "        'quality_issues': []\n",
    "    }\n",
    "    \n",
    "    # TODO: Calculate missing value patterns\n",
    "    print(\"üìã Missing Value Analysis:\")\n",
    "    missing_analysis = {}\n",
    "    for col in df.columns:\n",
    "        missing_count = df.filter(F.col(col).isNull()).count()\n",
    "        missing_pct = (missing_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "        missing_analysis[col] = {'count': missing_count, 'percentage': missing_pct}\n",
    "        \n",
    "        if missing_pct > 5:  # Flag columns with >5% missing\n",
    "            profile['quality_issues'].append(f\"High missing values in {col}: {missing_pct:.2f}%\")\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            print(f\"   {col}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    profile['missing_analysis'] = missing_analysis\n",
    "    \n",
    "    # TODO: Temporal data gaps (if timestamp column exists and is not None)\n",
    "    if time_col and time_col in df.columns:\n",
    "        print(\"‚è∞ Temporal Analysis:\")\n",
    "        \n",
    "        # Get time range\n",
    "        time_stats = df.agg(\n",
    "            F.min(time_col).alias('min_time'),\n",
    "            F.max(time_col).alias('max_time'),\n",
    "            F.count(time_col).alias('time_count')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"   Time Range: {time_stats['min_time']} to {time_stats['max_time']}\")\n",
    "        print(f\"   Records with timestamps: {time_stats['time_count']:,}\")\n",
    "        \n",
    "        # TODO: Check for temporal gaps\n",
    "        # Calculate expected vs actual record counts\n",
    "        if dataset_name == 'traffic':\n",
    "            expected_interval_minutes = 5\n",
    "        elif dataset_name == 'air_quality':\n",
    "            expected_interval_minutes = 15\n",
    "        elif dataset_name == 'weather':\n",
    "            expected_interval_minutes = 30\n",
    "        elif dataset_name == 'energy':\n",
    "            expected_interval_minutes = 10\n",
    "        else:\n",
    "            expected_interval_minutes = 15\n",
    "        \n",
    "        print(f\"   Expected interval: {expected_interval_minutes} minutes\")\n",
    "        \n",
    "        # TODO: Data freshness (for time series data)\n",
    "        latest_record = df.agg(F.max(time_col).alias('latest')).collect()[0]['latest']\n",
    "        if latest_record:\n",
    "            from datetime import datetime\n",
    "            hours_old = (datetime.now() - latest_record).total_seconds() / 3600\n",
    "            print(f\"üìÖ Data Freshness: Latest record is {hours_old:.1f} hours old\")\n",
    "    \n",
    "    # TODO: Numeric column distributions\n",
    "    numeric_cols = [field.name for field in df.schema.fields \n",
    "                   if field.dataType in [IntegerType(), DoubleType(), FloatType(), LongType()]]\n",
    "    \n",
    "    if numeric_cols:\n",
    "        print(\"üìà Numeric Column Analysis:\")\n",
    "        # Get basic statistics for numeric columns\n",
    "        stats_df = df.select(numeric_cols).describe()\n",
    "        stats_df.show()\n",
    "        \n",
    "        # TODO: Check for suspicious patterns in numeric data\n",
    "        for col in numeric_cols:\n",
    "            if col not in ['location_lat', 'location_lon']:\n",
    "                # Check for columns with very low variance (potentially stuck sensors)\n",
    "                variance_check = df.agg(F.variance(col).alias('variance')).collect()[0]['variance']\n",
    "                if variance_check is not None and variance_check < 0.001:\n",
    "                    profile['quality_issues'].append(f\"Very low variance in {col}: {variance_check}\")\n",
    "    \n",
    "    # TODO: Categorical column analysis\n",
    "    categorical_cols = [field.name for field in df.schema.fields \n",
    "                       if field.dataType == StringType() and field.name not in [time_col] if time_col]\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(\"üìÇ Categorical Column Analysis:\")\n",
    "        for col in categorical_cols:\n",
    "            distinct_count = df.select(col).distinct().count()\n",
    "            print(f\"   {col}: {distinct_count} distinct values\")\n",
    "            \n",
    "            # Show top values\n",
    "            if distinct_count < 20:\n",
    "                top_values = df.groupBy(col).count().orderBy(F.desc(\"count\")).limit(5)\n",
    "                print(f\"      Top values:\")\n",
    "                top_values.show(5, truncate=False)\n",
    "    \n",
    "    # TODO: Check for duplicate records\n",
    "    duplicate_count = total_rows - df.dropDuplicates().count()\n",
    "    if duplicate_count > 0:\n",
    "        profile['quality_issues'].append(f\"Duplicate records found: {duplicate_count}\")\n",
    "        print(f\"üîÑ Duplicate Records: {duplicate_count:,}\")\n",
    "    \n",
    "    return profile\n",
    "\n",
    "      \n",
    "\n",
    "# TODO: Profile all datasets\n",
    "print(\"üîç Starting comprehensive data profiling...\")\n",
    "\n",
    "profiles = {}\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        try:\n",
    "            # Check if the dataset has a timestamp column\n",
    "            if name != 'zones' and 'timestamp' in df.columns:\n",
    "                profiles[name] = comprehensive_data_profile(df, name, time_col=\"timestamp\")\n",
    "            else:\n",
    "                # For datasets without timestamp column (like zones), don't pass time_col\n",
    "                profiles[name] = comprehensive_data_profile(df, name, time_col=None)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error profiling {name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d6f93",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TODO 1.3: Sensor Health Analysis (45 minutes)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "üéØ TASK: Identify sensors with potential operational issues\n",
    "üí° HINT: Look for patterns that indicate sensor malfunctions\n",
    "üìö CONCEPTS: Sensor diagnostics, operational monitoring, health scoring\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473a8028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè• Analyzing sensor health across all datasets...\n",
      "\n",
      "üè• Sensor Health Analysis\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataFrame.withColumn() missing 1 required positional argument: 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Traffic sensors\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtraffic\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     traffic_health = \u001b[43manalyze_sensor_health\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraffic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msensor_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvehicle_count\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mavg_speed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     sensor_health_results[\u001b[33m'\u001b[39m\u001b[33mtraffic\u001b[39m\u001b[33m'\u001b[39m] = traffic_health\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöó Traffic Sensor Health Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36manalyze_sensor_health\u001b[39m\u001b[34m(df, sensor_id_col, value_cols, time_col)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m value_cols:\n\u001b[32m     28\u001b[39m     missing_col_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_missing_pct\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     health_metrics = \u001b[43mhealth_metrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_col_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# TODO: Calculate percentage of missing values for this sensor and column\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HINT: Use window functions to calculate missing percentage per sensor\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# TODO: Add variance analysis (detect stuck sensors)\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m value_cols:\n",
      "\u001b[31mTypeError\u001b[39m: DataFrame.withColumn() missing 1 required positional argument: 'col'"
     ]
    }
   ],
   "source": [
    "def analyze_sensor_health(df, sensor_id_col, value_cols, time_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Analyze individual sensor health and identify problematic sensors\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with sensor data\n",
    "        sensor_id_col: Column name for sensor ID\n",
    "        value_cols: List of measurement columns to analyze\n",
    "        time_col: Timestamp column\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with sensor health metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nüè• Sensor Health Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # TODO: Calculate health metrics per sensor\n",
    "    window_spec = Window.partitionBy(sensor_id_col)\n",
    "    \n",
    "    health_metrics = df.groupBy(sensor_id_col).agg(\n",
    "        F.count(\"*\").alias(\"total_readings\"),\n",
    "        F.min(time_col).alias(\"first_reading\"),\n",
    "        F.max(time_col).alias(\"last_reading\")\n",
    "    )\n",
    "    \n",
    "    # TODO: Add missing data percentage per sensor\n",
    "    for col in value_cols:\n",
    "        missing_col_name = f\"{col}_missing_pct\"\n",
    "        health_metrics = health_metrics.withColumn(\n",
    "            missing_col_name,\n",
    "            # TODO: Calculate percentage of missing values for this sensor and column\n",
    "            # HINT: Use window functions to calculate missing percentage per sensor\n",
    "        )\n",
    "    \n",
    "    # TODO: Add variance analysis (detect stuck sensors)\n",
    "    for col in value_cols:\n",
    "        if col in df.columns:\n",
    "            variance_col_name = f\"{col}_variance\"\n",
    "            sensor_variance = df.groupBy(sensor_id_col).agg(\n",
    "                F.variance(col).alias(variance_col_name)\n",
    "            )\n",
    "            health_metrics = health_metrics.join(sensor_variance, sensor_id_col, \"left\")\n",
    "    \n",
    "    # TODO: Calculate data gaps (irregular reporting)\n",
    "    # This is more complex - calculate time differences between consecutive readings\n",
    "    \n",
    "    # TODO: Create overall health score\n",
    "    # Combine multiple factors into a single health score (0-100)\n",
    "    health_metrics = health_metrics.withColumn(\n",
    "        \"health_score\",\n",
    "        # TODO: Create a formula that combines:\n",
    "        # - Missing data percentage (lower is better)\n",
    "        # - Data variance (too low = stuck sensor, too high = noisy sensor)\n",
    "        # - Reporting regularity\n",
    "        # - Recent data availability\n",
    "        F.lit(100.0)  # Placeholder - implement your scoring logic\n",
    "    )\n",
    "    \n",
    "    # TODO: Flag problematic sensors\n",
    "    health_metrics = health_metrics.withColumn(\n",
    "        \"status\",\n",
    "        F.when(F.col(\"health_score\") > 80, \"healthy\")\n",
    "         .when(F.col(\"health_score\") > 60, \"warning\")\n",
    "         .otherwise(\"critical\")\n",
    "    )\n",
    "    \n",
    "    return health_metrics\n",
    "\n",
    "# TODO: Analyze health for each sensor type\n",
    "print(\"üè• Analyzing sensor health across all datasets...\")\n",
    "\n",
    "sensor_health_results = {}\n",
    "\n",
    "# Traffic sensors\n",
    "if 'traffic' in datasets:\n",
    "    traffic_health = analyze_sensor_health(\n",
    "        datasets['traffic'], \n",
    "        'sensor_id', \n",
    "        ['vehicle_count', 'avg_speed']\n",
    "    )\n",
    "    sensor_health_results['traffic'] = traffic_health\n",
    "    \n",
    "    print(\"üöó Traffic Sensor Health Summary:\")\n",
    "    traffic_health.groupBy(\"status\").count().show()\n",
    "\n",
    "# TODO: Analyze other sensor types\n",
    "# Air quality sensors\n",
    "if 'air_quality' in datasets:\n",
    "    # TODO: Implement air quality sensor health analysis\n",
    "    pass\n",
    "\n",
    "# Weather sensors  \n",
    "if 'weather' in datasets:\n",
    "    # TODO: Implement weather sensor health analysis\n",
    "    pass\n",
    "\n",
    "# Energy sensors\n",
    "if 'energy' in datasets:\n",
    "    # TODO: Implement energy sensor health analysis\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
